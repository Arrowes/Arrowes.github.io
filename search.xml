<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>DL：深度学习相关概念</title>
    <url>/DL/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>记录深度学习基本概念，不断更新中，项目地址：<a href="https://github.com/Arrowes/DLpractice">DLpractice</a></p>
<span id="more"></span>

<p>从机器学习到深度学习：<a href="https://www.cnblogs.com/subconscious/p/4107357.html">从机器学习谈起</a>，<a href="https://www.cnblogs.com/subconscious/p/5058741.html">从神经元到深度学习</a><br>什么是卷积讲解视频：<a href="https://www.bilibili.com/video/BV1sb411P7pQ/?share_source=copy_web&vd_source=b148fb6f311bfe6f3870ad8f4dfda92a">大白话讲解卷积神经网络工作原理</a></p>
<h1 id="深度学习框架"><a href="#深度学习框架" class="headerlink" title="深度学习框架"></a>深度学习框架</h1><pre class="mermaid">graph LR
A[程序框架]-->B[A.黑箱]
A-->C[B.模块化] -->1.处理数据
C-->2.构建网络
C-->3.损失函数
C-->4.优化函数
C-->5.模型保存
A-->E[C.定义]</pre>

<img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL1.png" width = "80%" />

<p>GPU 网络和数据要同时送进GPU</p>
<h2 id="激活函数-Activate-Function"><a href="#激活函数-Activate-Function" class="headerlink" title="激活函数 Activate Function"></a>激活函数 Activate Function</h2><p>激活函数是深度学习神经网络中的一个重要组成部分，它用来引入<em>非线性性质</em>，使神经网络能够学习复杂的函数关系。激活函数接收神经元的输入，并产生输出作为下一层神经元的输入。在神经网络中，激活函数通常被应用于每个神经元的输出，使神经网络能够进行非线性映射和学习。</p>
<p>激活函数的主要作用有以下几点：</p>
<ul>
<li>非线性映射：激活函数引入非线性性质，使神经网络可以逼近和表示复杂的非线性函数。如果没有激活函数，多层神经网络的组合将等效于单一层的线性变换。</li>
<li>特征提取：激活函数有助于神经网络从输入数据中提取关键特征，例如边缘、纹理、形状等，以便更好地完成分类、回归和其他任务。</li>
<li>解决梯度消失问题：某些激活函数（如ReLU）有助于减轻梯度消失问题，使深层网络能够更好地进行反向传播和训练。</li>
</ul>
<p>一些常见的激活函数包括：</p>
<table>
<thead>
<tr>
<th>激活函数</th>
<th>特点</th>
<th>图像</th>
</tr>
</thead>
<tbody><tr>
<td>$$softmax(x_i) &#x3D; \frac{e^{x_i}}{\sum_{j&#x3D;0}^{N} e^{x_j}}$$</td>
<td>将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质;常用在分类网络的最后一层，把网络输出转化为各类别的概率。</td>
<td>首先对每个未规范化的预测求幂，这样可以确保输出非负。为了确保最终输出的概率值总和为1，再让每个求幂后的结果除以它们的总和</td>
</tr>
<tr>
<td>挤压函数（squashing function）$$sigmoid(x) &#x3D; \frac 1{1 + exp(−x)}$$</td>
<td>将输入映射到范围(0, 1)，常用于二元分类问题。sigmoid可以视为softmax的特例</td>
<td><img data-src="https://zh.d2l.ai/_images/output_mlp_76f463_51_0.svg"  /></td>
</tr>
<tr>
<td>双曲正切 $$tanh(x) &#x3D; \frac {1 − exp(−2x)}{1 + exp(−2x)}$$</td>
<td>将输入映射到范围(-1, 1)，也用于某些分类和回归问题, 当输入在0附近时，tanh函数接近线性变换。形状类似于sigmoid函数，不同的是tanh函数关于坐标系原点中心对称。（LSTM）</td>
<td><img data-src="https://zh.d2l.ai/_images/output_mlp_76f463_81_0.svg"  /></td>
</tr>
<tr>
<td>修正线性单元（Rectified Linear Unit）$$ReLU(x) &#x3D; max(x, 0)$$  $$LeakyReLU&#x3D;max(αx,x)$$</td>
<td>求导表现得特别好：要么让参数消失，要么让参数通过。最常用的激活函数，通常能够加速收敛和减轻梯度消失问题（Transfromer）； LeakyReLU中通常设α&#x3D;0.01来调整负值的零梯度，缓解dead ReLU问题（YOLO） 若α为可学习参数，则为PReLU</td>
<td><img data-src="https://zh.d2l.ai/_images/output_mlp_76f463_21_0.svg"  /></td>
</tr>
<tr>
<td>指数线性单元 (Exponential Linear Units) <img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL_ELU.png"/></td>
<td>对小于零的情况采用类似指数计算的方式进行输出。与 ReLU 相比，ELU 有负值，这会使激活的平均值接近零。均值激活接近于零可以使学习更快，因为它们使梯度更接近自然梯度。但计算量较大</td>
<td><img data-src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMyLnpoaW1nLmNvbS84MC92Mi02MDRiZTExNGZhMDQ3OGYzYTEwNTk5MjNmZDEwMjJkMV9oZC5wbmc?x-oss-process=image/format,png"  /></td>
</tr>
</tbody></table>
<h2 id="感受野-Receptive-field"><a href="#感受野-Receptive-field" class="headerlink" title="感受野(Receptive field)"></a>感受野(Receptive field)</h2><p>感受野是指在卷积神经网络中，输出特征图上的一个像素点对应于输入图像上的感受区域大小。感受野的大小可以用来衡量网络在某一层上能够“看到”输入图像的范围，从而影响网络对局部和全局信息的感知能力。<br><img data-src="https://pic1.zhimg.com/80/v2-93a99cd695aeb1b8edf0c4b4eac8b7a9_1440w.webp?source=1940ef5c"  /></p>
<p>$   n_{output.features}&#x3D;[\frac{n_{input.features}+2p_{adding.size}-k_{ernel.size}}{s_{tride.size}}+1]   $<br>较小的感受野通常用于捕获局部特征，而较大的感受野则有助于捕获全局信息。<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL2.png" width = "50%" /><br><img data-src="https://pic1.zhimg.com/50/v2-d552433faa8363df84c53b905443a556_720w.webp?source=1940ef5c" width = "50%" /></p>
<h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><img alt="图 37" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL-Conv.jpg" />  


<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>待续</p>
<h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL3.gif" width = "60%" />

<p>$$SGD → SGDM → NAG → AdaGrad → AdaDelta → Adam → Nadam$$<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL4.png" alt="图 4">  </p>
<h2 id="Batch-size"><a href="#Batch-size" class="headerlink" title="Batch size"></a>Batch size</h2><p>batch size的大小影响的是训练过程中的完成<em>每个epoch所需的时间</em> $^1$（假设算力确定了）和每次迭代(iteration)之间<em>梯度的平滑程度</em> $^2$。</p>
<blockquote>
<ol>
<li>假设训练集大小为N，每个epoch中mini-batch大小为b，那么完成每个epoch所需的迭代次数为 N&#x2F;b , 因此完成每个epoch所需的时间会随着迭代次数的增加而增加</li>
<li>如pytorch\tensorflow等深度学习框架，在进行mini-batch的loss反向传播时，一般都是先将每个mini-batch中每个样本得到的loss求sum后再平均化之后再反求梯度，进行迭代，因此b的大小决定了相邻迭代batch之间的梯度平滑程度。一个batch内所含样本越多，这个batch的梯度应该越能反映真实的梯度，因此这样的大batch间梯度不会跨越太大</li>
</ol>
</blockquote>
<p>因此：大的batch_size往往建议可以相应取大点learning_rate, 因为梯度震荡小，大 learning_rate可以加速收敛过程，也可以防止陷入到局部最小值，而小batch_size用小learning_rate迭代，防止错过最优点，一直上下震荡没法收敛 </p>
<blockquote>
<ol>
<li>若是loss还能降，指标还在升，那说明欠拟合，还没收敛，应该继续train，增大epoch。</li>
<li>若是loss还能再降，指标也在降，说明过拟合了，那就得采用提前终止（减少epoch）或采用weight_decay等防过拟合措施。</li>
<li>若是设置epoch&#x3D;16，到第8个epoch，loss也不降了，指标也不动了，说明8个epoch就够了，剩下的白算了。</li>
</ol>
</blockquote>
<h2 id="损失函数「loss-function」"><a href="#损失函数「loss-function」" class="headerlink" title="损失函数「loss function」"></a>损失函数「loss function」</h2><p>来度量模型的预测值$\hat{y}$与真实值$y$的差异程度的运算函数，它是一个非负实值函数，通常使用$L(y, \hat{y})$来表示，损失函数越小，模型的鲁棒性就越好。</p>
<h3 id="基于距离度量的损失函数"><a href="#基于距离度量的损失函数" class="headerlink" title="基于距离度量的损失函数"></a>基于距离度量的损失函数</h3><p>基于距离度量的损失函数通常将输入数据映射到基于距离度量的特征空间上，如欧氏空间、汉明空间等，将映射后的样本看作空间上的点，采用合适的损失函数度量特征空间上样本真实值和模型预测值之间的距离。特征空间上两个点的距离越小，模型的预测性能越好。<br><strong>L1范数损失函数（MAE）</strong><br>$$L_{MSE}&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^{n}|y_i-\hat{y_i}|$$<br>又称为曼哈顿距离，表示残差的绝对值之和。L1损失函数对离群点有很好的鲁棒性，但它在残差为零处却不可导,且更新的梯度始终相同；<br><strong>L2损失函数（MSE均方误差损失函数）</strong><br>$$L_{MSE}&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^{n}(y_i-\hat{y_i})^2$$<br>在回归问题中，均方误差损失函数用于度量样本点到回归曲线的距离，通过最小化平方损失使样本点可以更好地拟合回归曲线。（L2损失又被称为欧氏距离，是一种常用的距离度量方法，通常用于度量数据点之间的相似度。）</p>
<h3 id="基于概率分布度量的损失函数"><a href="#基于概率分布度量的损失函数" class="headerlink" title="基于概率分布度量的损失函数"></a>基于概率分布度量的损失函数</h3><p>基于概率分布度量的损失函数是将样本间的相似性转化为随机事件出现的可能性，即通过度量样本的真实分布与它估计的分布之间的距离，判断两者的相似度，一般用于涉及概率分布或预测类别出现的概率的应用问题中，在分类问题中尤为常用。<br><strong>KL散度（ Kullback-Leibler divergence）</strong><br>$$L_{MSE}&#x3D;\sum_{i&#x3D;1}^{n}\hat{y_i}log(\frac{y_i}{\hat{y_i}})$$<br>也被称为相对熵，是一种非对称度量方法，常用于度量两个概率分布之间的距离。KL散度也可以衡量两个随机分布之间的距离，两个随机分布的相似度越高的，它们的KL散度越小，可以用于比较文本标签或图像的相似性。<br><strong>交叉熵损失函数「Cross Entropy Loss」</strong><br>$$L&#x3D;-[ylog\hat{y}+(1-y)log(1-\hat{y})]$$<br>$$L&#x3D;\sum_{i&#x3D;1}^{N}y^ilog\hat{y}^i+(1-y^i)log(1-\hat{y}^i)$$<br>交叉熵是信息论中的一个概念，最初用于估算平均编码长度，引入机器学习后，用于评估当前训练得到的概率分布与真实分布的差异情况。为了使神经网络的每一层输出从线性组合转为非线性逼近，以提高模型的预测精度，在以交叉熵为损失函数的神经网络模型中一般选用tanh、sigmoid、softmax或ReLU作为激活函数。</p>
<p>交叉熵损失函数刻画了实际输出概率与期望输出概率之间的相似度，也就是交叉熵的值越小，两个概率分布就越接近，特别是在正负样本不均衡的分类问题中，常用交叉熵作为损失函数。目前，交叉熵损失函数是卷积神经网络中最常使用的分类损失函数，它可以有效避免梯度消散。在二分类情况下也叫做对数损失函数</p>
<p>在多分类任务中，经常采用 softmax 激活函数+交叉熵损失函数，因为交叉熵描述了两个概率分布的差异，然而神经网络输出的是向量，并不是概率分布的形式。所以需要 softmax激活函数将一个向量进行“归一化”成概率分布的形式，再采用交叉熵损失函数计算 loss。</p>
<p>在Pytorch中，BCELoss和BCEWithLogitsLoss是一组常用的二元交叉熵损失函数，常用于二分类问题。区别在于BCELoss的输入需要先进行Sigmoid处理，而BCEWithLogitsLoss则是将Sigmoid和BCELoss合成一步，也就是说BCEWithLogitsLoss函数内部自动先对output进行Sigmoid处理，再对output 和target进行BCELoss计算。 </p>
<p>one-hot独热编码：将类别变量转换为机器学习算法易于利用的一种形式的过程。</p>
<h1 id="注意力机制（Attention-Mechanism）"><a href="#注意力机制（Attention-Mechanism）" class="headerlink" title="注意力机制（Attention Mechanism）"></a>注意力机制（Attention Mechanism）</h1><p>自上而下有意识的聚焦称为<strong>聚焦式注意力</strong>，自下而上无意识、由外界刺激引发的注意力称为<strong>显著式注意力</strong>。<br>神经网络中的注意力机制是在计算能力有限的情况下，将计算资源分配给更重要的任务，同时解决信息超载问题的一种资源分配方案，到2014年，Volodymyr的《Recurrent Models of Visual Attention》一文中将其应用在视觉领域，后来伴随着2017年Ashish Vaswani的《Attention is all you need》中Transformer结构的提出，注意力机制在NLP,CV相关问题的网络设计上被广泛应用。<br>注意力有两种，一种是软注意力(soft attention)，另一种则是强注意力(hard attention)。<br><strong>软注意力</strong>更关注区域或者通道，是确定性的注意力，学习完成后直接可以通过网络生成，最关键的地方是软注意力是可微的，这是一个非常重要的地方。可以微分的注意力就可以通过神经网络算出梯度并且前向传播和后向反馈来学习得到注意力的权重。<br><strong>强注意力</strong>是更加关注点，也就是图像中的每个点都有可能延伸出注意力，同时强注意力是一个随机的预测过程，更强调动态变化。当然，最关键是强注意力是一个不可微的注意力，训练过程往往是通过增强学习(reinforcement learning)来完成的。</p>
<h2 id="软注意力的注意力域"><a href="#软注意力的注意力域" class="headerlink" title="软注意力的注意力域"></a>软注意力的注意力域</h2><h3 id="空间域（Spatial-Domain）"><a href="#空间域（Spatial-Domain）" class="headerlink" title="空间域（Spatial Domain）"></a>空间域（Spatial Domain）</h3><p>空间域将原始图片中的空间信息变换到另一个空间中并保留了关键信息。<br>普通的卷积神经网络中的池化层（pooling layer）直接用一些max pooling 或者average pooling 的方法，将图片信息压缩，减少运算量提升准确率。<br>发明者认为之前pooling的方法太过于暴力，直接将信息合并会导致关键信息无法识别出来，所以提出了一个叫 <strong>空间转换器（spatial transformer）</strong> 的模块，将图片中的的空间域信息做对应的空间变换，从而能将关键的信息提取出来。<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL5.png" width = "50%" /></p>
<h3 id="通道域（Channel-Domain）"><a href="#通道域（Channel-Domain）" class="headerlink" title="通道域（Channel Domain）"></a>通道域（Channel Domain）</h3><p>通道注意力机制在计算机视觉中，更关注特征图中channel之间的关系，而普通的卷积会对通道做通道融合，这个开山鼻祖是SENet,后面有GSoP-Net，FcaNet 对SENet中的squeeze部分改进，EACNet对SENet中的excitation部分改进，SRM,GCT等对SENet中的scale部分改进。</p>
<p><a href="https://arxiv.org/abs/1709.01507">SENet</a>,<a href="https://github.com/moskomule/senet.pytorch">pytorch</a><br>SENet《Squeeze-and-Excitation Networks》是CVPR17年的一篇文章，提出SE module。在卷积神经网络中，卷积操作更多的是关注感受野，在通道上默认为是所有通道的融合（深度可分离卷积不对通道进行融合，但是没有学习通道之间的关系，其主要目的是为了减少计算量），SENet提出SE模块，将注意力放到通道之间，希望模型可以学习到不同通道之间的权重：<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL6.png" alt="图 6">  </p>
<h3 id="时域注意力机制"><a href="#时域注意力机制" class="headerlink" title="时域注意力机制"></a>时域注意力机制</h3><p>时域注意力机制在cv领域主要考虑有时序信息的领域，如视频领域中的动作识别方向，其注意力机制主要是在时序列中，关注某一时序即某一帧的信息。</p>
<h3 id="通道和空间注意力机制"><a href="#通道和空间注意力机制" class="headerlink" title="通道和空间注意力机制"></a>通道和空间注意力机制</h3><p>通道和空间注意力是基于通道注意力和空间注意力机制，将两者有效的结合在一起，让注意力能关注到两者，又称混合注意力机制，如CBAM,BAM,scSE等，同时基于混合注意力机制的一些关注点，如Triplet Attention 关注各种跨维度的相互作用；Coordinate Attention, DANet关注长距离的依赖；RGA 关注关系感知注意力。还有一种混合注意力机制，为3D的attention :Residual attention,SimAM, Strip Pooling, SCNet等。</p>
<p><a href="https://arxiv.org/abs/1807.06521">CBAM</a>,<a href="https://github.com/luuuyi/CBAM.PyTorch">github</a><br>CBAM (Convolutional Block Attention Module)是SENet的一种拓展，SENet主要基于通道注意力，CBAM是通道注意力和空间注意力融合的注意力机制。<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL7.png" alt="图 7"><br>如上图所示，输入一个h<em>w</em>c的特征图，通过channel Attention Module 生成通道注意力权重对输入特征图在通道层添加权重，再通过spatial Attention Module 生成空间注意力权重，对特征图在空间层添加权重，输出特征图。</p>
<h1 id="Metrics-评估"><a href="#Metrics-评估" class="headerlink" title="Metrics 评估"></a>Metrics 评估</h1><h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL8.png" width = "70%" />

<p>X横坐标为正确的分类（即你用标签所标注的真实分类）<br>Y纵坐标为模型所预测的分类（即图片经过模型推理后模型将其辨别为的分类）</p>
<blockquote>
<p>True positives (TP): 猫🐱的图片被正确识别成了猫🐱。（猫🐱的正确分类预测）<br>True negatives(TN): 背景的图片被正确识别为背景。（非猫🐱被预测为其他动物或背景）<br>False positives(FP): 背景的图片被错误识别为猫🐱。（非猫🐱被预测为猫🐱）<br>False negatives(FN): 猫🐱的图片被错误识别为背景。（猫🐱被预测为其他动物或者背景）</p>
</blockquote>
<h2 id="Evaluation-parameters"><a href="#Evaluation-parameters" class="headerlink" title="Evaluation parameters"></a>Evaluation parameters</h2><p><strong>准确率 Accuracy</strong>：在正负样本数量接近的情况下，准确率越高，模型的性能越好（当测试样本不平衡时，该指标会失去意义。）<br>$$Accuracy&#x3D;\frac{TP+TN}{TP+FP+TN+FN}$$<br><strong>精准率（查准率） precision</strong>：代表在总体预测结果中真阳性的预测数，针对预测结果，当区分能力强时，容易将部分（与负样本相似度高）正样本排除。<br>$$precision(P)&#x3D;\frac{TP}{TP+FP}$$<br><strong>召回率（查全率） recall</strong>：所有ground truths中真阳性的预测数，针对原样本，当敏感度高时，容易将部分（与正样本相似度高）负样本也判断为正样本。<br>$$recall(R)&#x3D;\frac{TP}{TP+FN}$$<br><strong>F1 score</strong>：对Precision和Recall两个指标的调和平均值（类似平均速度），F1分值越高，目标检测的准确性越好。<br>$$F_1 score&#x3D;2\cdot \frac{P\cdot R}{P+R}$$<br><strong>AP</strong>：同时考察Precision和Recall两个指标来衡量模型对于各个类别的性能。<br>$$AP_i&#x3D;\int_0^1P_i(R_i)dR_i$$<br><strong>mAP</strong>：表示AP的平均值，并用作衡量目标检测算法的总体检测精度的度量。<br>将recall设置为横坐标，precision设置为纵坐标。PR曲线下围成的面积即AP，所有类别AP平均值即mAP.<br>$$mAP&#x3D;\frac1n\sum_{i &#x3D; 1}^{n}AP_i$$<br><strong>置信度 Confidence</strong>：置信度设定越大，Prediction约接近1，Recall越接近0，要寻找最优的F1分数，需要遍历置信度。<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL9.png" alt="图 9">  </p>
<p><strong>交并比 IoU</strong>（Intersection over Union）：是目标检测中使用的一个概念，IoU计算的是“预测的边框”和“真实的边框”的交叠率，即它们的交集和并集的比值。最理想情况是完全重叠，即比值为1。</p>
<p><em><a href="mailto:&#109;&#97;&#x70;&#64;&#x30;&#x2e;&#x35;">&#109;&#97;&#x70;&#64;&#x30;&#x2e;&#x35;</a></em>即IoU&#x3D;0.5，预测框和标注框的交集与非交集占比相同，都为50%；<br><em>mAP@.5:.95</em>表示在不同IoU阈值（从0.5到0.95，步长0.05）（0.5、0.55、0.6、0.65、0.7、0.75、0.8、0.85、0.9、0.95）上的平均mAP。<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL10.png" width = "60%" /></p>
<p><strong>ROC曲线</strong>(Receiver Operating Characteristic 受试者工作特征)<br>$$TPR&#x3D;\frac{TP}{TP+FN},FPR&#x3D;\frac{FP}{FP+TN}$$可以理解为分类器对正样本的覆盖敏感性和对负样本的敏感性的权衡。<br>在ROC曲线图中，每个点以对应的FPR值为横坐标，以TPR值为纵坐标<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL11ROC.jpg" width = "40%" /></p>
<p><strong>AUC值</strong>：PR曲线下方的面积<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL12AUC.png" width = "70%" /></p>
<blockquote>
<p>1.AUC &#x3D; 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。<br>2.0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。<br>3.AUC &#x3D; 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。<br>4.AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。</p>
</blockquote>
<p>ROC曲线图中，越靠近(0,1)的点对应的模型分类性能越好。而且可以明确的一点是，ROC曲线图中的点对应的模型，它们的不同之处仅仅是在分类时选用的阈值(Threshold)不同，每个点所选用的阈值都对应某个样本被预测为正类的概率值。</p>
<h2 id="模型计算量-FLOPs-和参数量-Params"><a href="#模型计算量-FLOPs-和参数量-Params" class="headerlink" title="模型计算量(FLOPs)和参数量(Params)"></a>模型计算量(FLOPs)和参数量(Params)</h2><p><strong>计算量 FLOPs</strong>：FLOP时指浮点运算次数，s是指秒，即每秒浮点运算次数的意思，考量一个网络模型的计算量的标准。硬件要求是在于芯片的floaps（指的是gpu的运算能力）<br><strong>参数量 Params</strong>：是指网络模型中需要训练的参数总数。硬件要求在于显存大小<br>1.<strong>卷积层</strong><br>计算时间复杂度(计算量)<br>$$Time\sim O(\sum_{l&#x3D;1}^D M_l^2\cdot K_l^2\cdot C_{l-1}\cdot C_l)$$</p>
<p>计算空间复杂度(参数量)<br>$$Space\sim O(\sum_{l&#x3D;1}^D K_l^2\cdot C_{l-1}\cdot C_l+\sum_{l&#x3D;1}^D M^2\cdot C_l)$$</p>
<figure class="highlight arcade"><table><tr><td class="code"><pre><span class="line">参数量</span><br><span class="line">(kernel*kernel) *channel_input*channel_output</span><br><span class="line">kernel*kernel 就是 weight * weight</span><br><span class="line">其中kernel*kernel ＝ <span class="number">1</span>个<span class="built_in">feature</span>的参数量</span><br><span class="line"></span><br><span class="line">计算量</span><br><span class="line">(kernel*kernel*<span class="built_in">map</span>*<span class="built_in">map</span>) *channel_input*channel_output</span><br><span class="line">kernel*kernel 就是weight*weight</span><br><span class="line"><span class="built_in">map</span>*<span class="built_in">map</span>是下个featuremap的大小，也就是上个weight*weight到底做了多少次运算</span><br><span class="line">其中kernel*kernel*<span class="built_in">map</span>*<span class="built_in">map</span>＝　<span class="number">1</span>个<span class="built_in">feature</span>的计算量</span><br></pre></td></tr></table></figure>
<p>2.池化层<br>无参数<br>3.<strong>全连接层</strong><br><code>参数量＝计算量＝weight_in*weight_out  #模型里面最费参数的就是全连接层</code></p>
<p><strong>换算计算量</strong>,一般一个参数是指一个float，也就是４个字节,1kb&#x3D;1024字节</p>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL13.png" alt="图 13"><br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL14.png" alt="图 14"><br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL15.png" alt="图 15">  </p>
]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>C, C++, Cmake</title>
    <url>/C/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>C, C++, Cmake的相关入门学习笔记，项目地址：<a href="https://github.com/Arrowes/C-coding">C-coding</a></p>
<span id="more"></span>

<h1 id="C"><a href="#C" class="headerlink" title="C"></a>C</h1><p><a href="https://img.anfulai.cn/bbs/94810/C%20Primer%20Plus(%E7%AC%AC%E5%85%AD%E7%89%88)%E4%B8%AD%E6%96%87%E7%89%88.pdf">C Primer Plus(第六版)中文版</a></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">system(<span class="string">&quot;read -p &#x27;Press Enter to continue...&#x27; var&quot;</span>);<span class="comment">//linux按任意键继续命令</span></span><br><span class="line">system(<span class="string">&quot;clear&quot;</span>);<span class="comment">//linux清屏操作</span></span><br><span class="line"></span><br><span class="line">system(<span class="string">&quot;pause&quot;</span>); <span class="comment">//windows请按任意键继续</span></span><br><span class="line">system(<span class="string">&quot;cls&quot;</span>);<span class="comment">//windows清屏操作</span></span><br></pre></td></tr></table></figure>




<hr>
<hr>
<h1 id="C-1"><a href="#C-1" class="headerlink" title="C++"></a>C++</h1><p><a href="https://github.com/Arrowes/C-coding/blob/main/C%2B%2B/21%E5%A4%A9%E5%AD%A6%E9%80%9AC%2B%2B%E7%AC%AC8%E7%89%88%20%E9%AB%98%E6%B8%85%E5%AE%8C%E6%95%B4PDF.pdf">21天学通C++第8版</a></p>
<h2 id="1-绪论"><a href="#1-绪论" class="headerlink" title="1.绪论"></a>1.绪论</h2><p>C++最初由 Bjarne Stroustroup 于 1979 年在贝尔实验室开发，旨在作为 C 语言的继任者。但不同于C 语言，C++是一种面向对象的语言，实现了继承、抽象、多态和封装等概念。<br>C++是一种中级编程语言，这意味着使用它既可以高级编程方式编写应用程序，又可以低级编程方式编写与硬件紧密协作的库。</p>
<p>构建可执行文件：编写代码(.cpp) &gt; 编译器(.o &#x2F; .obj) &gt; 链接器(.exe)</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span>  <span class="comment">//标准头文件，引入std::cout</span></span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Hello World!!&quot;</span> &lt;&lt; std::endl; <span class="comment">// &lt;&lt;：流插入运算符</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  <span class="comment">//cout 是在名称空间 std 中定义的一个流, 用来显示</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>开发环境</strong>：Visual studio code + GCC compiler&#x2F;MinGW, 按F5 Choose <code>C/C++: g++.exe build and debug active file</code>, 将编译、链接并执行应用程序</p>
<h2 id="2-C-程序的组成部分"><a href="#2-C-程序的组成部分" class="headerlink" title="2.C++程序的组成部分"></a>2.C++程序的组成部分</h2><ul>
<li>预处理器编译指令 <code>#include</code><br> <code>#include &quot;...relative path to .\FileB&quot;</code>包含自定义头文件，<code>&lt;&gt;</code>用来包含自定义头文件 </li>
<li>程序主体 <code>main()</code><br>程序的起点，前面的int是一种标准化约定，表示返回类型为整数</li>
<li>返回值<br>在 C++中，除非明确声明了不返回值，否则函数必须返回一个值，根据约定，程序员在程序运行成功时返回 0，并在出现错误时返回−1</li>
</ul>
<p><strong>namespace名称空间</strong>：是给代码指定的名称，有助于降低命名冲突的风险，如<code>std::cout</code>:调用名称空间 std 中独一无二的 cout, 若要省略std::, 先加入<code>using namespace std</code></p>
<p><strong>注释</strong>：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//单行注释</span></span><br><span class="line"><span class="comment">/* 跨行</span></span><br><span class="line"><span class="comment">   注释</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>
<p>cin 可用于从用户那里获取文本输入和数字输入<code>std::cin &gt;&gt; Variable1 &gt;&gt; Variable2; </code></p>
<h2 id="3-使用变量和常量"><a href="#3-使用变量和常量" class="headerlink" title="3.使用变量和常量"></a>3.使用变量和常量</h2><h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><p><code>VariableType VariableName = InitialValue;</code><br>变量类型向编译器指出了变量可存储的数据的性质，编译器将为变量预留必要的空间。变量名由程序员选择，它替代了变量值在内存中的存储地址;<br>函数内部声明的为<em>局部变量</em>，作用域为局部，被限定在声明它的函数内，函数结束后，将销毁所有局部变量，并归还它们占用的内存；在函数外部声明的则为<em>全局变量</em>。</p>
<p>命名约定：对于变量名，采用<em>骆驼拼写法</em>(firstNumber, 第一个单词的首字母采用小写)，而对于诸如函数名等其他元素，采用 <em>Pascal 拼写法</em>(MultiplyNumbers(), 函数名每个首字母都大写)。</p>
<p>编译器支持的常见 C++变量类型:</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>值</th>
<th>概念</th>
</tr>
</thead>
<tbody><tr>
<td>bool</td>
<td>true&#x2F;false</td>
<td>布尔变量</td>
</tr>
<tr>
<td>char</td>
<td>256个字符值</td>
<td>存储单字符,如’A’</td>
</tr>
<tr>
<td>unsigned short int</td>
<td>0～65535</td>
<td>占16位内存&#x3D;$2^{16}$&#x3D;65536</td>
</tr>
<tr>
<td>short int</td>
<td>–32768～32767</td>
<td>最高有效位（MSB）做符号位</td>
</tr>
<tr>
<td>unsigned long int</td>
<td>0～4294967295</td>
<td>$2^{32}$</td>
</tr>
<tr>
<td>long int</td>
<td>–2147483648～2147483647</td>
<td></td>
</tr>
<tr>
<td>int (16位)</td>
<td>–32768～32767</td>
<td></td>
</tr>
<tr>
<td>int (32位)</td>
<td>–2147483648～2147483647</td>
<td></td>
</tr>
<tr>
<td>unsigned int（16位）</td>
<td>0～65535</td>
<td></td>
</tr>
<tr>
<td>unsigned int（32位）</td>
<td>0～4294967295</td>
<td></td>
</tr>
<tr>
<td>float</td>
<td>1.2e–38～3.4e38</td>
<td>浮点数</td>
</tr>
<tr>
<td>double</td>
<td>2.2e–308～1.8e308</td>
<td>双精度浮点数</td>
</tr>
</tbody></table>
<p>sizeof 确定变量长度（字节）：<code>sizeof (int)</code><br>使用列表初始化避免缩窄转换错误：<code>int anotherNum&#123; largeNum &#125;;</code><br>关键字 auto 自动推断类型：<code>auto coinFlippedHeads = true</code><br>typedef 替换变量类型: <code>typedef unsigned int MYINT; </code></p>
<h3 id="常量"><a href="#常量" class="headerlink" title="常量"></a>常量</h3><p>在 C++中，常量类似于变量，只是不能修改。</p>
<ul>
<li>字面常量：可以是任何类型：布尔型、整型、字符串等</li>
<li>使用关键字 const 将变量声明为常量(最实用): <code>const double pi = 22.0 / 7;</code></li>
<li>使用 constexpr 定义常量表达式: <code>constexpr double GetPi() &#123;return 22.0 / 7;&#125; </code></li>
<li>使用关键字 enum 声明枚举: 指定一组特定的取值，枚举量起始值默认为0</li>
<li><del>使用#define 定义常量：<code>#define pi 3.14286</code>, 已被摒弃</del></li>
</ul>
<p>务必确保变量名阐述了变量的用途。<br>务必对变量进行初始化，确保变量包含非随机的确定值；并使用列表初始化来避免缩窄转换错误。<br>不要将保留的 C++关键字用作变量名，因为这将导致程序无法通过编译。</p>
<h2 id="4-管理数组和字符串"><a href="#4-管理数组和字符串" class="headerlink" title="4.管理数组和字符串"></a>4.管理数组和字符串</h2><h3 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h3><p>在 C++中，数组让您能够按顺序将一系列相同类型的数据存储到内存中。</p>
<p><strong>静态数组：</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> myNumbers [<span class="number">5</span>] = &#123;&#125;; <span class="comment">//声明一个包含 5 个 int 元素的数组，并将每个元素都初始化为零</span></span><br><span class="line"><span class="type">char</span> myCharacters [<span class="number">5</span>];  <span class="comment">//定义一个包含 5 个字符的数组</span></span><br><span class="line">Num = myNumbers [<span class="number">0</span>];    <span class="comment">//取出第一个元素 </span></span><br><span class="line">myNumbers [<span class="number">3</span>] = <span class="number">2023</span>;   <span class="comment">//重新赋值</span></span><br><span class="line"><span class="comment">//Bytes consumed by an array = sizeof(element-type) * Number of Elements </span></span><br><span class="line"><span class="comment">//务必初始化数组，否则其元素将包含未知值。使用数组时，务必确保在其边界内。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//在 C++中，可在内存中模拟多维数组(但存储数组的内存是一维的)</span></span><br><span class="line"><span class="type">int</span> <span class="built_in">array</span> [<span class="number">2</span>][<span class="number">3</span>] = &#123;&#123;<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>&#125;, &#123;<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125;&#125;;  <span class="comment">//or &#123;0, 1, 2, 3, 4, 5&#125;</span></span><br><span class="line">Num1 = <span class="built_in">array</span> [<span class="number">0</span>][<span class="number">1</span>]  <span class="comment">//取出元素1</span></span><br></pre></td></tr></table></figure>
<p><strong>动态数组</strong> <code>std::vector</code>：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="type">int</span>&gt; <span class="title function_">dynArray</span> <span class="params">(<span class="number">3</span>)</span>; <span class="comment">//这个矢量能动态地调整其长度，以存储更多数据，且无需初始化</span></span><br></pre></td></tr></table></figure>
<h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><p>C 风格字符串（危险）：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Hello World&quot;</span>; </span><br><span class="line"><span class="comment">//等同于：</span></span><br><span class="line"><span class="type">char</span> sayHello[] = &#123;<span class="string">&#x27;H&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;W&#x27;</span>, <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;\0&#x27;</span>&#125;;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; sayHello</span><br><span class="line"><span class="comment">//空字符‘\0’,也被称为字符串结束字符，告诉编译器字符串到此结束。不算长度</span></span><br><span class="line"><span class="comment">//如果没有在字符数组末尾添加空字符，可能跨越字符数组的边界,被称为缓冲区溢出</span></span><br></pre></td></tr></table></figure>
<p><strong>C++字符串</strong>：使用 <code>std::string</code>:<br>使用 C++标准字符串是更高效、更安全的方式。不同于字符数组（C 风格字符串实现），std::string 是动态的</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span> </span></span><br><span class="line"><span class="built_in">string</span> <span class="title function_">greetString</span> <span class="params">(<span class="string">&quot;Hello std::string!&quot;</span>)</span>;</span><br></pre></td></tr></table></figure>

<h2 id="5-使用表达式、语句和运算符"><a href="#5-使用表达式、语句和运算符" class="headerlink" title="5.使用表达式、语句和运算符"></a>5.使用表达式、语句和运算符</h2><p>从本质上说，程序是一组按顺序执行的命令。这些命令为表达式和语句，使用运算符执行特定的计算或操作。</p>
<ul>
<li>语句：分号界定了语句的边界; 要将一条语句放到两行中，可在第一行末尾添加反斜杠<code>\</code>; 可使用花括号（{}）将多条语句组合在一起，以创建复合语句（语句块）</li>
<li>运算符：<ul>
<li><code>=</code> 赋值运算符，左值通常是内存单元，右值可以是内存单元的内容。</li>
<li><code>+ - * / %</code> 求模运算符%返回除法运算的余数，只能用于整数</li>
<li><code>++ --</code> 递增和递减运算符，分为前缀与后缀： <figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> num = ++num;  <span class="comment">//前缀，先＋再赋值</span></span><br><span class="line"><span class="type">int</span> num = num++;  <span class="comment">//后缀，先赋值再+</span></span><br></pre></td></tr></table></figure></li>
<li><code>== !=</code>   相等性检查的结果为布尔值，即 true 或 false, 1 &#x2F; 0</li>
<li><code>&lt; &gt; &lt;= &gt;=</code></li>
<li>逻辑运算符(返回布尔值)： <figure class="highlight c"><table><tr><td class="code"><pre><span class="line">!  <span class="comment">//NOT 用于单个操作数，用于反转</span></span><br><span class="line">&amp;&amp; <span class="comment">//AND, 2true则true</span></span><br><span class="line">|| <span class="comment">//OR, 1true就true</span></span><br><span class="line">^  <span class="comment">//XOR异或，1true才true</span></span><br></pre></td></tr></table></figure></li>
<li>按位运算符（返回运算结果）：<code>~  &amp;  |  ^</code></li>
<li>移位运算符,用途之一是将数据乘以或除以 $2^n$ <figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> halfNum = inputNum &gt;&gt; <span class="number">1</span>;</span><br><span class="line"><span class="type">int</span> quadrupleNum = inputNum &lt;&lt; <span class="number">2</span>;</span><br></pre></td></tr></table></figure></li>
<li>复合赋值运算符,将运算结果赋给左边的操作数 <code>num1 += num2;</code></li>
<li>运算符 <code>sizeof</code>, 确定变量占用的内存量</li>
</ul>
</li>
</ul>
<p>运算符优先级,C++标准非常严格地指定了各种运算的执行顺序:<br><code>int myNumber = 10 * 30 + 20 – 5 * 5 &lt;&lt; 2;</code> 应写作 <code>int myNumber = ((10 * 30) – (5 * 5) + 20) &lt;&lt; 2;</code>，使用括号让代码和表达式易于理解</p>
<h2 id="6-控制程序流程"><a href="#6-控制程序流程" class="headerlink" title="6.控制程序流程"></a>6.控制程序流程</h2><h3 id="条件执行"><a href="#条件执行" class="headerlink" title="条件执行"></a>条件执行</h3><p><code>if...else</code>  条件不为0就被视为true<br><code>if...else if...else</code><br><code>switch-case</code>  条件处理,相比if-else-if结构化程度更高</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">switch</span>(expression)   </span><br><span class="line">&#123; <span class="comment">//计算 expression 的值，并将其与每个 case 标签进行比较</span></span><br><span class="line"><span class="keyword">case</span> LabelA:   <span class="comment">//务必将枚举量用作 case 标签，以提高代码的可读性。</span></span><br><span class="line"> DoSomething; </span><br><span class="line"> <span class="keyword">break</span>;  <span class="comment">//退出当前代码块</span></span><br><span class="line"><span class="comment">// And so on... </span></span><br><span class="line"><span class="keyword">default</span>: </span><br><span class="line"> DoStuffWhenExpressionIsNotHandledAbove; </span><br><span class="line"> <span class="keyword">break</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>?:</code> 条件运算符&#x2F;三目运算符, 相当于紧凑的 if-else 结构<br><code>(conditional expression evaluated to bool) ? expression1 if true : expression2  if false; </code></p>
<h3 id="循环执行"><a href="#循环执行" class="headerlink" title="循环执行"></a>循环执行</h3><p><code>goto</code>   (避免使用 goto，可防止代码不直观且难以维护。)</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">SomeFunction() </span><br><span class="line">&#123;  <span class="comment">//不推荐使用 goto 语句来编写循环</span></span><br><span class="line">Start: <span class="comment">// Called a label </span></span><br><span class="line"> CodeThatRepeats; </span><br><span class="line"> <span class="keyword">goto</span> Start; </span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<p><code>while</code> 只要条件为 true，就将反复执行该语句块</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span>(expression) </span><br><span class="line">&#123; </span><br><span class="line"> <span class="comment">// Expression evaluates to true </span></span><br><span class="line"> StatementBlock; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>do...while</code> 循环逻辑至少执行一次时</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">do</span> </span><br><span class="line">&#123; </span><br><span class="line"> StatementBlock; <span class="comment">// executed at least once </span></span><br><span class="line">&#125; <span class="keyword">while</span>(condition); <span class="comment">// ends loop if condition evaluates to false </span></span><br></pre></td></tr></table></figure>
<p><code>for</code></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (初始化语句 executed only once;  <span class="comment">//迭代器</span></span><br><span class="line"> 条件表达式 executed at the beginning of every loop; </span><br><span class="line"> 修改变量 executed at the end of every loop) </span><br><span class="line">&#123; <span class="comment">//以上三项都是可选的</span></span><br><span class="line"> DoSomething; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>C++11引入了<em>基于范围的 for 循环</em>，让对一系列值（如数组包含的值）进行操作的代码更容易编写和理解。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">char</span> charArray[] = &#123; <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;o&#x27;</span> &#125;; </span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> aChar : charArray) </span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; aChar &lt;&lt; <span class="string">&#x27; &#x27;</span>; </span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;  <span class="comment">//h e l l o </span></span><br></pre></td></tr></table></figure>
<p><code>continue</code> 能够跳转到循环开头，跳过循环块中后面的代码;<br><code>break</code> 退出循环块，即结束当前循环。</p>
<p>控制无限循环</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (;;) <span class="comment">// no condition supplied = unending for </span></span><br><span class="line">&#123; </span><br><span class="line"> DoSomethingRepeatedly; </span><br><span class="line"> <span class="keyword">if</span>(expression) </span><br><span class="line"> <span class="keyword">break</span>; <span class="comment">// 使用 break 退出无限 for 循环</span></span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>

<h2 id="7-使用函数组织代码"><a href="#7-使用函数组织代码" class="headerlink" title="7.使用函数组织代码"></a>7.使用函数组织代码</h2><p>函数让您能够划分和组织程序的执行逻辑。通过使用函数，可将应用程序的内容划分成依次调用的逻辑块。</p>
<ul>
<li>声明函数原型<br><code>double Area(double radius)</code> &#x3D; 返回值类型 函数名(函数接受的参数列表)<br>函数可接受用逗号分隔的多个参数，但只能有一种返回类型。可以给多个参数指定默认值，但这些参数<em>必须位于参数列表的末尾</em>。</li>
<li>定义函数<br>函数定义由一个语句块组成。除非返回类型被声明为 void，否则函数必须包含一条 return 语句;</li>
<li>调用函数<br>如果函数声明中包含形参（parameter），调用函数时必须提供实参（argument）</li>
</ul>
<p>递归函数：调用自己，必须有明确的退出条件<br>多条 return 语句的函数：可使用 return 语句退出</p>
<h3 id="函数数据处理"><a href="#函数数据处理" class="headerlink" title="函数数据处理"></a>函数数据处理</h3><p>函数重载：名称和返回类型相同，但参数不同的函数被称为重载函数。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">double</span> <span class="title function_">Area</span><span class="params">(<span class="type">double</span> radius)</span>; <span class="comment">// for circle </span></span><br><span class="line"><span class="type">double</span> <span class="title function_">Area</span><span class="params">(<span class="type">double</span> radius, <span class="type">double</span> height)</span>; <span class="comment">// for cylinder</span></span><br><span class="line"><span class="comment">//根据不同的输入使用不同的函数，实现不同的功能</span></span><br></pre></td></tr></table></figure>
<p>数组传递给函数: <code>void DisplayIntegers(int[] numbers, int Length); </code></p>
<p><code>&amp;</code> 按引用传递参数, 让函数修改的变量在其外部（如调用函数）中也可用(详见第八章引用)：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">Area</span><span class="params">(<span class="type">double</span> radius, <span class="type">double</span> &amp;result)</span> </span><br><span class="line"><span class="comment">//此时，result是指向调用函数中相应变量的引用，而不是其拷贝</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>：...</span><br><span class="line">   <span class="title function_">Area</span><span class="params">(radius, areaFetched)</span>; </span><br><span class="line"><span class="comment">//Area( )中的变量 result，与 main( )中的 double areaFetched 指向同一个内存单元。</span></span><br></pre></td></tr></table></figure>
<h3 id="微处理器如何处理函数调用"><a href="#微处理器如何处理函数调用" class="headerlink" title="微处理器如何处理函数调用"></a>微处理器如何处理函数调用</h3><p><em>函数调用</em> 在微处理器中的过程：跳转到属于被调用函数的下一条指令处执行。执行完函数的指令后，返回到最初<em>离开的地方</em>;<br>因此，编译器将函数调用转换为一条供微处理器执行的 <code>CALL</code> 指令, 指出接下来要获取的指令所在的地址，该地址归函数所有。遇到 <code>CALL</code> 指令时，微处理器将调用函数后将要执行的指令的位置保存到 <strong>栈</strong> 中，再跳转到 <code>CALL</code> 指令包含的内存单元处。</p>
<blockquote>
<p>栈是一种后进先出的内存结构，将数据加入栈被称为压入操作, 从栈中取出数据被称为弹出操作。栈增大时，栈指针将不断递增，始终指向栈顶;<br>栈的性质使其非常适合用于处理函数调用。函数被调用时，所有局部变量都在栈中实例化，即被压入栈中。函数执行完毕时，这些局部变量都从栈中弹出，栈指针返回到原来的地方。<br>如：微处理器执行CALL指令指出的内存单元包含属于函数的指令，直到 RET 语句（return 语句对应的微处理器代码）导致微处理器从栈中弹出执行 CALL 指令时存储的地址。该地址包含调用函数中接下来要执行的语句的位置</p>
</blockquote>
<p><strong>内联函数</strong><br>使用关键字 inline 发出请求，要求在函数被调用时就地展开它们：<code>inline double GetPi() </code> 编译器通常将该关键字视为请求，请求将函数 GetPi()的内容直接放到调用它的地方，以提高代码的执行速度(因为执行函数调用的开销可能非常高)，仅当函数非常简单，需要降低其开销时，才应使用该关键字<br>(根据性能设置，大多数较新的编译器都能判断应内联哪些函数，进而为程序员这样做)</p>
<p><strong>自动推断返回类型</strong>: <code> auto Area(double radius)</code></p>
<p><strong>lambda 函数</strong>: <code>[optional parameters](parameter list)&#123; statements; &#125; </code><br>lambda函数是 C++11 引入的，有助于使用 STL 算法对数据进行排序或处理，可以在需要函数对象的地方使用，用于简化代码和提高可读性。</p>
<h2 id="8-阐述指针和引用"><a href="#8-阐述指针和引用" class="headerlink" title="8.阐述指针和引用"></a>8.阐述指针和引用</h2><p>C++最大的优点之一是，既可使用它来编写不依赖于机器的高级应用程序，又可使用它来编写与硬件紧密协作的应用程序。能够在字节和比特级调整应用程序的性能。要编写高效地利用系统资源的程序，理解<em>指针和引用</em>是必不可少的一步。</p>
<h3 id="指针"><a href="#指针" class="headerlink" title="指针 *"></a>指针 *</h3><p><strong>指针是存储内存地址的变量</strong>，是一种指向内存单元的特殊变量。<br>（内存单元地址通常使用十六进制表示法）</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span>* pointsToInt = <span class="literal">NULL</span>;   <span class="comment">//声明指针并初始化，务必初始化指针变量，否则它将包含垃圾值。</span></span><br><span class="line"><span class="comment">//例如int在内存中的地址为0x002EFB34，则占用 0x002EFB34～0x002EFB37</span></span><br><span class="line">&amp;pointsToInt      <span class="comment">//引用运算符（&amp;）, 也叫地址运算符，用来获取变量的地址。</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span>* pointsToInt = &amp;age;   <span class="comment">//使用指针存储地址(age是int变量)</span></span><br><span class="line"><span class="type">int</span> dogsAge = <span class="number">9</span>; </span><br><span class="line">pointsToInt = &amp;dogsAge;    <span class="comment">//同一个 int 指针可指向任何 int 变量</span></span><br><span class="line"></span><br><span class="line">++pointsToInt     <span class="comment">//将指向下一个int, Address + sizeof(int)</span></span><br><span class="line"></span><br><span class="line">*pointsToInt              <span class="comment">//解除引用运算符（*）,也叫间接运算符, 访问指向的数据</span></span><br><span class="line"><span class="built_in">cin</span> &gt;&gt; *pointsToInt;      <span class="comment">//使用 * 操纵数据</span></span><br></pre></td></tr></table></figure>
<p><strong>动态内存分配 new delete</strong><br>静态数组的长度是固定的，不能根据应用程序的需求增大或缩小, 因此使用 new 和 delete 动态地分配和释放内存</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span>* pointToAnInt = new <span class="type">int</span>;  <span class="comment">//给整型分配内存（int* Pointer = new int[10]; 为一系列元素分配内存</span></span><br><span class="line">delete pointToAnInt;          <span class="comment">//释放内存（delete[] Pointer; </span></span><br><span class="line"><span class="comment">//如果不释放，会造成内存泄露</span></span><br></pre></td></tr></table></figure>
<p>运算符 new 和 delete 分配和释放自由存储区中的内存。自由存储区是一种内存抽象，表现为一个内存池，应用程序可分配（预留）和释放其中的内存。</p>
<p><strong>将关键字 const 用于指针</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> Age=<span class="number">23</span>;</span><br><span class="line"><span class="type">int</span>* <span class="type">const</span> point = &amp;Age;         <span class="comment">//指针包含的地址是常量，不能修改，但可修改指针指向的数据</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span>* point = &amp;Age;         <span class="comment">//指针指向的数据为常量，不能修改，但可以修改指针包含的地址，即指针可以指向其他地方</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span>* <span class="type">const</span> point = &amp;Age;   <span class="comment">//指针包含的地址以及它指向的值都是常量，不能修改（这种组合最严格）</span></span><br><span class="line"><span class="comment">//函数参数应声明为最严格的 const 指针</span></span><br><span class="line"><span class="comment">//**将指针传递给函数**: 指针是一种将内存空间传递给函数的有效方式，其中可包含函数完成其工作所需的数据，也可包含操作结果。</span></span><br></pre></td></tr></table></figure>
<p>数组变量是指向第一个元素的指针, 类似于在固定内存范围内发挥作用的指针，因此也可将用于指针的解除引用运算符（*）用于数组</p>
<p><strong>使用指针相关错误</strong></p>
<ul>
<li>内存泄漏：new动态分配的内存没有用delete释放</li>
<li>无效指针：务必确保指针指向了有效的内存单元, 否则使用 * 和 delete 时会崩溃</li>
<li>悬浮指针：使用 delete 释放后，任何有效指针都将无效，很多程序员在初始化指针或释放指针后将其设置为 NULL，并在使用运算符 * 对指针解除引用前检查它是否有效（将其与 NULL 比较）</li>
<li>new内存分配失败：大块内存分配请求不一定能成功，失败会引发 <code>std::bad_alloc</code> 异常并中断执行<br>（<code>try-catch</code> 异常处理结构让程序能够向用户指出这一点，再正常退出；或可使用 new 变种 <code>new(nothrow)</code>，在内存分配失败时不引发异常，而返回 NULL，让您能够在使用指针前检查其有效性）</li>
</ul>
<h3 id="引用"><a href="#引用" class="headerlink" title="引用 &amp;"></a>引用 &amp;</h3><p>引用运算符（&amp;）, 也叫地址运算符，用来获取变量的地址。<br>引用是变量的别名，只是另一种访问相应变量存储的数据的方式。直接调用，避免将形参复制给形参，减少复制步骤的开销，极大地提高性能</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> original = <span class="number">20</span>;</span><br><span class="line"><span class="type">int</span>&amp; ref = original; <span class="comment">//指向相应变量所在的内存单元</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//可避免复制步骤的函数</span></span><br><span class="line">ReturnType <span class="title function_">DoSomething</span><span class="params">(Type&amp; parameter)</span>;     <span class="comment">//Parameter 不再是 argument 的拷贝，而是它的别名</span></span><br><span class="line">ReturnType Result = DoSomething(argument);   <span class="comment">//argument 是按引用传递的</span></span><br><span class="line"><span class="comment">//函数直接使用调用者栈中的数据</span></span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span>&amp; constRef = original;  <span class="comment">//使禁止通过引用修改它指向的变量的值</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">GetSquare</span><span class="params">(<span class="type">const</span> <span class="type">int</span>&amp; number, <span class="type">int</span>&amp; result)</span>  <span class="comment">//const 引用将参数标识为输入参数</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">GetSquare</span><span class="params">(<span class="type">const</span> <span class="type">int</span>* <span class="type">const</span> number, <span class="type">int</span>* <span class="type">const</span> result)</span>  <span class="comment">//效果同上，但指针不同于引用，可能为 NULL 或无效，因此使用前必须核实它们是有效的</span></span><br></pre></td></tr></table></figure>

<h2 id="9-类和对象"><a href="#9-类和对象" class="headerlink" title="9.类和对象"></a>9.类和对象</h2><p><u>现在开始面向对象</u></p>
<h3 id="类和对象"><a href="#类和对象" class="headerlink" title="类和对象"></a>类和对象</h3><p>将一系列数据和函数整合在一起的结构就是<strong>类</strong>,让您能够创建自己的数据类型，并在其中封装属性和使用它们的函数。<br>(<em>封装指的是将数据以及使用它们的函数进行逻辑编组，这是面向对象编程的重要特征</em>)</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//声明类, 使用关键字class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Human</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">   <span class="built_in">string</span> name;</span><br><span class="line">   <span class="built_in">string</span> age;</span><br><span class="line">   <span class="type">void</span> <span class="title function_">Talk</span><span class="params">()</span></span><br><span class="line">   ...</span><br><span class="line">&#125;；         <span class="comment">// ;结尾</span></span><br></pre></td></tr></table></figure>

<p>在程序执行阶段，<strong>对象</strong>是类的化身。要使用类的功能，通常需要创建其实例—对象，并通过对象访问成员方法和属性。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建 Human 的对象</span></span><br><span class="line">Human Man;  <span class="comment">//Man是Human类的对象，是运行阶段的化身</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//可使用 new 为 Human 对象动态地分配内存</span></span><br><span class="line">Human* Woman = new Human(); <span class="comment">// dynamically allocated Human </span></span><br><span class="line">delete Woman; <span class="comment">// de-allocating memory </span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>句点运算符 (.) 用于访问对象的属性</p>
 <figure class="highlight c"><table><tr><td class="code"><pre><span class="line">Man.age= <span class="string">&quot;23&quot;</span>;</span><br><span class="line">Man.Talk();</span><br><span class="line"></span><br><span class="line">Human* Woman = new Human(); </span><br><span class="line">(*Woman).Talk();</span><br></pre></td></tr></table></figure>
</li>
<li><p>指针运算符（-&gt;）访问成员</p>
 <figure class="highlight c"><table><tr><td class="code"><pre><span class="line">Human* Woman = new Human(); </span><br><span class="line">Woman-&gt;age = <span class="string">&quot;22&quot;</span>;</span><br><span class="line">Woman-&gt;Talk();</span><br><span class="line">delete Woman;</span><br></pre></td></tr></table></figure>
<p>例：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Human</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">public:</span><br><span class="line">   <span class="built_in">string</span> name;</span><br><span class="line">   <span class="type">int</span> age;</span><br><span class="line">   <span class="type">void</span> <span class="title function_">Talk</span><span class="params">()</span></span><br><span class="line">   &#123; <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;I am &quot;</span> + name &lt;&lt;<span class="string">&quot;, &quot;</span> &lt;&lt; age &lt;&lt;   <span class="string">&quot; years old&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">   Human Man;</span><br><span class="line">   Man.name = <span class="string">&quot;wyj&quot;</span>;</span><br><span class="line">   Man.age = <span class="number">23</span>;</span><br><span class="line"></span><br><span class="line">   Human Woman;</span><br><span class="line">   Woman.name = <span class="string">&quot;girl&quot;</span>;</span><br><span class="line">   Woman.age = <span class="number">22</span>;</span><br><span class="line"></span><br><span class="line">   Man.Talk();</span><br><span class="line">   Woman.Talk();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//output:</span></span><br><span class="line">I am wyj, <span class="number">23</span> years old</span><br><span class="line">I am girl, <span class="number">22</span> years old</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="关键字-public-和-private"><a href="#关键字-public-和-private" class="headerlink" title="关键字 public 和 private"></a>关键字 public 和 private</h3><p>在面向对象编程语言中，抽象是一个非常重要的概念, 作为类的设计者，使用 C++关键字 public 和 private 来指定哪些部分可从外部（如 main( )）访问，哪些部分不能。<br>private私有属性和方法，访问和修改的唯一的途径就是通过类的public公有方法，这个以编写类的程序员认为的合适方式暴露。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Human</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line">private:</span><br><span class="line">   <span class="type">int</span> age; </span><br><span class="line"></span><br><span class="line">public: </span><br><span class="line">   <span class="type">void</span> <span class="title function_">SetAge</span><span class="params">(<span class="type">int</span> humansAge)</span> </span><br><span class="line">   &#123; </span><br><span class="line">      <span class="keyword">if</span> (humansAge &gt; <span class="number">0</span>) </span><br><span class="line">      age = humansAge; </span><br><span class="line">   &#125; </span><br><span class="line"></span><br><span class="line">   <span class="type">int</span> <span class="title function_">GetAge</span><span class="params">()</span></span><br><span class="line">   &#123;</span><br><span class="line">      <span class="keyword">if</span>(age &gt; <span class="number">30</span>)</span><br><span class="line">         <span class="keyword">return</span>(age - <span class="number">2</span>)   <span class="comment">//隐藏实际数据</span></span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">         <span class="keyword">return</span> age;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;; </span><br></pre></td></tr></table></figure>

<h3 id="构造函数"><a href="#构造函数" class="headerlink" title="构造函数"></a>构造函数</h3><p>构造函数是一种特殊的函数，它与类同名且不返回任何值，在实例化对象时被调用。</p>
<p><strong>声明和实现</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//在类声明中实现</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Human</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line">public: </span><br><span class="line">   Human()  </span><br><span class="line">   &#123; </span><br><span class="line">   <span class="comment">// constructor code here </span></span><br><span class="line">   &#125; </span><br><span class="line">&#125;；</span><br><span class="line"></span><br><span class="line"><span class="comment">//在类声明外实现</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Human</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line">public: </span><br><span class="line"> Human(); <span class="comment">// constructor declaration </span></span><br><span class="line">&#125;; </span><br><span class="line"><span class="comment">// constructor implementation (definition) </span></span><br><span class="line">Human::Human() <span class="comment">//::被称为作用域解析运算符。例如，Human::dateOfBirth 指的是在 Human 类中声明的变量 dateOfBirth，而::dateOfBirth 表示全局作用域中的变量 dateOfBirth</span></span><br><span class="line">&#123; </span><br><span class="line"> <span class="comment">// constructor code here </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>构造函数总是在创建对象时被调用</strong>，这让构造函数是将类成员变量（int、指针等）<strong>初始化为选定值</strong>的理想场所。<br>与函数一样，构造函数也可重载，创建对象时提供不同的参数会调用不同的构造函数，（<em>可在不提供参数的情况下调用的构造函数被称为<strong>默认构造函数</strong></em>）</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Human</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line">private: </span><br><span class="line"> <span class="built_in">string</span> name; </span><br><span class="line"> <span class="type">int</span> age; </span><br><span class="line"></span><br><span class="line">public: </span><br><span class="line"> Human(<span class="built_in">string</span> humansName, <span class="type">int</span> humansAge = <span class="number">25</span>) </span><br><span class="line"> &#123; </span><br><span class="line"> name = humansName; </span><br><span class="line"> age = humansAge; </span><br><span class="line"> <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Overloaded constructor creates &quot;</span> &lt;&lt; name; </span><br><span class="line"> <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot; of age &quot;</span> &lt;&lt; age &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line"> &#125; </span><br><span class="line"> <span class="comment">// ... other members </span></span><br><span class="line">&#125;; </span><br><span class="line"></span><br><span class="line"><span class="comment">//实例化这个类时，可使用下面的语法：</span></span><br><span class="line">Human <span class="title function_">adam</span><span class="params">(<span class="string">&quot;Adam&quot;</span>)</span>; <span class="comment">// adam.age is assigned a default value 25 </span></span><br><span class="line">Human <span class="title function_">eve</span><span class="params">(<span class="string">&quot;Eve&quot;</span>, <span class="number">18</span>)</span>; <span class="comment">// eve.age is assigned 18 as specified</span></span><br></pre></td></tr></table></figure>
<p>另一种初始化成员的方式是使用<strong>初始化列表</strong>, 冒号后面列出了各个成员变量及其初始值, 可以将上下代码对比着看：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Human</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line">private: </span><br><span class="line"> <span class="built_in">string</span> name; </span><br><span class="line"> <span class="type">int</span> age; </span><br><span class="line"></span><br><span class="line">public: </span><br><span class="line"> <span class="comment">// two parameters to initialize members age and name </span></span><br><span class="line"> Human(<span class="built_in">string</span> humansName, <span class="type">int</span> humansAge) </span><br><span class="line"> :name(humansName), age(humansAge) </span><br><span class="line"> &#123; </span><br><span class="line"> <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Constructed a human called &quot;</span> &lt;&lt; name; </span><br><span class="line"> <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;, &quot;</span> &lt;&lt; age &lt;&lt; <span class="string">&quot; years old&quot;</span> &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line"> &#125; </span><br><span class="line"><span class="comment">// ... other class members </span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h3 id="析构函数"><a href="#析构函数" class="headerlink" title="析构函数"></a>析构函数</h3><p>也是一种特殊的函数，与类同名，但前面有一个腭化符号（～）<br>构造函数在实例化对象时被调用，而析构函数在<strong>对象销毁时</strong>自动被调用。</p>
<p><strong>声明和实现</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//在类声明中实现（定义）</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Human</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line">public: </span><br><span class="line"> ~Human() </span><br><span class="line"> &#123; </span><br><span class="line"> <span class="comment">// destructor code here </span></span><br><span class="line"> &#125; </span><br><span class="line">&#125;; </span><br><span class="line"></span><br><span class="line"><span class="comment">//在类声明外定义</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Human</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line">public: </span><br><span class="line"> ~Human(); <span class="comment">// destructor declaration </span></span><br><span class="line">&#125;; </span><br><span class="line"></span><br><span class="line">Human::~Human() </span><br><span class="line">&#123; </span><br><span class="line"> <span class="comment">// destructor code here </span></span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<p>每当对象不再在作用域内或通过 delete 被删除进而被销毁时，都将调用析构函数。这使得析构函数成为<em>重置</em>变量以及<em>释放</em>动态分配的内存和其他资源的理想场所<br>如：某个类中，在构造函数中new, 在析构函数中delete, 使该类不仅对程序员隐藏了内存管理实现，还正确地释放了分配的内存。<br>（<em>析构函数不能重载</em>）</p>
<h3 id="复制构造函数"><a href="#复制构造函数" class="headerlink" title="复制构造函数"></a>复制构造函数</h3><p>浅复制的问题：<em>复制类的对象时</em>，将复制其指针成员，但不复制指针指向的缓冲区，其结果是两个对象指向同一块动态分配的内存。销毁其中一个对象时，delete[]释放这个内存块，导致另一个对象存储的指针拷贝无效。这种复制被称为浅复制，会威胁程序的稳定性</p>
<p>因此使用复制构造函数确保<em>深复制</em>，这是一个重载的构造函数，每当<strong>对象被复制时</strong>，编译器都将调用复制构造函数。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyString</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line">   private:...</span><br><span class="line">   public:</span><br><span class="line">   MyString(<span class="type">const</span> <span class="type">char</span>* initString)</span><br><span class="line">   ...</span><br><span class="line">   MyString(<span class="type">const</span> MyString&amp; copySource) <span class="comment">// copy constructor </span></span><br><span class="line">   &#123;  <span class="comment">//使用 const，可确保复制构造函数不会修改指向的源对象</span></span><br><span class="line">   <span class="comment">// Copy constructor implementation code </span></span><br><span class="line">   &#125;</span><br><span class="line">&#125;; </span><br><span class="line"></span><br><span class="line"><span class="comment">//以确保在main中函数调用时进行深复制</span></span><br><span class="line">MyString <span class="title function_">sayHello</span><span class="params">(<span class="string">&quot;Hello world！&quot;</span>)</span>;</span><br><span class="line">UseMyString(sayHello);  <span class="comment">//自动调用复制构造函数</span></span><br></pre></td></tr></table></figure>
<p>(<em>类包含原始指针成员（char* 等）时，务必编写复制构造函数和复制赋值运算符。<br>务必将类成员声明为 std::string 和智能指针类（而不是原始指针），因为它们实现了复制构造函数，可减少工作量。</em>)</p>
<p>移动构造函数 <code>MyString(MyString&amp;&amp; moveSource) </code>：编译器将自动使用它来“移动”临时资源，从而避免深复制</p>
<h3 id="构造函数和析构函数的其他用途"><a href="#构造函数和析构函数的其他用途" class="headerlink" title="构造函数和析构函数的其他用途"></a>构造函数和析构函数的其他用途</h3><p>禁止类对象被复制：声明一个私有的复制构造函数 <code>private: President(const President&amp;);</code></p>
<p>只能有一个实例的单例类：使用关键字 <code>static</code></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">President</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    private:</span><br><span class="line">        President() &#123;&#125;;  <span class="comment">// 私有的默认构造函数，防止外部创建实例</span></span><br><span class="line">        President(<span class="type">const</span> President&amp;);  <span class="comment">// 私有的拷贝构造函数，防止对象拷贝</span></span><br><span class="line">        <span class="type">const</span> President&amp; operator=(<span class="type">const</span> President);  <span class="comment">// 私有的赋值运算符重载，防止对象赋值</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">string</span> name;  <span class="comment">// 私有成员变量，用于存储总统的名字</span></span><br><span class="line"></span><br><span class="line">    public:</span><br><span class="line">        <span class="type">static</span> President&amp; <span class="title function_">GetInstance</span><span class="params">()</span>  <span class="comment">// 静态方法，用于获取唯一的实例</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">static</span> President onlyInstance;  <span class="comment">// 在首次调用时创建唯一实例</span></span><br><span class="line">            <span class="keyword">return</span> onlyInstance;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">string</span> <span class="title function_">GetName</span><span class="params">()</span>  <span class="comment">// 公有方法，用于获取总统的名字</span></span><br><span class="line">        &#123; <span class="keyword">return</span> name; &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">void</span> <span class="title function_">SetName</span><span class="params">(<span class="built_in">string</span> InputName)</span>  <span class="comment">// 公有方法，用于设置总统的名字</span></span><br><span class="line">        &#123; name = InputName; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    President&amp; onlyPresident = President::GetInstance();  <span class="comment">// 获取 President 实例的引用</span></span><br><span class="line">    onlyPresident.SetName(<span class="string">&quot;Abraham Lincoln&quot;</span>);  <span class="comment">// 设置总统名字</span></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;President is: &quot;</span> &lt;&lt; President::GetInstance().GetName() &lt;&lt; <span class="built_in">endl</span>;  <span class="comment">// 输出总统名字</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>禁止在栈中实例化的类(栈空间通常有限): 将析构函数声明为私有的</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MonsterDB</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line">private: </span><br><span class="line"> ~MonsterDB(); <span class="comment">// private destructor </span></span><br><span class="line"> <span class="comment">//... members that consume a huge amount of data </span></span><br><span class="line">&#125;; </span><br><span class="line">通过声明私有的析构函数，可禁止像下面这样创建实例：</span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span> </span><br><span class="line">&#123; </span><br><span class="line"> MonsterDB myDatabase; <span class="comment">// compile error </span></span><br><span class="line"> <span class="comment">// … more code </span></span><br><span class="line"> <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<p>使用构造函数进行类型转换：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Human</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line"> <span class="type">int</span> age;  <span class="comment">// 私有成员变量 age，表示人的年龄</span></span><br><span class="line">public: </span><br><span class="line"> Human(<span class="type">int</span> humansAge): age(humansAge) &#123;&#125;  <span class="comment">// 构造函数，接受人的年龄作为参数并初始化成员变量 age</span></span><br><span class="line">&#125;; </span><br><span class="line"></span><br><span class="line"><span class="comment">// Function that takes a Human as a parameter </span></span><br><span class="line"><span class="type">void</span> <span class="title function_">DoSomething</span><span class="params">(Human person)</span> </span><br><span class="line">&#123; </span><br><span class="line"> <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Human sent did something&quot;</span> &lt;&lt; <span class="built_in">endl</span>;  <span class="comment">// 输出信息</span></span><br><span class="line"> <span class="keyword">return</span>;  <span class="comment">// 返回</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line"> Human <span class="title function_">kid</span><span class="params">(<span class="number">10</span>)</span>;  <span class="comment">//利用构造函数显式转换：将整数 10 转换为 Human 类型对象</span></span><br><span class="line"> <span class="comment">//在这里，通过构造函数 Human(int humansAge) 创建了一个名为 kid 的 Human 类型对象，传递整数值 10 作为构造函数的参数。</span></span><br><span class="line"> <span class="comment">//这个构造函数被用来创建 Human 类型对象，并将整数 10 转换为 kid 的一个属性，即年龄</span></span><br><span class="line"> DoSomething(kid);  <span class="comment">// 调用 DoSomething 函数，将 kid 作为参数传递</span></span><br><span class="line"> <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"> <span class="comment">//隐式转换:</span></span><br><span class="line"> Human anotherKid = <span class="number">11</span>; <span class="comment">// int converted to Human </span></span><br><span class="line"> DoSomething(<span class="number">10</span>); <span class="comment">// 10 converted to Human! </span></span><br><span class="line">&#125;</span><br><span class="line"> <span class="comment">//使用关键字 explicit 可禁止隐式转换，使上面两行编译失败:</span></span><br><span class="line"> explicit <span class="title function_">Human</span><span class="params">(<span class="type">int</span> humansAge)</span>: <span class="title function_">age</span><span class="params">(humansAge)</span> &#123;&#125;</span><br></pre></td></tr></table></figure>

<p><strong>this 指针</strong> :  在类中，关键字 this 包含当前对象的地址,当您在类成员方法中调用其他成员方法时，编译器将隐式地传递 this 指针—函数调用中不可见的参数<br><code>Talk(&quot;Bla bla&quot;); // same as Talk(this, &quot;Bla Bla&quot;) </code><br>*this表示当前对象的指针。它是一个特殊的指针，指向类的实例或对象自身</p>
<p><strong>sizeof( )</strong> ： 指出类声明中所有数据属性占用的总内存量，单位为字节 （结果受字填充word padding和其他因素的影响）</p>
<p><strong>关键字 struct</strong> 来自 C 语言，在 C++编译器看来，它与类及其相似，差别在于程序员未指定时，默认的访问限定符（public 和 private）不同，不同于结构，类的成员默认为私有</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//C++ Class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Human</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">private:</span><br><span class="line">   <span class="type">int</span> age;</span><br><span class="line">   <span class="type">bool</span> gender;</span><br><span class="line">   MyString name;</span><br><span class="line"></span><br><span class="line">public:</span><br><span class="line">   Human(<span class="type">const</span> MyString&amp; InputName, <span class="type">int</span> InputAge, <span class="type">bool</span> InputGender)</span><br><span class="line">      : name(InputName), age (InputAge), gender(InputGender) &#123;&#125;</span><br><span class="line"></span><br><span class="line">   <span class="type">int</span> <span class="title function_">GetAge</span> <span class="params">()</span></span><br><span class="line">   &#123; <span class="keyword">return</span> age; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//C struct, 除非指定了，否则结构中的成员默认为公有的, 另外，除非指定了，否则结构以公有方式继承基结构</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Human</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line"> Human(<span class="type">const</span> MyString&amp; humansName, <span class="type">int</span> humansAge, <span class="type">bool</span> humansGender) </span><br><span class="line"> : name(humansName), age (humansAge), Gender(humansGender) &#123;&#125; </span><br><span class="line"></span><br><span class="line"> <span class="type">int</span> <span class="title function_">GetAge</span> <span class="params">()</span> </span><br><span class="line"> &#123; </span><br><span class="line"> <span class="keyword">return</span> age; </span><br><span class="line"> &#125; </span><br><span class="line"></span><br><span class="line">private: </span><br><span class="line"> <span class="type">int</span> age; </span><br><span class="line"> <span class="type">bool</span> gender; </span><br><span class="line"> MyString name; </span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//结构 Human 与类 Human 很像；结构的实例化与类的实例化也很像：</span></span><br><span class="line">Human <span class="title function_">firstMan</span><span class="params">(<span class="string">&quot;Adam&quot;</span>, <span class="number">25</span>, <span class="literal">true</span>)</span>; <span class="comment">// an instance of struct Human</span></span><br></pre></td></tr></table></figure>

<p><strong>声明友元</strong>： 使用关键字 friend ，从外部访问类的私有数据成员和方法</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">private: </span><br><span class="line">   friend <span class="type">void</span> <span class="title function_">DisplayAge</span><span class="params">(<span class="type">const</span> Human&amp; person)</span>; <span class="comment">//指出DisplayAge( )是 Human 类的友元，能够访问Human类的私有数据成员</span></span><br><span class="line">   <span class="comment">//friend class Utility; //指出 Utility 类是 Human 类的友元</span></span><br><span class="line">   ...</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">DisplayAge</span><span class="params">(<span class="type">const</span> Human&amp; person)</span> </span><br><span class="line">   &#123; </span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; person.age &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line">   &#125; </span><br></pre></td></tr></table></figure>

<p><strong>共用体</strong>：使用关键字 union声明，是一种特殊的类，每次只有一个非静态数据成员处于活动状态。在结构中，常使用共用体来模拟复杂的数据类型</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">union</span> <span class="title">UnionName</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line"> Type1 member1; </span><br><span class="line"> Type2 member2; </span><br><span class="line">…</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">//实例化并使用共用体：</span></span><br><span class="line">UnionName unionObject; </span><br><span class="line">unionObject.member2 = value; <span class="comment">// choose member2 as the active member</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//在结构中，常使用共用体来模拟复杂的数据类型</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span>  <span class="title">ComplexType</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">enum</span> <span class="title">DataType</span> //使用枚举来存储信息类型</span></span><br><span class="line"><span class="class">    &#123;</span></span><br><span class="line">        Int,</span><br><span class="line">        Char</span><br><span class="line">    &#125;Type;</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">union</span> <span class="title">Value</span>   //使用共用体来存储实际值</span></span><br><span class="line"><span class="class">    &#123;</span></span><br><span class="line">        <span class="type">int</span> num;</span><br><span class="line">        <span class="type">char</span> alphabet;</span><br><span class="line"></span><br><span class="line">        Value() &#123;&#125;</span><br><span class="line">        ~Value() &#123;&#125;</span><br><span class="line">    &#125;value;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p><strong>聚合初始化</strong>：即满足如下条件的类或结构为聚合类型，可作为一个整体进行初始化：只包含公有和非静态数据成员，而不包含私有或受保护的数据成员；不包含任何虚成员函数；只涉及公有继承（不涉及私有、受保护和虚拟继承）；不包含用户定义的构造函数。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Aggregate2</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line"> <span class="type">int</span> num; </span><br><span class="line"> <span class="type">char</span> hello[<span class="number">6</span>]; </span><br><span class="line"> <span class="type">int</span> impYears[<span class="number">5</span>]; </span><br><span class="line">&#125;; </span><br><span class="line"></span><br><span class="line"><span class="comment">//对于这个结构，可像下面这样进行初始化：</span></span><br><span class="line">Aggregate2 a2 &#123;<span class="number">42</span>, &#123;<span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;o&#x27;</span>&#125;, &#123;<span class="number">1998</span>, <span class="number">2003</span>, <span class="number">2011</span>, <span class="number">2014</span>, <span class="number">2017</span>&#125;&#125;;</span><br></pre></td></tr></table></figure>
<p>定义常量表达式的关键字 <strong>constexpr</strong> 也可用于类和结果为常量的对象<br><code>constexpr Human(int humansAge) :age(humansAge) &#123;&#125;</code></p>
<h2 id="10-实现继承"><a href="#10-实现继承" class="headerlink" title="10.实现继承"></a>10.实现继承</h2><p>面向对象编程基于 4 个重要方面：封装、抽象、继承和多态。继承是一种强大的属性重用方式，是通向多态的跳板.</p>
<h3 id="继承和派生"><a href="#继承和派生" class="headerlink" title="继承和派生"></a>继承和派生</h3><p>继承: 从一个包含通用属性且实现了通用功能的基类（超类）派生出类似的类，并在派生类（子类）中覆盖基本功能，以实现让每个类都独一无二的行为。</p>
<p><strong>公有继承 public</strong>：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line"> <span class="comment">// ... base class members </span></span><br><span class="line">&#125;; </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived</span>:</span> public Base    <span class="comment">//public：公有继承，is-a关系，可通过派生类的对象来访问基类的公有成员</span></span><br><span class="line">&#123; </span><br><span class="line"> <span class="comment">// ... derived class members </span></span><br><span class="line">&#125;; </span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p><strong>基类初始化</strong> 向基类传递参数: 如果基类包含重载的构造函数，需要在实例化时给它提供实参,就使用初始化列表，并通过派生类的构造函数调用合适的基类构造函数</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line">public: </span><br><span class="line"> Base(<span class="type">int</span> someNumber) <span class="comment">// overloaded constructor </span></span><br><span class="line"> &#123; </span><br><span class="line"> <span class="comment">// Use someNumber </span></span><br><span class="line"> &#125; </span><br><span class="line">&#125;; </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived</span>:</span> public Base </span><br><span class="line">&#123;</span><br><span class="line">public: </span><br><span class="line"> Derived(): Base(<span class="number">25</span>) <span class="comment">// instantiate Base with argument 25 </span></span><br><span class="line"> &#123; </span><br><span class="line"> <span class="comment">// derived class constructor code </span></span><br><span class="line"> &#125; </span><br><span class="line">&#125;;    </span><br></pre></td></tr></table></figure>

<p><strong>覆盖基类</strong>: 派生类实现从基类继承的函数，且返回值和特征标相同的情况</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line">public: </span><br><span class="line"> <span class="type">void</span> <span class="title function_">DoSomething</span><span class="params">()</span> </span><br><span class="line"> &#123; </span><br><span class="line"> <span class="comment">// implementation code… Does something </span></span><br><span class="line"> &#125; </span><br><span class="line">&#125;; </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived</span>:</span>public Base </span><br><span class="line">&#123; </span><br><span class="line">public: </span><br><span class="line"> <span class="type">void</span> <span class="title function_">DoSomething</span><span class="params">()</span> </span><br><span class="line"> &#123; </span><br><span class="line"> <span class="comment">// implementation code… Does something else </span></span><br><span class="line"> <span class="comment">//也可以用作用域解析运算符（::）在派生类中调用基类方法</span></span><br><span class="line">   Base::DoSomething   </span><br><span class="line"> &#125; </span><br><span class="line">&#125;; </span><br><span class="line"></span><br><span class="line"><span class="comment">//调用基类中被覆盖的方法</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">   Derived test;</span><br><span class="line">   test.DoSomething();     <span class="comment">//被覆盖</span></span><br><span class="line">   test.Base::DoSomething; <span class="comment">//未覆盖，调用基类中的方法</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//隐藏问题：覆盖可能导致派生类隐藏基类的所有重载版本，使调用重载产生编译错误（被隐藏）</span></span><br><span class="line"><span class="comment">//可使用关键字 using 避免隐藏基类方法</span></span><br></pre></td></tr></table></figure>
<p>构造顺序：基类对象在派生类对象之前被实例化，实例化时，先实例化成员属性，再调用构造函数；析构顺序正好相反。</p>
<p><strong>私有继承 private</strong><br>私有继承使得只有子类才能使用基类的属性和方法，继承派生类的类不能访问基类的成员, 因此也被称为 <em>has-a</em> 关系, 指定派生类的基类时使用关键字 private：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line"> <span class="comment">// ... base class members and methods </span></span><br><span class="line">&#125;; </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived</span>:</span> private Base <span class="comment">// private inheritance  类的继承关系默认为私有</span></span><br><span class="line">&#123; </span><br><span class="line"> <span class="comment">// ... derived class members and methods </span></span><br><span class="line">&#125;; </span><br></pre></td></tr></table></figure>

<p><strong>保护继承 protected</strong><br>继承派生类的类能够访问基类的公有和保护方法，但不能通过派生类的对象来访问基类的公有成员；<br>使用访问限定符 protected: 对需要继承的基类属性进行保护,让基类的某些属性能在派生类中访问，但不能在继承层次结构外部访问</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived</span>:</span> protected Base</span><br><span class="line"><span class="comment">//...</span></span><br><span class="line"><span class="comment">//子类的子类能够访问 Base 类的公有和保护成员:</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived2</span>:</span> protected Derived </span><br><span class="line">&#123; </span><br><span class="line"> <span class="comment">// can access public &amp; protected members of Base </span></span><br><span class="line">&#125;; </span><br></pre></td></tr></table></figure>

<p>切除（slicing）问题: 复制对象时不要按值传递参数，而应以指向基类的指针或 const 引用的方式传递</p>
<p>多继承: </p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived</span>:</span> public Base1, publice Base2 </span><br><span class="line">&#123; </span><br><span class="line"> <span class="comment">// class members </span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>使用 final 禁止继承: <code>class Derived final: public Base1, publice Base2 </code></p>
<blockquote>
<p>要建立 is-a 关系，务必创建公有继承层次结构。<br>要建立 has-a 关系，务必创建私有或保护继承层次结构。(仅当必要时才使用私有或保护继承)<br>无论继承关系是什么，派生类都不能访问基类的私有成员。一个例外是类的友元函数和友元类</p>
</blockquote>
<h2 id="11-多态"><a href="#11-多态" class="headerlink" title="11.多态"></a>11.多态</h2><p>面向对象编程的核心——多态<br>多态：将派生类对象视为基类对象，并执行派生类的实现</p>
<p><strong>虚函数 virtual</strong><br>使用虚函数实现多态行为</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line"> virtual ReturnType <span class="title function_">FunctionName</span> <span class="params">(Parameter List)</span>; </span><br><span class="line">&#125;; </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line"> ReturnType <span class="title function_">FunctionName</span> <span class="params">(Parameter List)</span>; </span><br><span class="line">&#125;; </span><br></pre></td></tr></table></figure>
<p>使用关键字 virtual, Swim( )被声明为虚函数，确保编译器调用覆盖版本<br>对于将被派生类覆盖的基类方法，务必将其声明为虚函数。</p>
<p>作用：对于使用 new 在自由存储区中实例化的派生类对象，如果将其赋给基类指针，并通过该指针调用 delete，将不会调用派生类的析构函数。这可能导致资源未释放、内存泄露等问题，因此可将<strong>析构函数声明为虚函数</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line">public: </span><br><span class="line"> virtual ~Base() &#123;&#125;; <span class="comment">// virtual destructor </span></span><br><span class="line">&#125;; </span><br></pre></td></tr></table></figure>

<p><strong>抽象基类和纯虚函数</strong><br>不能实例化的基类被称为抽象基类，这样的基类只有一个用途，那就是从它派生出其他类（充当接口）。在 C++中，要创建抽象基类，可声明纯虚函数。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AbstractBase</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line">public: </span><br><span class="line"> virtual <span class="type">void</span> <span class="title function_">DoSomething</span><span class="params">()</span> = <span class="number">0</span>; <span class="comment">// pure virtual method </span></span><br><span class="line">&#125;; </span><br><span class="line"><span class="comment">//该声明告诉编译器，AbstractBase 的派生类必须实现方法 DoSomething()：</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived</span>:</span> public AbstractBase </span><br><span class="line">&#123; </span><br><span class="line">public: </span><br><span class="line"> <span class="type">void</span> <span class="title function_">DoSomething</span><span class="params">()</span> <span class="comment">// pure virtual fn. must be implemented </span></span><br><span class="line"> &#123; </span><br><span class="line"> <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Implemented virtual function&quot;</span> &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line"> &#125; </span><br><span class="line">&#125;; </span><br></pre></td></tr></table></figure>
<p>抽象基类提供了一种非常好的机制，能够声明所有派生类都必须实现的函数。如果 Trout 类从Fish 类派生而来，但没有实现 Trout::Swim( )，将无法通过编译</p>
<p><strong>虚继承 virtual</strong><br>使用<strong>虚继承</strong>解决菱形问题：在继承层次结构中，继承多个从同一个类派生而来的基类时，如果这些基类没有采用虚继承，将导致二义性，因此，如果派生类可能被用作基类，派生时最好使用<em>虚继承</em>：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived1</span>:</span> public virtual Base </span><br><span class="line">&#123; </span><br><span class="line"> <span class="comment">// ... members and functions </span></span><br><span class="line">&#125;; </span><br></pre></td></tr></table></figure>
<blockquote>
<p>用于创建继承层次结构和声明基类函数时，关键字 virtual 的作用不同:<br>在函数声明中，virtual 意味着当基类指针指向派生对象时，通过它可调用派生类的相应函数。<br>从 Base 类派生出 Derived1 和 Derived2 类时，如果使用了关键字 virtual，则意味着再从Derived1 和 Derived2 派生出 Derived3 时，每个 Derived3 实例只包含一个 Base 实例。</p>
</blockquote>
<p>表明覆盖意图的限定符 <strong>override</strong> , 来核实被覆盖的函数在基类中是否被声明为虚的</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tuna</span>:</span>public Fish </span><br><span class="line">&#123; </span><br><span class="line">public: </span><br><span class="line"> <span class="type">void</span> <span class="title function_">Swim</span><span class="params">()</span> <span class="type">const</span> override <span class="comment">// Error: no virtual fn with this sig in Fish </span></span><br><span class="line"> &#123; </span><br><span class="line"> <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Tuna swims!&quot;</span> &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line"> &#125; </span><br><span class="line">&#125;; </span><br></pre></td></tr></table></figure>
<p>在派生类中声明要覆盖基类函数的函数时，务必使用关键字 override。</p>
<p>使用 <strong>final</strong> 来禁止覆盖函数, 被声明为 final 的虚函数，不能在派生类中进行覆盖</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tuna</span>:</span>public Fish </span><br><span class="line">&#123; </span><br><span class="line">public: </span><br><span class="line"> <span class="type">void</span> <span class="title function_">Swim</span><span class="params">()</span> override final  <span class="comment">// override Fish::Swim and make this final </span></span><br><span class="line"> &#123; </span><br><span class="line"> <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Tuna swims!&quot;</span> &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line"> &#125; </span><br><span class="line">&#125;;<span class="comment">//可继承这个版本的 Tuna 类，但不能进一步覆盖函数 Swim()</span></span><br></pre></td></tr></table></figure>

<p>虚函数 Clone 模拟虚复制构造函数：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span>        <span class="comment">//头文件</span></span></span><br><span class="line">using namespace <span class="built_in">std</span>;     <span class="comment">//名称空间</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Fish</span>     //定义<span class="title">Fish</span>类作为基类</span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">   public:</span><br><span class="line">      virtual Fish* <span class="title function_">Clone</span><span class="params">()</span>=<span class="number">0</span>;   <span class="comment">//声明一个纯虚函数Clone 用于克隆对象</span></span><br><span class="line">      virtual <span class="type">void</span> <span class="title function_">Swim</span><span class="params">()</span>=<span class="number">0</span>;     <span class="comment">//声明一个纯虚函数Swim</span></span><br><span class="line">      virtual ~Fish() &#123;&#125;;     <span class="comment">//声明虚析构函数</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tuna</span>:</span> public Fish    <span class="comment">//定义Tuna类，继承自Fish</span></span><br><span class="line">&#123;</span><br><span class="line">   public:</span><br><span class="line">      Fish* <span class="title function_">Clone</span><span class="params">()</span> override  <span class="comment">//实现Clone函数，返回一个克隆对象指针</span></span><br><span class="line">      &#123;</span><br><span class="line">         <span class="keyword">return</span> new Tuna (*this);   <span class="comment">//*this表示当前对象的指针</span></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">   <span class="type">void</span> <span class="title function_">Swim</span><span class="params">()</span> override final <span class="comment">//final使它的派生类无法覆盖swim</span></span><br><span class="line">   &#123;</span><br><span class="line">      <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Tuna swims fast in the sea&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BluefinTuna</span> <span class="keyword">final</span>:</span>public Tuna <span class="comment">// 定义BluefinTuna类，继承自Tuna</span></span><br><span class="line">&#123;</span><br><span class="line">   public:</span><br><span class="line">      Fish* <span class="title function_">Clone</span><span class="params">()</span> override  <span class="comment">//无法覆盖Tuna类中的Swim函数</span></span><br><span class="line">      &#123;<span class="comment">//调用 Swim()时执行 Tuna::Swim()</span></span><br><span class="line">         <span class="keyword">return</span> new BluefinTuna(*this);</span><br><span class="line">      &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Carp</span> <span class="keyword">final</span>:</span> public Fish <span class="comment">// 定义Carp类，继承自Fish</span></span><br><span class="line">&#123;</span><br><span class="line">   Fish* <span class="title function_">Clone</span><span class="params">()</span> override&#123;</span><br><span class="line">      <span class="keyword">return</span> new Carp(*this);</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="type">void</span> <span class="title function_">Swim</span><span class="params">()</span> override final  <span class="comment">// 实现Carp的Swim函数</span></span><br><span class="line">   &#123;</span><br><span class="line">      <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Carp swims slow in the lake&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">   &#125;</span><br><span class="line">&#125; ;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">   <span class="type">const</span> <span class="type">int</span> ARRAY_SIZE =<span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">   Fish* myFishes[ARRAY_SIZE]=&#123;<span class="literal">NULL</span>&#125;;  <span class="comment">//声明静态基类指针（Fish *）数组，创建对象</span></span><br><span class="line">   myFishes[<span class="number">0</span>]=new Tuna();</span><br><span class="line">   myFishes[<span class="number">1</span>]=new Carp();</span><br><span class="line">   myFishes[<span class="number">2</span>]=new BluefinTuna();</span><br><span class="line">   myFishes[<span class="number">3</span>]=new Carp();</span><br><span class="line"></span><br><span class="line">   Fish* myNewFishes[ARRAY_SIZE];</span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> index=<span class="number">0</span>; index &lt; ARRAY_SIZE; ++index)</span><br><span class="line">      myNewFishes[index]=myFishes[index]-&gt;Clone(); <span class="comment">// 使用Clone函数克隆原对象到另一个数组</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> index=<span class="number">0</span>; index&lt;ARRAY_SIZE; ++index)</span><br><span class="line">      myNewFishes[index]-&gt;Swim();   <span class="comment">// 调用克隆对象的Swim函数，以验证 Clone( )复制了整个派生类对象</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> index=<span class="number">0</span>; index&lt;ARRAY_SIZE; ++index)</span><br><span class="line">      &#123;  <span class="comment">// 释放内存</span></span><br><span class="line">         delete myFishes[index];</span><br><span class="line">         delete myNewFishes[index];</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>














<hr>
<hr>
<h1 id="Cmake"><a href="#Cmake" class="headerlink" title="Cmake"></a>Cmake</h1><p><a href="www.cmake.org">CMake</a> 是一个跨平台的开源构建管理系统，用于自动化应用程序的构建、测试和打包过程。它使用类似于Makefile的文本文件来描述构建过程中所需的所有组件和依赖项，并将其转换为适合各种不同编译器和操作系统的本地构建系统的配置文件。总之，CMake就是一个将多个cpp,hpp文件组合构建为一个大工程的语言。<br><a href="https://cmake.org/download/">CMake下载</a> (Linux无需下载)<br><a href="https://github.com/gavinliu6/CMake-Practice-zh-CN">Cmake 实践</a> 在实践中上手的教程<br><a href="https://github.com/SFUMECJF/cmake-examples-Chinese">cmake-examples-Chinese</a> 例程</p>
<p><a href="https://github.com/Arrowes/C-coding/tree/main/Cmake">C-coding&#x2F;Cmake at main · Arrowes&#x2F;C-coding</a></p>
<h2 id="Cmake-实践"><a href="#Cmake-实践" class="headerlink" title="Cmake 实践"></a><a href="https://gavinliu6.github.io/CMake-Practice-zh-CN/#/">Cmake 实践</a></h2><h3 id="t1-创建Hello-world"><a href="#t1-创建Hello-world" class="headerlink" title="t1 创建Hello world"></a>t1 <a href="https://github.com/gavinliu6/CMake-Practice-zh-CN/blob/master/hello-world.md">创建Hello world</a></h3><p>建立main.c与CMakeLists.txt并编译（需要为每一个子目录建立一个CMakeLists.txt）</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">PROJECT (HELLO)     <span class="comment">#PROJECT(projectname [CXX] [C] [Java])</span></span><br><span class="line">SET(SRC_LIST main.c)    <span class="comment">#提供要编译的源代码文件列表，可定义多个源文件main.c 1.c</span></span><br><span class="line">MESSAGE([SEND_ERROR|STATUS|FATAL_ERROR] <span class="string">&quot;message to display&quot;</span>)</span><br><span class="line">ADD_EXECUTABLE(hello <span class="variable">$&#123;SRC_LIST&#125;</span>)   <span class="comment">#创建名为hello的可执行文件，IF不用$&#123;&#125;引用变量</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#内部编译(在文件夹中产生文件太多)</span></span><br><span class="line">cmake . <span class="comment">#构建工程，生成makefile</span></span><br><span class="line">make    <span class="comment">#构建目标文件hello二进制</span></span><br><span class="line">./hello <span class="comment">#运行目标文件</span></span><br><span class="line"><span class="comment">#外部编译(out of source build,推荐)</span></span><br><span class="line"><span class="built_in">mkdir</span> build &amp;&amp; <span class="built_in">cd</span> build <span class="comment">#新建并进入build文件夹</span></span><br><span class="line">cmake ..    <span class="comment">#在父目录找到cmakelists构建工程</span></span><br><span class="line">make    <span class="comment">#在build编译目录中获得目标文件，不影响原有工程</span></span><br></pre></td></tr></table></figure>
<p>目标文件:在linux下，是ELF格式（Executable Linkable Format，可执行可链接格式），而在windows下是PE（Portable Executable，可移植可执行）。通常有三种形式：</p>
<ul>
<li>可执行目标文件。即我们通常所认识的，可直接运行的二进制文件。</li>
<li>可重定位目标文件。包含了二进制的代码和数据，可以与其他可重定位目标文件合并，并创建一个可执行目标文件。</li>
<li>共享目标文件。它是一种在加载或者运行时进行链接的特殊可重定位目标文件。</li>
</ul>
<h3 id="t2-完善项目并安装"><a href="#t2-完善项目并安装" class="headerlink" title="t2 完善项目并安装"></a>t2 <a href="https://github.com/gavinliu6/CMake-Practice-zh-CN/blob/master/better-hello-world.md">完善项目并安装</a></h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#在CMakeLists中加入</span></span><br><span class="line">ADD_SUBDIRECTORY(src bin) <span class="comment">#把src子目录加入工程，编译输出路径为bin</span></span><br><span class="line">INSTALL(FILES COPYRIGHT README DESTINATION doc)</span><br><span class="line">INSTALL(PROGRAMS runhello.sh DESTINATION bin)</span><br><span class="line">INSTALL(DIRECTORY doc/ DESTINATION doc) <span class="comment">#不同的安装类型</span></span><br><span class="line"><span class="comment">#在 src的 CMakeLists.txt中添加，以安装hello到bin中</span></span><br><span class="line">INSTALL(TARGETS hello RUNTIME DESTINATION bin) </span><br><span class="line"></span><br><span class="line"><span class="comment">#Install</span></span><br><span class="line">cmake -DCMAKE_INSTALL_PREFIX=tmp ..</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure>
<h3 id="t3-lib静态库和动态库构建"><a href="#t3-lib静态库和动态库构建" class="headerlink" title="t3 lib静态库和动态库构建"></a>t3 <a href="https://github.com/gavinliu6/CMake-Practice-zh-CN/blob/master/static-and-dynamic.md">lib静态库和动态库构建</a></h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">SET(LIBHELLO_SRC hello.c)</span><br><span class="line">ADD_LIBRARY(hello SHARED <span class="variable">$&#123;LIBHELLO_SRC&#125;</span>)   <span class="comment">#SHARED动态库（.so）</span></span><br><span class="line">ADD_LIBRARY(hello_static STATIC <span class="variable">$&#123;LIBHELLO_SRC&#125;</span>)    <span class="comment">#STATIC静态库(.a) </span></span><br><span class="line">SET_TARGET_PROPERTIES(hello_static PROPERTIES OUTPUT_NAME <span class="string">&quot;hello&quot;</span>)  <span class="comment">#设置静态库名称，以得到名字相同的.so .a</span></span><br><span class="line">SET_TARGET_PROPERTIES(hello PROPERTIES VERSION 1.2 SOVERSION 1) <span class="comment">#动态库版本号 </span></span><br><span class="line">INSTALL(TARGETS hello hello_static LIBRARY DESTINATION lib ARCHIVE DESTINATION lib) <span class="comment">#关键字ARCHIVE 特指静态库，LIBRARY 特指动态库，RUNTIME 特指可执行目标二进制</span></span><br><span class="line">INSTALL(FILES hello.h DESTINATION include/hello)</span><br></pre></td></tr></table></figure>
<p>静态库.a（Static Library），所有函数和数据都在编译时被静态链接到可执行文件中。文件较大，但不容易受到环境变量和库版本变化的影响，能够提供更好的性能和稳定性。<br>动态库.so（Dynamic Library）（共享库），在程序运行时才被加载到内存中，而不是在程序编译时被静态链接到可执行文件中，每个动态库只需要一个副本，可以供多个程序使用，因此可以减小可执行文件的大小，减少内存占用，并且如果库文件更新，则只需要替换动态库文件即可，但由于需要在运行时加载库文件，因此可能会稍微降低程序的启动和运行速度。</p>
<h3 id="t4-使用外部共享库和头文件"><a href="#t4-使用外部共享库和头文件" class="headerlink" title="t4 使用外部共享库和头文件"></a>t4 <a href="https://github.com/gavinliu6/CMake-Practice-zh-CN/blob/master/the-use-of-lib-and-header-file.md">使用外部共享库和头文件</a></h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#在src/CMakeLists.txt中添加头文件.h搜索路径</span></span><br><span class="line">INCLUDE_DIRECTORIES(XXX/include/hello)</span><br><span class="line">TARGET_LINK_LIBRARIES(main XXX/lib/libhello.so) <span class="comment">#添加共享库链接</span></span><br><span class="line"><span class="comment">#若要链接静态库：TARGET_LINK_LIBRARIES(main XXX/lib/libhello.a)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#运行ldd查看链接情况</span></span><br><span class="line">ldd src/main</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改环境变量，在bash中运行：</span></span><br><span class="line"><span class="built_in">export</span> CMAKE_INCLUDE_PATH=XXX/include/hello <span class="comment">#然后利用FIND_PATH相关指令替换INCLUDE_DIRECTORIES</span></span><br></pre></td></tr></table></figure>
<h3 id="常用变量与环境变量"><a href="#常用变量与环境变量" class="headerlink" title="常用变量与环境变量"></a><a href="https://github.com/gavinliu6/CMake-Practice-zh-CN/blob/master/common-var.md">常用变量与环境变量</a></h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#常用变量</span></span><br><span class="line">CMAKE_BINARY_DIR    <span class="comment">#如果是 in source 编译，指工程顶层目录，如果是 out-of-source 编译，指工程编译发生的目录,还有PROJECT_BINARY_DIR，&lt;projectname&gt;_BINARY_DIR</span></span><br><span class="line">CMAKE_SOURCE_DIR    <span class="comment">#工程顶层目录，PROJECT_SOURCE_DIR，&lt;projectname&gt;_SOURCE_DIR</span></span><br><span class="line">CMAKE_CURRENT_SOURCE_DIR    <span class="comment">#当前处理的 CMakeLists.txt 所在的路径</span></span><br><span class="line">CMAKE_CURRRENT_BINARY_DIR   <span class="comment">#若是 in-source 编译，同上一致，对out-ofsource 编译，他指的是 target 编译目录。</span></span><br><span class="line">CMAKE_CURRENT_LIST_FILE <span class="comment">#输出调用这个变量的 CMakeLists.txt 的完整路径</span></span><br><span class="line">CMAKE_CURRENT_LIST_LINE <span class="comment">#输出这个变量所在的行</span></span><br><span class="line">CMAKE_MODULE_PATH       <span class="comment">#定义自己的 cmake 模块所在的路径</span></span><br><span class="line">EXECUTABLE_OUTPUT_PATH，LIBRARY_OUTPUT_PATH <span class="comment">#分别用来重新定义最终结果的存放目录，如SET(EXECUTABLE_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;/bin)</span></span><br><span class="line">PROJECT_NAME    <span class="comment">#返回通过 PROJECT 指令定义的项目名称</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#环境变量，使用$ENV&#123;NAME&#125; 调用系统环境变量</span></span><br><span class="line">SET(ENV&#123;变量名&#125; 值) <span class="comment">#设置环境变量</span></span><br><span class="line">    CMAKE_INCLUDE_CURRENT_DIR</span><br><span class="line">    CMAKE_INCLUDE_DIRECTORIES_PROJECT_BEFORE</span><br><span class="line">    CMAKE_INCLUDE_PATH，CMAKE_LIBRARY_PATH</span><br></pre></td></tr></table></figure>

<h3 id="cmake常用指令"><a href="#cmake常用指令" class="headerlink" title="cmake常用指令"></a><a href="https://github.com/gavinliu6/CMake-Practice-zh-CN/blob/master/common-directives.md">cmake常用指令</a></h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">make VERBOSE=1  <span class="comment">#查看make过程</span></span><br><span class="line">make clean  <span class="comment">#清理工程</span></span><br></pre></td></tr></table></figure>
<h3 id="t5-t6-模块"><a href="#t5-t6-模块" class="headerlink" title="t5,t6 模块"></a><a href="https://github.com/gavinliu6/CMake-Practice-zh-CN/blob/master/module.md">t5,t6 模块</a></h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#系统预定义的模块</span></span><br><span class="line">FIND_PACKAGE(CURL)  <span class="comment">#FindCURL模块</span></span><br><span class="line">IF(CURL_FOUND)  <span class="comment">#判断模块是否被找到</span></span><br><span class="line">   INCLUDE_DIRECTORIES(<span class="variable">$&#123;CURL_INCLUDE_DIR&#125;</span>)</span><br><span class="line">   TARGET_LINK_LIBRARIES(curltest <span class="variable">$&#123;CURL_LIBRARY&#125;</span>)</span><br><span class="line">ELSE(CURL_FOUND)</span><br><span class="line">     MESSAGE(FATAL_ERROR ”CURL library not found”)</span><br><span class="line">ENDIF(CURL_FOUND)</span><br><span class="line"></span><br><span class="line"><span class="comment">#自定义FindHELLO模块</span></span><br><span class="line">FIND_PATH(HELLO_INCLUDE_DIR hello.h /usr/include/hello /usr/local/include/hello)    <span class="comment"># 在指定目录中搜索hello.h文件</span></span><br><span class="line">FIND_LIBRARY(HELLO_LIBRARY NAMES hello PATH /usr/lib /usr/local/lib)    <span class="comment"># 在指定目录中搜索名为hello的库文件</span></span><br><span class="line">IF(HELLO_INCLUDE_DIR AND HELLO_LIBRARY) <span class="comment"># 如果找到了头文件和库，则标记为HELLO_FOUND</span></span><br><span class="line">   SET(HELLO_FOUND TRUE)</span><br><span class="line">ENDIF(HELLO_INCLUDE_DIR AND HELLO_LIBRARY)</span><br><span class="line">IF(HELLO_FOUND) <span class="comment"># 如果找到了头文件和库，则输出一个消息</span></span><br><span class="line">   IF(NOT HELLO_FIND_QUIETLY)   <span class="comment">#如果没有被标记为“安静模式”，则输出</span></span><br><span class="line">       MESSAGE(STATUS <span class="string">&quot;Found Hello: <span class="variable">$&#123;HELLO_LIBRARY&#125;</span>&quot;</span>)</span><br><span class="line">   ENDIF(NOT HELLO_FIND_QUIETLY)</span><br><span class="line">ELSE(HELLO_FOUND)   <span class="comment"># 如果没有找到，并且被标记为必需，则输出错误信息</span></span><br><span class="line">   IF(HELLO_FIND_REQUIRED)  <span class="comment">#如果被标记为“必需”，则输出致命错误消息</span></span><br><span class="line">      MESSAGE(FATAL_ERROR <span class="string">&quot;Could not find hello library&quot;</span>)</span><br><span class="line">   ENDIF(HELLO_FIND_REQUIRED)</span><br><span class="line">ENDIF(HELLO_FOUND)</span><br></pre></td></tr></table></figure>
<h2 id="Cmake-Opencv-Demo"><a href="#Cmake-Opencv-Demo" class="headerlink" title="Cmake Opencv Demo"></a>Cmake Opencv Demo</h2><p><strong>1.安装OpenCV</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/opencv/opencv.git</span><br><span class="line"><span class="built_in">cd</span> opencv</span><br><span class="line"><span class="comment">#cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/usr/local ..   #cmake把opencv的一些库和可执行文件安装到系统目录下 需要权限</span></span><br><span class="line">cmake -DCMAKE_INSTALL_PREFIX=<span class="variable">$HOME</span>/opencv .. <span class="comment">#本地安装无需权限</span></span><br><span class="line">make -j8 <span class="comment">#使用8个线程进行编译,否则很久</span></span><br><span class="line">make install   <span class="comment">#安装库文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#配置OpenCV环境变量</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/home/ywang85/opencv/lib:<span class="variable">$LD_LIBRARY_PATH</span>  <span class="comment">#链接库文件</span></span><br><span class="line"><span class="built_in">export</span> PKG_CONFIG_PATH=/home/ywang85/opencv/lib/cmake/opencv4/:<span class="variable">$PKG_CONFIG_PATH</span>  <span class="comment">#链接配置文件</span></span><br></pre></td></tr></table></figure>
<p><strong>2.写主程序</strong></p>
<details>
  <summary>边缘提取程序</summary>

<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/core/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/imgproc/imgproc.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/highgui/highgui.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line">using namespace cv;</span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> threshold_value = <span class="number">100</span>, threshold_max = <span class="number">255</span>;</span><br><span class="line"><span class="type">int</span> threshold_type = <span class="number">0</span>, threshold_type_max = <span class="number">4</span>;</span><br><span class="line"><span class="built_in">string</span> outwindow = <span class="string">&quot;threshold img&quot;</span>;</span><br><span class="line">Mat src, dst;</span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">   Mat src1;</span><br><span class="line">   src1 = imread(<span class="string">&quot;1.jpg&quot;</span>);</span><br><span class="line">   resize(src1, src, Size(src1.cols, src1.rows)); </span><br><span class="line">   <span class="comment">//resize(src1, src, Size(src1.cols/2, src1.rows/2)); //缩小一半</span></span><br><span class="line">   <span class="keyword">if</span> (!src.data)&#123;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">&quot;cannot load image ...&quot;</span>);</span><br><span class="line">      <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   Mat src_gray;</span><br><span class="line">   cvtColor(src, src_gray, COLOR_BGR2GRAY);</span><br><span class="line">   Canny(src_gray, dst, <span class="number">100</span>, <span class="number">200</span>);<span class="comment">//canny边缘检测算子</span></span><br><span class="line">   imwrite(<span class="string">&quot;canny.jpg&quot;</span>, dst);</span><br><span class="line">   imwrite(<span class="string">&quot;canny2.jpg&quot;</span>, ~dst); <span class="comment">//dst按照像素值取反</span></span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</details>

<p>使用OpenCV的canny算子检测边缘</p>
<p><strong>3.写CMake</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">cmake_minimum_required(VERSION <span class="number">2.8</span>)</span><br><span class="line">project(EDGE)</span><br><span class="line"><span class="built_in">set</span>(OpenCV_DIR <span class="string">&quot;$&#123;CMAKE_SOURCE_DIR&#125;/opencv/lib/cmake/opencv4/&quot;</span>) #设置 OpenCV 的 CMake 路径</span><br><span class="line">find_package(OpenCV REQUIRED)</span><br><span class="line">add_executable(EDGE main.cpp)</span><br><span class="line">target_include_directories(EDGE PUBLIC $&#123;OpenCV_INCLUDE_DIRS&#125;)  #头文件路径添加到编译器的include路径中</span><br><span class="line">target_link_libraries(EDGE PUBLIC $&#123;OpenCV_LIBS&#125;)   #链接OpenCV库</span><br><span class="line">#需要注意opencv库的链接</span><br></pre></td></tr></table></figure>
<p><strong>4.编译运行</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">mkdir build &amp;&amp; cd build</span><br><span class="line">cmake ..</span><br><span class="line">make  #生成可执行文件</span><br><span class="line">./EDGE   #运行边缘提取执行文件</span><br></pre></td></tr></table></figure>
<img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Cedge.png" width="50%">


<h2 id="基于VScode用cmake搭建C-编译调试环境"><a href="#基于VScode用cmake搭建C-编译调试环境" class="headerlink" title="基于VScode用cmake搭建C++编译调试环境"></a>基于VScode用cmake搭建C++编译调试环境</h2><ol>
<li>安装VScode插件：C&#x2F;C++，cmake，cmake tools</li>
<li>按F1，选择cmake:Quick Start,创建一个cmake工程</li>
<li>点击左侧栏的CMake工具按钮,右键可执行文件，选择Debug,进入调试界面</li>
</ol>
<h2 id="gcc-g-MinGW-MSVC与make-CMake-qmake"><a href="#gcc-g-MinGW-MSVC与make-CMake-qmake" class="headerlink" title="gcc&#x2F;g++,MinGW&#x2F;MSVC与make&#x2F;CMake&#x2F;qmake"></a>gcc&#x2F;g++,MinGW&#x2F;MSVC与make&#x2F;CMake&#x2F;qmake</h2><p><strong>GNU</strong>&#x2F;Linux：简称Linux，包括Ubuntu，Debian，CentOS，自带gcc；<br><strong>gcc&#x2F;g++</strong> ：GNU编译器套件（GNU Compiler Collection）,在<em>Linux</em>或MacOS上使用，gcc主要用于C语言,g++支持更多的C++特性。</p>
<p><strong>MinGW</strong>(Minimalist GNUfor Windows)，是<em>Windows</em>下运行的GNU环境，包含gcc和一系列工具，让开发者在Windows下可以写GNU的c&#x2F;c++代码, 编译的结果是windows的可执行文件exe；<br><strong>MSVC</strong>:微软开发的C&#x2F;C++编译器，在<em>Windows</em>下编译C&#x2F;C++程序。它被集成在Visual Studio IDE中。</p>
<p><strong>Makefile</strong>包含了描述如何编译和链接程序的规则和指令,指定哪些文件需要先编译，后编译以及重新编译，甚至更复杂的功能操作,通常被用于自动化构建C&#x2F;C++项目;<br><strong>Make</strong>是一个自动化构建工具，执行Make命令时，它会读取Makefile中的规则，并根据依赖项关系来判断哪些规则需要被执行，来实现编译、链接等操作。<br><strong>CMake</strong>是一个跨平台的自动化构建工具，与Make类似，但是它不直接构建项目，而是生成适合不同构建系统的配置文件，如Makefile或Visual Studio的.sln文件，并调用相应的构建系统来进行项目构建。<br><strong>qmake</strong>是Qt框架提供的自动化构建工具，用于构建Qt项目。</p>
]]></content>
      <tags>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title>DL模型转换及部署：torch &gt; onnx &gt; deploy</title>
    <url>/DLdeploy/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>深度学习模型部署相关记录，目前仅开了个头，项目地址：<a href="https://github.com/Arrowes/DLpractice">DLpractice</a></p>
<span id="more"></span>

<p><strong>算法部署</strong></p>
<ul>
<li>Network selection：</li>
<li>Optimization：分组卷积、深度可分离卷积、稀疏卷积</li>
<li>Deployment：<img alt="图 1" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/TDA4VMdeploy.png" width="70%"/></li>
</ul>
<p>Netron神经网络可视化: <a href="https://github.com/lutzroeder/netron/releases/tag/v7.0.0">软件下载</a>, <a href="https://netron.app/">在线网站</a></p>
<h1 id="ONNX"><a href="#ONNX" class="headerlink" title="ONNX"></a>ONNX</h1><p>Open Neural Network Exchange 开源机器学习通用中间格式，兼容各种深度学习框架、推理引擎、终端硬件、操作系统，是深度学习框架到推理引擎的桥梁<br>链接：<a href="https://onnx.ai/">ONNX</a>，<a href="https://github.com/onnx/onnx">Github</a>，<a href="https://onnxruntime.ai/">ONNX Runtime</a>，<a href="https://onnx.coderai.cn/">ONNX Runtime Web</a></p>
<p><a href="https://pytorch.org/docs/stable/onnx.html">TORCH.ONNX</a>，<a href="https://github.com/pytorch/pytorch/tree/main/torch/onnx">Github</a><br>Pytorch 模型导出使用自带的接口：<code>torch.onnx.export</code><br> PyTorch 转 ONNX，实际上就是把每个 PyTorch 的操作<strong>映射</strong>成了 ONNX 定义的<strong>算子</strong>。PyTorch 对 ONNX 的算子支持:<a href="https://github.com/onnx/onnx/blob/main/docs/Operators.md">官方算子文档</a></p>
<p>在转换普通的torch.nn.Module模型时，PyTorch 一方面会用跟踪法执行前向推理，把遇到的算子整合成计算图；另一方面，PyTorch 还会把遇到的每个算子翻译成 ONNX 中定义的算子。要使 PyTorch 算子顺利转换到 ONNX ，我们需要保证：</p>
<blockquote>
<p>1.算子在 PyTorch 中有实现<br>2.有把该 PyTorch 算子映射成一个或多个 ONNX 算子的方法<br>3.ONNX 有相应的算子</p>
</blockquote>
<h2 id="以超分辨率模型为例"><a href="#以超分辨率模型为例" class="headerlink" title="以超分辨率模型为例"></a>以超分辨率模型为例</h2><p>参考：<a href="https://www.zhihu.com/column/c_1497987564452114432">模型部署那些事</a><br>以超分辨率模型为例，实现pytorch模型转onnx<br>其中， PyTorch 的 interpolate 插值算子可以在运行阶段选择放大倍数，但该算子不兼容，需要<strong>自定义算子</strong>:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NewInterpolate</span>(torch.autograd.Function):</span><br><span class="line">    <span class="comment"># 自定义的插值算子，继承自torch.autograd.Function</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">symbolic</span>(<span class="params">g, <span class="built_in">input</span>, scales</span>):</span><br><span class="line">        <span class="comment"># 静态方法，用于定义符号图的构建过程, g: 符号图构建器, input: 输入张量, scales: 缩放因子</span></span><br><span class="line">        <span class="comment">#ONNX 算子的具体定义由 g.op 实现。g.op 的每个参数都可以映射到 ONNX 中的算子属性</span></span><br><span class="line">        <span class="comment">#对于其他参数，可以照着 Resize 算子文档填</span></span><br><span class="line">        <span class="keyword">return</span> g.op(<span class="string">&quot;Resize&quot;</span>,  <span class="comment"># 使用Resize操作</span></span><br><span class="line">                    <span class="built_in">input</span>,  <span class="comment"># 输入张量</span></span><br><span class="line">                    g.op(<span class="string">&quot;Constant&quot;</span>, value_t=torch.tensor([], dtype=torch.float32)),  <span class="comment"># 空的常量张量</span></span><br><span class="line">                    scales,  <span class="comment"># 缩放因子</span></span><br><span class="line">                    coordinate_transformation_mode_s=<span class="string">&quot;pytorch_half_pixel&quot;</span>,  <span class="comment"># 坐标转换模式为pytorch_half_pixel</span></span><br><span class="line">                    cubic_coeff_a_f=-<span class="number">0.75</span>,  <span class="comment"># cubic插值的系数a为-0.75</span></span><br><span class="line">                    mode_s=<span class="string">&#x27;cubic&#x27;</span>,  <span class="comment"># 插值模式为cubic</span></span><br><span class="line">                    nearest_mode_s=<span class="string">&quot;floor&quot;</span>)  <span class="comment"># 最近邻插值模式为floor</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, <span class="built_in">input</span>, scales</span>):    <span class="comment">#算子的推理行为由算子的 foward 方法决定</span></span><br><span class="line">        scales = scales.tolist()[-<span class="number">2</span>:]   <span class="comment">#截取输入张量的后两个元素,把 [1, 1, w, h] 格式的输入对接到原来的 interpolate 函数上</span></span><br><span class="line">        <span class="keyword">return</span> interpolate(<span class="built_in">input</span>,   <span class="comment">#把这两个元素以 list 的格式传入 interpolate 的 scale_factor 参数。</span></span><br><span class="line">                           scale_factor=scales,</span><br><span class="line">                           mode=<span class="string">&#x27;bicubic&#x27;</span>,</span><br><span class="line">                           align_corners=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<details>
    <summary>SRCNN超分辨率代码</summary>

<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">StrangeSuperResolutionNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">9</span>, padding=<span class="number">4</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">64</span>, <span class="number">32</span>, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">32</span>, <span class="number">3</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, upscale_factor</span>):</span><br><span class="line">        x = NewInterpolate.apply(x, upscale_factor)</span><br><span class="line">        out = self.relu(self.conv1(x))</span><br><span class="line">        out = self.relu(self.conv2(out))</span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_torch_model</span>():</span><br><span class="line">    torch_model = StrangeSuperResolutionNet()</span><br><span class="line"></span><br><span class="line">    state_dict = torch.load(<span class="string">&#x27;srcnn.pth&#x27;</span>)[<span class="string">&#x27;state_dict&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Adapt the checkpoint</span></span><br><span class="line">    <span class="keyword">for</span> old_key <span class="keyword">in</span> <span class="built_in">list</span>(state_dict.keys()):</span><br><span class="line">        new_key = <span class="string">&#x27;.&#x27;</span>.join(old_key.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">1</span>:])</span><br><span class="line">        state_dict[new_key] = state_dict.pop(old_key)</span><br><span class="line"></span><br><span class="line">    torch_model.load_state_dict(state_dict)</span><br><span class="line">    torch_model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">return</span> torch_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = init_torch_model()</span><br><span class="line">factor = torch.tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">input_img = cv2.imread(<span class="string">&#x27;face.png&#x27;</span>).astype(np.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># HWC to NCHW</span></span><br><span class="line">input_img = np.transpose(input_img, [<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">input_img = np.expand_dims(input_img, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inference</span></span><br><span class="line">torch_output = model(torch.from_numpy(input_img), factor).detach().numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># NCHW to HWC</span></span><br><span class="line">torch_output = np.squeeze(torch_output, <span class="number">0</span>)</span><br><span class="line">torch_output = np.clip(torch_output, <span class="number">0</span>, <span class="number">255</span>)</span><br><span class="line">torch_output = np.transpose(torch_output, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>]).astype(np.uint8)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show image</span></span><br><span class="line">cv2.imwrite(<span class="string">&quot;face_torch2.png&quot;</span>, torch_output)</span><br><span class="line">input_img1 = cv2.imread(<span class="string">&#x27;face.png&#x27;</span>)</span><br><span class="line">cv2.imshow(<span class="string">&quot;Input Image&quot;</span>, input_img1)</span><br><span class="line">cv2.imshow(<span class="string">&quot;Torch Output&quot;</span>, torch_output)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>
</details>

<hr>
<p>模型转换为ONNX，验证正确性，运行推理：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pth2onnx</span></span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>)</span><br><span class="line"><span class="comment"># 一种叫做追踪（trace）的模型转换方法：给定一组输入，再实际执行一遍模型，即把这组输入对应的计算图记录下来，保存为 ONNX 格式</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    torch.onnx.export(model, (x, factor),</span><br><span class="line">                      <span class="string">&quot;srcnn2.onnx&quot;</span>,</span><br><span class="line">                      opset_version=<span class="number">11</span>,</span><br><span class="line">                      input_names=[<span class="string">&#x27;input&#x27;</span>, <span class="string">&#x27;factor&#x27;</span>],</span><br><span class="line">                      output_names=[<span class="string">&#x27;output&#x27;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证onnx, 此外可以使用Netron可视化检查网络结构</span></span><br><span class="line">onnx_model = onnx.load(<span class="string">&quot;srcnn.onnx&quot;</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    onnx.checker.check_model(onnx_model)</span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Model incorrect&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Model correct&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择放大倍数，运行ONNX Runtime 推理</span></span><br><span class="line">input_factor = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>], dtype=np.float32)</span><br><span class="line">ort_session = onnxruntime.InferenceSession(<span class="string">&quot;srcnn2.onnx&quot;</span>)   <span class="comment"># 用于获取一个 ONNX Runtime 推理器</span></span><br><span class="line">ort_inputs = &#123;<span class="string">&#x27;input&#x27;</span>: input_img, <span class="string">&#x27;factor&#x27;</span>: input_factor&#125;</span><br><span class="line">ort_output = ort_session.run(<span class="literal">None</span>, ort_inputs)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">ort_output = np.squeeze(ort_output, <span class="number">0</span>)</span><br><span class="line">ort_output = np.clip(ort_output, <span class="number">0</span>, <span class="number">255</span>)</span><br><span class="line">ort_output = np.transpose(ort_output, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>]).astype(np.uint8)</span><br><span class="line">cv2.imwrite(<span class="string">&quot;face_torch2_run.png&quot;</span>, ort_output)  <span class="comment"># 生成上采样图片，运行成功</span></span><br></pre></td></tr></table></figure>
<img alt="picture 0" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DLdeploynetron.png" width="80%"/>  

<h2 id="torch-onnx-export模型转换接口"><a href="#torch-onnx-export模型转换接口" class="headerlink" title="torch.onnx.export模型转换接口"></a>torch.onnx.export模型转换接口</h2><p><a href="https://link.zhihu.com/?target=https://pytorch.org/docs/stable/onnx.html%23functions">torch.onnx ‒ PyTorch 1.11.0 documentation</a><br><a href="https://link.zhihu.com/?target=https://pytorch.org/docs/stable/jit.html">TorchScript</a> 是一种序列化和优化 PyTorch 模型的格式，在优化过程中，一个<code>torch.nn.Module</code>模型会被转换成 TorchScript 的 <code>torch.jit.ScriptModule</code>模型。<br>而要把普通 PyTorch 模型转一个 TorchScript 模型，有跟踪（trace）和记录（script）两种导出计算图的方法：</p>
<ul>
<li>trace: 以上一节为例，跟踪法只能通过实际运行一遍模型的方法导出模型的静态图，即无法识别出模型中的控制流（如循环）,对于循环中不同的n, ONNX 模型的结构是不一样的</li>
<li>script: 记录法则能通过解析模型来正确记录所有的控制流,模型不需要实际运行，用 Loop 节点来表示循环</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">export</span>(<span class="params">model, args, f, export_params=<span class="literal">True</span>, verbose=<span class="literal">False</span>, training=TrainingMode.EVAL, </span></span><br><span class="line"><span class="params">           input_names=<span class="literal">None</span>, output_names=<span class="literal">None</span>, aten=<span class="literal">False</span>, export_raw_ir=<span class="literal">False</span>, </span></span><br><span class="line"><span class="params">           operator_export_type=<span class="literal">None</span>, opset_version=<span class="literal">None</span>, _retain_param_name=<span class="literal">True</span>, </span></span><br><span class="line"><span class="params">           do_constant_folding=<span class="literal">True</span>, example_outputs=<span class="literal">None</span>, strip_doc_string=<span class="literal">True</span>, </span></span><br><span class="line"><span class="params">           dynamic_axes=<span class="literal">None</span>, keep_initializers_as_inputs=<span class="literal">None</span>, custom_opsets=<span class="literal">None</span>, </span></span><br><span class="line"><span class="params">           enable_onnx_checker=<span class="literal">True</span>, use_external_data_format=<span class="literal">False</span></span>): </span><br><span class="line"></span><br><span class="line"><span class="comment"># model: 模型， args：输入， f：导出文件名，</span></span><br><span class="line"><span class="comment"># export_params：是否存储模型权重， ONNX 是用同一个文件表示记录模型的结构和权重的。</span></span><br><span class="line"><span class="comment"># input_names, output_names：设置输入和输出张量的名称。如果不设置的话，会自动分配一些简单的名字（如数字）</span></span><br><span class="line"><span class="comment"># opset_version：转换时参考哪个 ONNX 算子集版本，默认为 9。</span></span><br><span class="line"><span class="comment"># dynamic_axes：指定输入输出张量的哪些维度是动态的。为了效率，ONNX 默认所有参与运算的张量都是静态的（张量的形状不发生改变），必要时需要显式地指明输入输出张量的哪几个维度的大小是可变的。</span></span><br></pre></td></tr></table></figure>

<h2 id="自定义算子"><a href="#自定义算子" class="headerlink" title="自定义算子"></a>自定义算子</h2><ul>
<li>PyTorch 算子<ul>
<li>组合现有算子</li>
<li>添加 TorchScript 算子</li>
<li>添加普通 C++ 拓展算子</li>
</ul>
</li>
<li>映射方法<ul>
<li>为 ATen 算子添加符号函数</li>
<li>为 TorchScript 算子添加符号函数</li>
<li>封装成 <code>torch.autograd.Function</code> 并添加符号函数</li>
</ul>
</li>
<li>ONNX 算子<ul>
<li>使用现有 ONNX 算子</li>
<li>定义新 ONNX 算子</li>
</ul>
</li>
</ul>
<p><a href="https://zhuanlan.zhihu.com/p/513387413">模型部署入门教程（四）：在 PyTorch 中支持更多 ONNX 算子</a></p>
<h1 id="量化"><a href="#量化" class="headerlink" title="量化"></a>量化</h1><p>量化一般是指把模型的单精度参数（Float32）转化为低精度参数(Int8,Int4)，把推理过程中的浮点运算转化为定点运算。<br><em>（float和int的本质区别在于小数点是否固定）</em></p>
<p>浮点数格式 (float32)：$$V &#x3D; (-1)^s×M×2^E$$</p>
<table>
<thead>
<tr>
<th>符号位s</th>
<th>阶码E</th>
<th>尾数M</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>8</td>
<td>23</td>
</tr>
</tbody></table>
<p>定点数格式 (int8)：</p>
<table>
<thead>
<tr>
<th>符号位</th>
<th>整数位（设定）</th>
<th>小数位(量化系数)</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>4</td>
<td>3</td>
</tr>
</tbody></table>
<p>若整数位占4位，小数位占3位，则其最大精度为0.125，最大值为15.875<br>若整数位占5位，小数位占2位，则其最大精度为0.250，最大值为31.750<br>$$int8&#x3D;float32∗2^3$$<br>$$float32&#x3D;int8&#x2F;2^3$$</p>
<p>浮点运算在运算过程中，小数点的位置是变动的，而定点运算则是固定不变。如果将浮点数转换成定点数，就可以实现一次读取多个数进行计算（1 float32 &#x3D; 4 int8），提高了运算效率。</p>
<blockquote>
<p>8位和16位是指量化的位深度，表示用多少个二进制位来表示每个权重或激活值。在量化时，8位会将每个权重或激活值分成256个不同的离散值，而16位则分为65536个离散值，因此16位的表示范围更广，可以更精确地表示模型中的参数和激活值。但是，使用较高的位深度会增加存储要求和计算成本，因此需要在预测精度和计算开销之间进行权衡。<br><img data-src="https://img2018.cnblogs.com/blog/947235/201905/947235-20190513143437402-715176586.png" width='70%'></p>
</blockquote>
<p>乘一个系数把float类型的小数部分转换成整数部分，然后用这个转换出来的整数进行计算，计算结果再还原成float</p>
<img alt="图 3" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DLdeployquantized.png" width="80%"/>  

<p><a href="https://arxiv.org/pdf/2106.08295.pdf">A White Paper on Neural Network Quantization</a></p>
]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>嵌入式</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello Blog! 从零开始搭建自己的博客网站</title>
    <url>/Hello-blog/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>第一篇博客用来记录搭建该网站并成功发表这篇博客的流程，使用Hexo静态博客框架，托管于Github，参考了多篇文章<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[知乎：GitHub+Hexo 搭建个人网站详细教程](https://zhuanlan.zhihu.com/p/26625249)|
[Hexo官方文档](https://hexo.io/zh-cn/docs/)|
[Next主题官方文档](http://theme-next.iissnan.com/getting-started.html)|
[知乎：hexo的next主题个性化教程:打造炫酷网站](https://zhuanlan.zhihu.com/p/28128674)|
[菜鸟教程：Markdown 教程](https://www.runoob.com/markdown/md-tutorial.html)|
[个人网站：Arrow的笔记本](http://wangyujie.site/)">[1]</span></a></sup>，项目地址：<a href="https://github.com/Arrowes/Blog">Blog</a></p>
<span id="more"></span>
<img alt="图 1" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Hello-blogBlogPhoto.png" width = "30%"/>  

<h3 id="总体流程"><a href="#总体流程" class="headerlink" title="总体流程"></a>总体流程</h3><p>其中主流程参考<a href="https://zhuanlan.zhihu.com/p/26625249">知乎：GitHub+Hexo 搭建个人网站详细教程</a>，虽然是老文章，但每一步都非常详细，框架搭建过程存在问题可以看文章的评论区或<a href="https://hexo.io/zh-cn/docs/">Hexo官方文档</a>进行补充；</p>
<p>域名在<a href="https://dc.console.aliyun.com/next/index?spm=5176.12818093.ProductAndResource--ali--widget-product-recent.dre1.265516d0P13nxv&accounttraceid=b4a1b2c1dcab4588a55aa2f5926041aazltv#/overview">阿里云</a>购买,买的 <em>wangyujie.site</em> 首年6元，这也是唯一的开销，如果愿意使用原网站 <em>&lt;arrowes.github.io&gt;</em> ,这一步甚至可以省略；</p>
<p>选用了Next主题，主题优化参考了<a href="http://theme-next.iissnan.com/getting-started.html">Next主题官方文档</a>以及<a href="https://zhuanlan.zhihu.com/p/28128674">知乎：hexo的next主题个性化教程:打造炫酷网站</a>（其中访问量、统计功能教程已过期,很多配置next主题已内置），可以把网站搭建的花里胡哨；</p>
<p>编写博客使用的Markdown语言通过看<a href="https://www.runoob.com/markdown/md-tutorial.html">菜鸟教程：Markdown 教程</a>非常简单，可以边学边写，使用VScode，安装<code>Markdown Preview Enhanced</code>及<code>markdown image</code>插件；</p>
<p>其他工具有：logo下载：<a href="https://www.iconfont.cn/">iconfont</a>，Next默认的icon网站（灰色为收费图标）：<a href="https://fontawesome.com/icons">Font Awesome</a>，图床url链接生成：<a href="https://sm.ms/">SM.MS</a>，此外，大多数网站需要用到<del>科学上网</del>。</p>
<p>Github网站项目地址：<a href="https://github.com/Arrowes/Arrowes.github.io">Arrowes.github.io</a></p>
<p>用关键词在谷歌里搜到自己的网页：<a href="https://zoharandroid.github.io/2019-08-03-%E8%AE%A9%E8%B0%B7%E6%AD%8C%E6%90%9C%E7%B4%A2%E5%88%B0%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2/">让Google搜索到自己的博客</a></p>
<h3 id="网站配置"><a href="#网站配置" class="headerlink" title="网站配置"></a>网站配置</h3><h4 id="添加动态背景，以动态线条为例："><a href="#添加动态背景，以动态线条为例：" class="headerlink" title="添加动态背景，以动态线条为例："></a>添加动态背景，以动态线条为例：</h4><ol>
<li>themes&#x2F;next&#x2F;layout&#x2F;_layout 在<code>&lt;/body&gt;</code>末尾添加如下代码：  <figure class="highlight html"><table><tr><td class="code"><pre><span class="line">&#123;% if theme.canvas_nest %&#125;</span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">&quot;text/javascript&quot;</span> <span class="attr">src</span>=<span class="string">&quot;//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure></li>
<li>&#x2F;next&#x2F;_config.yml,在里面添加如下代码：(可以放在最后面)  <figure class="highlight css"><table><tr><td class="code"><pre><span class="line">#是否打开动态背景</span><br><span class="line">canvas_nest: true</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="统计功能"><a href="#统计功能" class="headerlink" title="统计功能"></a>统计功能</h4><p>统计人数、阅读次数：&#x2F;next&#x2F;_config：找到busuanzi_count进行配置<br>统计字数、阅读时间： &#x2F;next&#x2F;_config:设置item_text_total为true</p>
<h4 id="配置网站超链接颜色"><a href="#配置网站超链接颜色" class="headerlink" title="配置网站超链接颜色"></a>配置网站超链接颜色</h4><p>打开 <code>Blog\themes\next\source\css\_common\components\post</code> 路径下的post.styl , 并在底部添加如下代码:</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">a</span><span class="selector-pseudo">:not</span>(<span class="selector-class">.btn</span>)&#123;</span><br><span class="line">  <span class="attribute">color</span>:; //超链接显示颜色</span><br><span class="line">  <span class="attribute">border-bottom</span>: none;</span><br><span class="line">  &amp;<span class="selector-pseudo">:hover</span> &#123;</span><br><span class="line">	<span class="attribute">color</span>: <span class="number">#469FF1</span>;  //鼠标移动上去后超链接颜色</span><br><span class="line">	<span class="attribute">font-weight</span>: none;</span><br><span class="line">	<span class="attribute">text-decoration</span>: none;</span><br><span class="line">	&#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>配置博客链接：_config中<code>permalink: :title/</code></p>
<h4 id="搜索功能"><a href="#搜索功能" class="headerlink" title="搜索功能"></a>搜索功能</h4><p>更改主题配置文件 &#x2F;next&#x2F;_config 将local_search下的enable改为true</p>
<h4 id="评论功能"><a href="#评论功能" class="headerlink" title="评论功能"></a>评论功能</h4><p>创建一个 Github Application 用来授权登录，如果没有 <a href="https://github.com/settings/applications/new">点击这里申请</a>, url都填<code>https://github.com/Arrowes/Arrowes.github.io</code></p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"># Gitalk</span><br><span class="line"># For more information: https://gitalk.github.io</span><br><span class="line">gitalk:</span><br><span class="line">  enable: true</span><br><span class="line">  github_id: Arrowes # GitHub repo owner</span><br><span class="line">  repo: Arrowes.github.io # Repository name to store issues</span><br><span class="line">  client_id: <span class="number">0</span>cf14e51c1582cf64289 # GitHub Application Client ID</span><br><span class="line">  client_secret: <span class="number">5</span>e8273d27714e40495267c73e607c******** # GitHub Application Client Secret</span><br><span class="line">  admin_user: Arrowes # GitHub repo owner and collaborators, only these guys can initialize gitHub issues</span><br><span class="line">  distraction_free_mode: true # Facebook-like distraction free mode</span><br><span class="line">  proxy: https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token # This is official proxy address</span><br><span class="line">  language: zh-CN</span><br></pre></td></tr></table></figure>

<h4 id="首页展示文章数"><a href="#首页展示文章数" class="headerlink" title="首页展示文章数"></a>首页展示文章数</h4><figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-id">#Hexo-config</span>文件修改</span><br><span class="line">index_generator:</span><br><span class="line">  path: <span class="string">&#x27;&#x27;</span></span><br><span class="line">  per_page: <span class="number">0</span> #不分页</span><br></pre></td></tr></table></figure>

<h4 id="设置阅读全文"><a href="#设置阅读全文" class="headerlink" title="设置阅读全文"></a>设置阅读全文</h4><ol>
<li><p>在项目根目录下执行 <code>npm install hexo-excerpt --save</code></p>
</li>
<li><p>在站点配置文件&#x2F;hexo&#x2F;_config.yml添加:</p>
 <figure class="highlight css"><table><tr><td class="code"><pre><span class="line">excerpt:			# 一定要顶格写，注意格式</span><br><span class="line">  depth: <span class="number">1</span>			# 他的大小就是全文阅读预览长度设置</span><br><span class="line">  excerpt_excludes: []</span><br><span class="line">  more_excludes: []</span><br><span class="line">  hideWholePostExcerpts: true</span><br></pre></td></tr></table></figure>
</li>
<li><p>在主题配置文件theme&#x2F;next&#x2F;_config中 excerpt_description 改为false</p>
</li>
</ol>
<h4 id="添加标签"><a href="#添加标签" class="headerlink" title="添加标签"></a>添加标签</h4><ol>
<li>配置：主题配置文件中删掉tags的注释</li>
<li>文件：新建tags文件 <code>hexo new page &quot;tags&quot;</code></li>
<li>文章：<code>tags: - XXX</code></li>
</ol>
<h4 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h4><p><a href="https://blog.csdn.net/konglongdanfo1/article/details/85204312">markdown公式符号大全</a><br>主题配置文件：<code>math:   every_page: true   mathjax:  enable: true</code></p>
<h4 id="hexo-添加自定义单静态页面-跳过hexo渲染，以resume为例"><a href="#hexo-添加自定义单静态页面-跳过hexo渲染，以resume为例" class="headerlink" title="hexo 添加自定义单静态页面 跳过hexo渲染，以resume为例:"></a>hexo 添加自定义单静态页面 跳过hexo渲染，以resume为例:</h4><ol>
<li>将resume文件夹放进Theme主题文件夹下的&#x2F;source</li>
<li>Hexo-config: skip_render: resume&#x2F;** （可省略）</li>
<li>引用时直接 &#x2F;resume&#x2F;</li>
</ol>
<h4 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h4><p><code>npm install hexo-filter-mermaid-diagrams</code><br>主题配置文件：<code>mermaid:  enable: true</code></p>
<h3 id="markdown插件"><a href="#markdown插件" class="headerlink" title="markdown插件"></a>markdown插件</h3><p><code>npm install hexo-reference --save</code> 支持Markdown脚注<br><code>npm install hexo-wordcount --save</code> 字数统计</p>
<p><strong>Markdown Preview Enhanced</strong> markdown预览插件（vs code）<br>如果无法正常导出：Chrome Puppeteer导出PDF &gt; 搜索“chrome”，在相应选项中填入你的浏览器的“chrome.exe”文件的地址即可<br><code>C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe</code></p>
<p><strong>Markdown 粘贴插件</strong><br>Markdown paste 右键智能粘贴<br>Paste URL Ctrl+Alt+P快速粘贴链接</p>
<p><strong>markdown image</strong> 图片插件（vs code）<br> <figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">Local</span><br><span class="line">E:\Github\Blog\blog\node_modules\hexo-theme-<span class="keyword">next</span>\source\images\ <span class="comment">#Path</span></span><br><span class="line"><span class="regexp">/images/</span> <span class="comment">#Reference Path</span></span><br><span class="line"></span><br><span class="line">Github</span><br><span class="line">main <span class="comment">#Branch</span></span><br><span class="line">/images <span class="comment">#Path</span></span><br><span class="line">https:<span class="regexp">//gi</span>thub.com<span class="regexp">/Arrowes/</span>Blog <span class="comment">#Repository</span></span><br><span class="line">github_pat_11AV245NA0ZkjSgYjmtK5T_fyxLh1yhNVlT13FFsC <span class="comment">#TokenMjaEB4LKd</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#使用raw.gitmirror.com代替raw.githubusercontent.com,实现github图片加速</span></span><br><span class="line">https:<span class="regexp">//</span>raw.gitmirror.com<span class="regexp">/$&#123;username&#125;/</span><span class="variable">$&#123;repository&#125;</span><span class="regexp">/$&#123;branch&#125;/</span><span class="variable">$&#123;filepath&#125;</span> <span class="comment">#Github: Cdn</span></span><br><span class="line">q3mbRZwwnnKUXHcibZg6UN8ulCHE2UDXMjaEB4LKd</span><br></pre></td></tr></table></figure></p>
<h3 id="常用Hexo指令"><a href="#常用Hexo指令" class="headerlink" title="常用Hexo指令"></a>常用<a href="https://hexo.io/zh-cn/docs/commands">Hexo指令</a></h3><p><code>hexo init [folder]</code> 新建一个网站<br><code>hexo new &quot;title&quot;</code> 新建一篇文章<br><code>hexo clean</code> 清除缓存文件 (db.json) 和已生成的静态文件 (public)<br><code>hexo g</code> 生成静态文件<br><code>hexo s</code> 生成本地预览 <a href="localhost:4000">localhost:4000</a><br><code>hexo d</code> 部署网站</p>
<p><code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo s</code> 快速预览<br><code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code> 快速部署</p>
<h3 id="Bug"><a href="#Bug" class="headerlink" title="Bug"></a>Bug</h3><p>√ 网站底部的图标不显示:Font Awesome部分图标收费<br>√ 访客数、文章字数没有数据：busuanzi链接过期<br>√ 生成的文章目录结构混乱：避免写跨级结构<br>部署经常超时 error：spawn failed，多试几次</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>熟悉了搭建网站流程，想起本科打电子商务比赛花钱请人做网站，还是本地的静态网站，有点冤种；<br>对Github的工作流有了一定了解，比一键<em>Download ZIP</em>有所进步；<br>看了很多人写的教程才完成，花了整整两天，还是有点费时间的，除了瞎折腾，更多的其实是想搭建一个<strong>输出</strong>的平台，锻炼一下自己的表达、总结能力，改善一下自己学了就忘，忘了就废的情况，希望自己能继续坚持，多写几篇！</p>
<div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://zhuanlan.zhihu.com/p/26625249">知乎：GitHub+Hexo 搭建个人网站详细教程</a>|
<a href="https://hexo.io/zh-cn/docs/">Hexo官方文档</a>|
<a href="http://theme-next.iissnan.com/getting-started.html">Next主题官方文档</a>|
<a href="https://zhuanlan.zhihu.com/p/28128674">知乎：hexo的next主题个性化教程:打造炫酷网站</a>|
<a href="https://www.runoob.com/markdown/md-tutorial.html">菜鸟教程：Markdown 教程</a>|
<a href="http://wangyujie.site/">个人网站：Arrow的笔记本</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title>Embedded：嵌入式应用知识</title>
    <url>/Embedded/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>嵌入式学习笔记，包括电容、PCB布局布线、RTOS等，嵌入式项目主页：<a href="https://oshwhub.com/arrows">arrows-立创开源平台</a></p>
<span id="more"></span>

<h1 id="相关常识"><a href="#相关常识" class="headerlink" title="相关常识"></a>相关常识</h1><h2 id="电容"><a href="#电容" class="headerlink" title="电容"></a>电容</h2><p>电容是电路设计中最为普通常用的器件，是无源元件之一，有源器件简单地说就是需能(电)源的器件叫有源器件，无需能(电)源的器件就是无源器件。<br>电容的作用和用途一般都有好多种，如：在旁路、去耦、滤波、储能方面的作用；在完成振荡、同步以及时间常数的作用：<br><strong>隔直流</strong>：作用是阻止直流通过而让交流通过。<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded23.png" width = "50%" /></p>
<p><strong>旁路（去耦）</strong>：为交流电路中某些并联的元件提供低阻抗通路。<br><img alt="图 24" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded24.png" width = "50%"/>  </p>
<p><strong>旁路电容</strong>：旁路电容，又称为退耦电容，是为某个器件提供能量的储能器件。<br>它利用了电容的频率阻抗特性，理想电容的频率特性随频率的升高，阻抗降低，就像一个水塘，它能使输出电压输出均匀，降低负载电压波动。<br>旁路电容要尽量靠近负载器件的供电电源管脚和地管脚，这是阻抗要求。<br>在画PCB时候特别要注意，只有靠近某个元器件时候才能抑制电压或其他输信号因过大而导致的地电位抬高和噪声。<br>说白了就是把直流电源中的交流分量，通过电容耦合到电源地中，起到了净化直流电源的作用。如图为旁路电容，画图时候要尽量靠近IC1。<br> <img alt="图 25" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded25.png" width = "50%"/>  </p>
<p><strong>去耦电容</strong>：去耦电容，是把输出信号的干扰作为滤除对象，去耦电容相当于电池，利用其充放电，使得放大后的信号不会因电流的突变而受干扰。<br>它的容量根据信号的频率、抑制波纹程度而定，去耦电容就是起到一个“电池”的作用，满足驱动电路电流的变化，避免相互间的耦合干扰。<br><strong>旁路电容实际也是去耦合的</strong>，只是旁路电容一般是指高频旁路，也就是给高频的开关噪声提高一条低阻抗泄防途径。<br>高频旁路电容一般比较小，根据谐振频率一般取 0.1μF、0.01μF 等。<br>而去耦合电容的容量一般较大，可能是 10μF 或者更大，依据电路中分布参数、以及驱动电流的变化大小来确定。上图C3为去耦电容<br><strong>区别</strong>：旁路是把输入信号中的干扰作为滤除对象，而去耦是把输出信号的干扰作为滤除对象，防止干扰信号返回电源。<br><strong>耦合</strong>：作为两个电路之间的连接，允许交流信号通过并传输到下一级电路 。<br><img alt="图 26" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded26.png" width = "50%"/>  </p>
<p>用电容做耦合的元件，是为了将前级信号传递到后一级，并且隔断前一级的直流对后一级的影响，使电路调试简单，性能稳定。<br>如果不加电容交流信号放大不会改变，只是各级工作点需重新设计，由于前后级影响，调试工作点非常困难，在多级时几乎无法实现。<br><strong>滤波</strong>：这个对电路而言很重要，CPU背后的电容基本都是这个作用。<br>即频率f越大，电容的阻抗Z越小。当低频时，电容C由于阻抗Z比较大，有用信号可以顺利通过；当高频时，电容C由于阻抗Z已经很小了，相当于把高频噪声短路到GND上去了。<br><img alt="图 27" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded27.png" width = "50%"/>  <img alt="图 39" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded28.png" width = "50%"/>  </p>
<p><strong>滤波作用</strong>：理想电容，电容越大，阻抗越小，通过的频率也越高。<br>电解电容一般都是超过 1uF ，其中的电感成份很大，因此频率高后反而阻抗会大。<br>我们经常看见有时会看到有一个电容量较大电解电容并联了一个小电容，其实大的电容通低频，小电容通高频，这样才能充分滤除高低频。<br>电容频率越高时候则衰减越大，电容像一个水塘，几滴水不足以引起它的很大变化，也就是说电压波动不是你很大时候电压可以缓冲，如图C2：<br> <img alt="图 29" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded29.png" width = "50%"/>  </p>
<p><strong>温度补偿</strong>：针对其它元件对温度的适应性不够带来的影响，而进行补偿，改善电路的稳定性。<br>  <img alt="图 30" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded30.png" />  </p>
<p><strong>分析</strong>：由于定时电容的容量决定了行振荡器的振荡频率，所以要求定时电容的容量非常稳定，不随环境湿度变化而变化，这样才能使行振荡器的振荡频率稳定。<br>因此采用正、负温度系数的电容释联，进行温度互补。<br>当工作温度升高时，Cl的容量在增大，而C2的容量在减小，两只电容并联后的总容量为两只电容容量之和，由于一个容量在增大而另一个在减小，所以总容量基本不变。<br>同理，在温度降低时，一个电容的容量在减小而另一个在增大，总的容量基本不变，稳定了振荡频率，实现温度补偿目的。<br><strong>计时</strong>：电容器与电阻器配合使用，确定电路的时间常数。<br> <img alt="图 31" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded31.png" />  </p>
<p>输入信号由低向高跳变时，经过缓冲1后输入RC电路。<br>电容充电的特性使B点的信号并不会跟随输入信号立即跳变，而是有一个逐渐变大的过程。<br>当变大到一定程度时，缓冲2翻转，在输出端得到了一个延迟的由低向高的跳变。<br><strong>时间常数</strong>：以常见的 RC 串联构成积分电路为例，当输入信号电压加在输入端时，电容上的电压逐渐上升。<br>而其充电电流则随着电压的上升而减小，电阻R和电容C串联接入输入信号VI，由电容C输出信号V0，当RC (τ)数值与输入方波宽度tW之间满足：τ》》tW，这种电路称为积分电路。<br><strong>调谐</strong>：对与频率相关的电路进行系统调谐，比如手机、收音机、电视机。<br><img alt="图 32" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded32.png" width = "50%"/>  </p>
<p>因为lc调谐的振荡电路的谐振频率是lc的函数，我们发现振荡电路的最大与最小谐振频率之比随着电容比的平方根变化。<br>此处电容比是指反偏电压最小时的电容与反偏电压最大时的电容之比。<br>因而，电路的调谐特征曲线（偏压一谐振频率）基本上是一条抛物线。<br><strong>整流</strong>：在预定的时间开或者关半闭导体开关元件。<br><img alt="图 44" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded33.png" width = "70%"/>  </p>
<p><strong>储能</strong>：储存电能，用于必须要的时候释放。<br>例如相机闪光灯，加热设备等等．（如今某些电容的储能水平己经接近锂电池的水准，一个电容储存的电能可以供一个手机使用一天。<br>一般地，电解电容都会有储能的作用，对于专门的储能作用的电容，电容储能的机理为双电层电容以及法拉第电容。<br>其主要形式为超级电容储能，其中超级电容器是利用双电层原理的电容器。<br>当外加电压加到超级电容器的两个极板上时，与普通电容器一样，极板的正电极存储正电荷，负极板存储负电荷。<br>在超级电容器的两极板上电荷产生的电场作用下，在电解液与电极间的界面上形成相反的电荷，以平衡电解液的内电场。<br>这种正电荷与负电荷在两个不同相之间的接触面上，以正负电荷之间极短间隙排列在相反的位置上，这个电荷分布层叫做双电层，因此电容量非常大。</p>
<h1 id="PCB布局布线"><a href="#PCB布局布线" class="headerlink" title="PCB布局布线"></a>PCB布局布线</h1><h2 id="布局"><a href="#布局" class="headerlink" title="布局"></a>布局</h2><p>元器件布局的10条规则：</p>
<ol>
<li>遵照<strong>先大后小，先难后易</strong>的布置原则，即重要的单元电路、核心元器件应当优先布局。</li>
<li>布局中应参考原理框图，根据单板的主信号流向规律安排主要元器件。</li>
<li>元器件的排列要便于调试和维修，亦即小元件周围不能放置大元件、需调试的元器件周围要有足够的空间。 </li>
<li>相同结构电路部分，尽可能采用“对称式”标准布局。</li>
<li>按照均匀分布、重心平衡、版面美观的标准优化布局。 </li>
<li>同类型插装元器件在X或Y方向上应朝一个方向放置。同一种类型的有极性分立元件也要力争在X或Y方向上保持一致，便于生产和检验。 </li>
<li>发热元件一般应均匀分布，以利于单板和整机的散热，除温度检测元件以外的温度敏感器件应远离发热量大的元器件。 </li>
<li>布局应尽量满足以下要求：总的连线尽可能短，关键信号线最短；高电压、大电流信号与小电流，低电压的弱信号完全分开；模拟信号与数字信号分开；高频信号与低频信号分开；高频元器件的间隔要充分。</li>
<li>去耦电容的布局要尽量靠近IC的电源管脚，并使之与电源和地之间形成的回路最短。 </li>
<li>元件布局时，应适当考虑使用同一种电源的器件尽量放在一起, 以便于将来的电源分隔。</li>
</ol>
<h2 id="布线"><a href="#布线" class="headerlink" title="布线"></a>布线</h2><h3 id="（1）布线优先次序"><a href="#（1）布线优先次序" class="headerlink" title="（1）布线优先次序"></a>（1）布线优先次序</h3><p>关键信号线优先：模拟小信号、高速信号、时钟信号和同步信号等关键信号优先布线<br>密度优先原则：从单板上连接关系最复杂的器件优先布线。从单板上连线 最密集的区域开始布线<br>注意点：</p>
<blockquote>
<p>a. 尽量为时钟信号、高频信号、敏感信号等关键信号提供专门的布线层，并保证其最小的回路面积。必要时应采取手工优先布线、屏蔽和加大安全间距等方法。保证信号质量。<br>b. 电源层和地层之间的EMC环境较差，应避免布置对干扰敏感的信号。<br>c. 有阻抗控制要求的网络应尽量按线长线宽要求布线。 </p>
</blockquote>
<h3 id="（2）四种具体走线方式"><a href="#（2）四种具体走线方式" class="headerlink" title="（2）四种具体走线方式"></a>（2）四种具体走线方式</h3><h4 id="1-时钟的布线："><a href="#1-时钟的布线：" class="headerlink" title="1. 时钟的布线："></a>1. 时钟的布线：</h4><p>时钟线是对EMC 影响最大的因素之一。在时钟线上应少打过孔，尽量避免和其它信号线并行走线，且应远离一般信号线，避免对信号线的干扰。同时应避开板上的电源部分，以防止电源和时钟互相干扰。<br>如果板上有专门的时钟发生芯片，其下方不可走线，应在其下方铺铜，必要时还可以对其专门割地。对于很多芯片都有参考的晶体振荡器，这些晶振下方也不应走线，要铺铜隔离。<br><img alt="图 45" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded1.png" width = "80%"/>  </p>
<h4 id="2-直角走线"><a href="#2-直角走线" class="headerlink" title="2. 直角走线:"></a>2. 直角走线:</h4><p>直角走线一般是PCB布线中要求尽量避免的情况，也几乎成为衡量布线好坏的标准之一，那么直角走线究竟会对信号传输产生多大的影响呢？从原理上说，直角走线会使传输线的线宽发生变化，造成阻抗的不连续。其实不光是直角走线，顿角，锐角走线都可能会造成阻抗变化的情况。</p>
<p>直角走线的对信号的影响就是主要体现在三个方面：</p>
<blockquote>
<p>一是拐角可以等效为传输线上的容性负载，减缓上升时间；<br>二是阻抗不连续会造成信号的反射；<br>三是直角尖端产生的EMI。</p>
</blockquote>
<h4 id="3-差分走线"><a href="#3-差分走线" class="headerlink" title="3. 差分走线:"></a>3. 差分走线:</h4><p>参看：Altium Designer – 差分布线和阻抗匹配<br><strong>差分信号</strong>（Differential Signal）在高速电路设计中的应用越来越广泛，电路中最关键的信号往往都要采用差分结构设计.定义:通俗地说，就是驱动端发送两个等值、反相的信号，接收端通过比较这两个电压的差值来判断逻辑状态“0”还是“1”。而承载差分信号的那一对走线就称为差分走线。<br>差分信号和普通的单端信号走线相比，最明显的优势体现在以下三个方面：</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">a</span>.抗干扰能力强，因为两根差分走线之间的耦合很好，当外界存在噪声干扰时，几乎是同时被耦合到两条线上，而接收端关心的只是两信号的差值，所以外界的共模噪声可以被完全抵消。</span><br><span class="line"><span class="selector-tag">b</span>.能有效抑制EMI，同样的道理，由于两根信号的极性相反，他们对外辐射的电磁场可以相互抵消，耦合的越紧密，泄放到外界的电磁能量越少。</span><br><span class="line">c.时序定位精确，由于差分信号的开关变化是位于两个信号的交点，而不像普通单端信号依靠高低两个阈值电压判断，因而受工艺，温度的影响小，能降低时序上的误差，同时也更适合于低幅度信号的电路。目前流行的LVDS（low voltage differential signaling）就是指这种小振幅差分信号技术。</span><br></pre></td></tr></table></figure>
<p>对于PCB工程师来说，最关注的还是如何确保在实际走线中能完全发挥差分走线的这些优势。也许只要是接触过Layout的人都会了解差分走线的一般要求，那就是“等长、等距”。</p>
<p>等长是为了保证两个差分信号时刻保持相反极性，减少共模分量；等距则主要是为了保证两者差分阻抗一致，减少反射。“尽量靠近原则”有时候也是差分走线的要求之一。</p>
<h4 id="4-蛇形线"><a href="#4-蛇形线" class="headerlink" title="4. 蛇形线:"></a>4. 蛇形线:</h4><p>蛇形线是Layout中经常使用的一类走线方式。其主要目的就是为了调节延时，满足系统时序设计要求。<br>设计者首先要有这样的认识：<br>蛇形线会破坏信号质量，改变传输延时，布线时要尽量避免使用。但实际设计中，为了保证信号有足够的保持时间，或者减小同组信号之间的时间偏移，往往不得不故意进行绕线。<br>注意点:<br>成对出现的差分信号线，一般平行走线，尽量少打过孔，必须打孔时，应两线一同打孔，以做到阻抗匹配。<br>相同属性的一组总线，应尽量并排走线，做到尽量等长。从贴片焊盘引出的过孔尽量离焊盘远些。<br><img alt="图 2" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded2.png" width = "60%"/>  </p>
<h3 id="（3）布线常用规则"><a href="#（3）布线常用规则" class="headerlink" title="（3）布线常用规则"></a>（3）布线常用规则</h3><ol>
<li><p>走线的方向控制规则：<br>即相邻层的走线方向成正交结构。避免将不同的信号线在相邻层走成同一方向，以减少不必要的层间串扰；<br>当由于板结构限制（如某些背板）难以避免出现该情况，特别是信号速率较高时，应考虑用地平面隔离各布线层，用地信号线隔离各信号线。 </p>
<img alt="图 3" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded3.png" width = "50%"/>  
</li>
<li><p>走线的开环检查规则：<br>一般不允许出现一端浮空的布线（Dangling Line), 主要是为了避免产生”天线效应”，减少不必要的干扰辐射和接受，否则可能带来不可预知的结果。   </p>
<img alt="图 4" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded4.png" width = "50%"/>  
</li>
<li><p>阻抗匹配检查规则：<br>同一网络的布线宽度应保持一致，线宽的变化会造成线路特性阻抗的不均匀，当传输的速度较高时会产生反射，在设计中应该尽量避免这种情况。<br>在某些条件下，如接插件引出线，BGA封装的引出线类似的结构时，可能无法避免线宽的变化，应该尽量减少中间不一致部分的有效长度。</p>
<img alt="图 5" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded5.png" width = "50%"/>  
</li>
<li><p>走线长度控制规则：<br>即短线规则，在设计时应该尽量让布线长度尽量短，以减少由于走线过长带来的干扰问题，特别是一些重要信号线，如时钟线，务必将其振荡器放在离器件很近的地方。对驱动多个器件的情况，应根据具体情况决定采用何种网络拓扑结构。 </p>
<img alt="图 6" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded6.png" width = "50%"/>  
</li>
<li><p>倒角规则：<br>PCB设计中应避免产生锐角和直角， 产生不必要的辐射，同时工艺性能也不好。</p>
 <img alt="图 7" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded7.png" width = "50%"/>  
</li>
<li><p>器件去耦规则：<br>A. 在印制版上增加必要的去耦电容，滤除电源上的干扰信号，使电源信号稳定。<br>在多层板中，对去耦电容的位置一般要求不太高，但对双层板，去耦电容的布局及电源的布线方式将直接影响到整个系统的稳定性，有时甚至关系到设计的成败。<br>B. 在双层板设计中，一般应该使电流先经过滤波电容滤波再供器件使用。<br>C. 在高速电路设计中，能否正确地使用去耦电容，关系到整个板的稳定性。 </p>
<img alt="图 8" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded8.png" width = "50%"/>  
</li>
<li><p>器件布局分区&#x2F;分层规则：<br>A. 主要是为了防止不同工作频率的模块之间的互相干扰，同时尽量缩短高频部分的布线长度。<br>B. 对混合电路，也有将模拟与数字电路分别布置在印制板的两面，分别使用不同的层布线，中间用地层隔离的方式。</p>
<img alt="图 9" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded9.png" width = "50%"/>  
</li>
<li><p>地线回路规则：<br>环路最小规则，即信号线与其回路构成的环面积要尽可能小，环面积越小，对外的辐射越少，接收外界的干扰也越小。 </p>
<img alt="图 10" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded10.png" width = "50%"/>  
</li>
<li><p>电源与地线层的完整性规则：<br>对于导通孔密集的区域，要注意避免孔在电源和地层的挖空区域相互连接，形成对平面层的分割，从而破坏平面层的完整性，并进而导致信号线在地层的回路面积增大。</p>
<img alt="图 11" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded11.png" width = "50%"/>  
</li>
<li><p>3W规则：<br>为了减少线间串扰，应保证线间距足够大，当线中心间距不少于3倍线宽时，则可保持70%的电场不互相干扰，称为3W规则。如要达到98%的电场不互相干扰，可使用10W的间距。</p>
 <img alt="图 12" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded12.png" width = "50%"/>  
</li>
<li><p>屏蔽保护：<br>对应地线回路规则，实际上也是为了尽量减小信号的回路面积，多见于一些比较重要的信号，如时钟信号，同步信号；<br>对一些特别重要，频率特别高的信号，应该考虑采用铜轴电缆屏蔽结构设计，即将所布的线上下左右用地线隔离，而且还要考虑好如何有效的让屏蔽地与实际地平面有效结合。 </p>
 <img alt="图 13" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded13.png" width = "50%"/>  
</li>
<li><p>走线终结网络规则：<br>在高速数字电路中， 当PCB布线的延迟时间大于信号上升时间（或下降时间） 的1&#x2F;4时，该布线即可以看成传输线，为了保证信号的输入和输出阻抗与传输线的阻抗正确匹配，可以采用多种形式的匹配方法， 所选择的匹配方法与网络的连接方式和布线的拓朴结构有关。<br>A. 对于点对点（一个输出对应一个输入） 连接， 可以选择始端串联匹配或终端并联匹配。前者结构简单，成本低，但延迟较大。后者匹配效果好，但结构复杂，成本较高。<br>B. 对于点对多点（一个输出对应多个输出） 连接， 当网络的拓朴结构为菊花链时，应选择终端并联匹配。<br>当网络为星型结构时，可以参考点对点结构。星形和菊花链为两种基本的拓扑结构, 其他结构可看成基本结构的变形, 可采取一些灵活措施进行匹配。<br>在实际操作中要兼顾成本、 功耗和性能等因素， 一般不追求完全匹配，只要将失配引起的反射等干扰限制在可接受的范围即可。</p>
<img alt="图 14" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded14.png" width = "50%"/>  
</li>
<li><p>走线闭环检查规则：<br>防止信号线在不同层间形成自环。在多层板设计中容易发生此类问题， 自环将引起辐射干扰。</p>
<img alt="图 15" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded15.png" width = "50%"/>  
</li>
<li><p>走线的分枝长度控制规则：<br>尽量控制分枝的长度，一般的要求是Tdelay&lt;&#x3D;Trise&#x2F;20。</p>
 <img alt="图 16" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded16.png" width = "50%"/>  
</li>
<li><p>走线的谐振规则：<br>主要针对高频信号设计而言， 即布线长度不得与其波长成整数倍关系， 以免产生谐振现象。</p>
<img alt="图 17" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded17.png" width = "50%"/>  
</li>
<li><p>孤立铜区控制规则：<br>孤立铜区的出现， 将带来一些不可预知的问题， 因此将孤立铜区与别的信号相接， 有助于改善信号质量，通常是将孤立铜区接地或删除。<br>在实际的制作中， PCB厂家将一些板的空置部分增加了一些铜箔，这主要是为了方便印制板加工，同时对防止印制板翘曲也有一定的作用。</p>
<img alt="图 18" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded18.png" width = "50%"/>  
</li>
<li><p>重叠电源与地线层规则：<br>不同电源层在空间上要避免重叠。主要是为了减少不同电源之间的干扰， 特别是一些电压相差很大的电源之间， 电源平面的重叠问题一定要设法避免， 难以避免时可考虑中间隔地层。</p>
<img alt="图 19" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded19.png" width = "50%"/>  
</li>
<li><p>20H规则：<br>由于电源层与地层之间的电场是变化的， 在板的边缘会向外辐射电磁干扰。称为边沿效应。<br>解决的办法是将电源层内缩， 使得电场只在接地层的范围内传导。以一个H（电源和地之间的介质厚度）为单位，若内缩20H则可以将70%的电场限制在接地层边沿内；内缩100H则可以将98%的电场限制在内。 </p>
<img alt="图 20" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded20.png" width = "50%"/></li>
</ol>
<h3 id="（4）其他"><a href="#（4）其他" class="headerlink" title="（4）其他"></a>（4）其他</h3><p>对于单双层板电源线应尽量粗而短。电源线和地线的宽度要求可以根据1mm的线宽最大对应1A 的电流来计算，电源和地构成的环路尽量小。<br> <img alt="图 21" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded21.png" width = "60%"/>  </p>
<p>为了防止电源线较长时，电源线上的耦合杂讯直接进入负载器件，应在进入每个器件之前，先对电源去耦。且为了防止它们彼此间的相互干扰，对每个负载的电源独立去耦，并做到先滤波再进入负载。<br>在布线中应保持接地良好。如下图。<br><img alt="图 22" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Embedded22.png" width = "60%"/>  </p>
<p><a href="https://blog.csdn.net/qq_36347513/article/details/121873555">立创EDA PCB设计</a></p>
<h1 id="RTOS"><a href="#RTOS" class="headerlink" title="RTOS"></a>RTOS</h1><p>应用程序是一个无限的循环，循环中调用相应的函数完成相应的操作,这部分可以看成后台行为。前台程序通过中断来处理事件；后台程序则掌管整个嵌入式系统软、硬件资源的分配、管理以及任务的调度，是一个系统管理调度程序。这就是通常所说的前后台系统。<br>前后台系统的优点：  </p>
<ol>
<li>上手简单， 2. 资源消耗少。</li>
</ol>
<p>前后台系统的缺点：  </p>
<ol>
<li>实时性问题。例如在执行task2的时候，突然发生了按键事件，这个时候需要 轮流执行完task2到task10，如果task2到task10之间的耗时较多，那么就不能及时的显示刚按下去的按键值。当while中的任务越复杂的时候，实时性就越差。  </li>
<li>编程难度较大。由于其实时性问题，导致了我们必须保证各个任务尽量的耗时少，这就大大增加了编程的难度。需要考虑处理延时delay问题和稳定性，如果在某个task里while卡死，那么整个系统就卡死了。所以对编写程序的要求较高。</li>
</ol>
<p>RTOS即实时操作系统（Real Time Operating System）。<br>实时操作系统是指当外界事件或数据产生时，能够接受并以足够快的速度予以处理，其处理的结果又能在规定的时间之内来控制生产过程或对处理系统做出快速响应，调度一切可利用的资源完成实时任务，并控制所有实时任务协调一致运行的操作系统。提供及时响应和高可靠性是其主要特点。</p>
<p>RTOS和前后台系统的最大区别就是支持多任务了。每个任务都是相互独立的。对于前后台系统来说，所有的任务都是能放在一个while大循环里轮流执行，而RTOS 则将每个任务分隔开来。可以在各自的while里运行。 各个任务同时进行，不必像前后台那样，需要等待前一个任务跑完才能执行。<br>当然，这并不是真正的并行执行。而是RTOS帮我们在适当的时候，非常快速的切换到另外一个任务，所以看起来就像在并行执行一样。实际上只是一个任务跑一点，cpu再切到另外一个任务执行一点。当然，如果多核的话，那就真的存在并行执行了。  </p>
<p>RTOS的优点：</p>
<ol>
<li>实时性较高，不必像前后台那样需要等待其他任务完成才能执行。  </li>
<li>利于开发、开发周期短，难度降低。  </li>
<li>系统稳定性好，当然需要一个好的RTOS。</li>
</ol>
<p>RTOS的缺点：  </p>
<ol>
<li>开销较大，系统需要占一定的资源。  </li>
<li>上手难度相对较大</li>
</ol>
]]></content>
      <tags>
        <tag>嵌入式</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux：Ubuntu，Git，Docker</title>
    <url>/Linux/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Linux常用指令，Ubuntu虚拟机使用指南，Git工作流，Docker基本概念</p>
<span id="more"></span>

<h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#系统</span></span><br><span class="line">lsb_release -a      <span class="comment">#查ubuntu版本    </span></span><br><span class="line">sudo passwd root    <span class="comment">#更改root账户的密码</span></span><br><span class="line">su                  <span class="comment">#切换到root用户，$ 是普通权限， #是管理员权限</span></span><br><span class="line">su username         <span class="comment">#切换到其他用户</span></span><br><span class="line">sudo usermod -aG sudo username   <span class="comment">#添加用户为root</span></span><br><span class="line">ps aux; <span class="built_in">kill</span> [PID]  <span class="comment">#查看进程; 根据进程号杀后台</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#安装 换源</span></span><br><span class="line">sudo apt-get install [] <span class="comment">#安装 失败则换源</span></span><br><span class="line">sudo gedit /ect/apt/source.list <span class="comment">#最后一行加入其他源</span></span><br><span class="line">sudo apt update	    <span class="comment">#更新索引(源)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#文件</span></span><br><span class="line"><span class="built_in">mkdir</span> []            <span class="comment">#新建文件夹  </span></span><br><span class="line"><span class="built_in">rmdir</span> []            <span class="comment">#删除文件夹( rm -r XXX  </span></span><br><span class="line">sudo nautilus       <span class="comment">#以root进入文件夹     </span></span><br><span class="line"><span class="built_in">touch</span> [] []         <span class="comment">#创建文件</span></span><br><span class="line"><span class="built_in">rm</span> []               <span class="comment">#删除文件</span></span><br><span class="line"><span class="built_in">mv</span> file1 [<span class="built_in">dir</span>]      <span class="comment">#移动文件 (无dir则相当于重命名)</span></span><br><span class="line"><span class="built_in">cp</span> -r [] []         <span class="comment">#复制文件</span></span><br><span class="line"><span class="built_in">chmod</span> u+x []        <span class="comment">#添加可执行文件</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> -    <span class="comment">#切换到上一工作目录</span></span><br><span class="line"><span class="built_in">cd</span> ~    <span class="comment">#导航到主目录 /home/user1</span></span><br><span class="line"><span class="comment">#.当前目录 ..父目录； cd XX相对路径，cd /XX绝对路径</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">ls</span>                  <span class="comment">#检索</span></span><br><span class="line">tree []             <span class="comment">#查看树状图</span></span><br><span class="line">wget [url]          <span class="comment">#下载, wget -O myfile.zip [url] 重命名文件</span></span><br><span class="line">vi []               <span class="comment">#命令行进入文件，按i进入插入模式，按Esc返回命令模式并输入:wq 保存退出, :q! 不保存退出</span></span><br><span class="line">gedit []            <span class="comment">#图形界面进入文件直接编辑</span></span><br><span class="line">whereis []          <span class="comment">#查找</span></span><br><span class="line">find / -name <span class="string">&quot;[]&quot;</span>   <span class="comment">#查找 find *XXX*</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> 变量名=值     <span class="comment">#设置或更新环境变量的值</span></span><br><span class="line"><span class="comment">#配置永久环境变量更方便，sudo gedit /etc/profile，末尾加入如上代码，然后source /etc/profile加载立即生效</span></span><br><span class="line"><span class="built_in">echo</span> []             <span class="comment">#输出指定的字符串或变量的值,用于调试程序、输出信息</span></span><br><span class="line">script -f log.txt   <span class="comment">#输出terminal内容到文件 exit退出记录，或在指令后加入 &gt; log.txt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#tar打包</span></span><br><span class="line">tar -cvf XX.tar XX  <span class="comment">#将XX文件夹打包为XX.tar文件</span></span><br><span class="line">tar -tf XX.tar      <span class="comment">#查看tar内容</span></span><br><span class="line">tar -xf []          <span class="comment">#解包 -C /path </span></span><br><span class="line"><span class="comment">#tar压缩</span></span><br><span class="line">tar -zcvf XX.tar.gz XX <span class="comment">#压缩XX（ 排除：XX前加--exclude=dataset）</span></span><br><span class="line">tar -xf XX.tar.gz XX   <span class="comment">#解压 -C /path</span></span><br><span class="line"><span class="comment">#zip</span></span><br><span class="line">zip -r XX.zip XX XX.txt <span class="comment">#压缩XX以及XX.txt（排除：加-x &quot;./XX/X&quot;）</span></span><br><span class="line">unzip *.zip -d /path    <span class="comment">#解压</span></span><br></pre></td></tr></table></figure>
<h1 id="Ubuntu"><a href="#Ubuntu" class="headerlink" title="Ubuntu"></a>Ubuntu</h1><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> /proc/version   <span class="comment">#查版本信息</span></span><br><span class="line">lsb_release -a      <span class="comment">#查ubuntu版本</span></span><br><span class="line">free -m             <span class="comment">#查内存使用情况</span></span><br><span class="line"><span class="built_in">df</span> -hl              <span class="comment">#查看磁盘剩余空间</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>截图：1.screen截图应用程序   2.设置-设备-键盘-截图快捷键</p>
<h2 id="虚拟机"><a href="#虚拟机" class="headerlink" title="虚拟机"></a>虚拟机</h2><p><a href="https://www.virtualbox.org/wiki/Downloads">Virtual Box</a> + <a href="http://releases.ubuntu.com/20.04/">Ubuntu 20.04</a>, 或<a href="https://releases.ubuntu.com/bionic/">18.04</a>（速度慢则换<a href="https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/20.04/">镜像源</a>）</p>
<blockquote>
<p>新建 &gt; 导入.iso镜像 &gt; 配置（分4G内存,100G硬盘）<br>设置 &gt; 共享文件夹 &gt; 添加（自动挂载，固定分配）<br>设置 &gt; 共享粘贴板、拖放 &gt; 双向<br>挂载U盘：USB &gt; USB设置 &gt; 添加一个USB &gt; 在ubuntu设备中勾选</p>
</blockquote>
<p>Host 键:右ctrl，方向键上：获取上次的命令，Tab：自动补全<br><code>Ctrl + alt + T</code>	Terminal<br><code>Ctrl + H</code>	显示隐藏文件   </p>
<blockquote>
<p>Debug:<br>VirtualBox安装 64位的Ubuntu系统，在安装时没有显示64位的Linux安装项: 1.CPU要是64位, 2.CPU开启了虚拟化 &gt; <code>进入BOIS &gt; Security&gt; Virtualization &gt; Enable</code> 还是不行则要查看Win10系统安装了自带的Hyper-V虚拟机是否占用了CPU虚拟化技术，将其卸载</p>
</blockquote>
<h2 id="学术加速"><a href="#学术加速" class="headerlink" title="学术加速"></a>学术加速</h2><h3 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h3><ol>
<li>找一个合适的梯子，建议直接买付费的，稳定快速，推荐 <a href="sockboom.link">SockBoom</a>, 购买后会得到一个订阅地址，类似于<code>https://sub.sockboom.pro/.../.ini</code>，之后把这个地址填进软件，即可成功挂上梯子</li>
<li>下载软件：<a href="https://github.com/Fndroid/clash_for_windows_pkg/releases">Clash.for.Windows</a>，解压后双击<code>Clash for Windows.exe</code>打开软件 (备用下载链接：<a href="https://sockboomdownload.com/ssr-download/ClashforWindows.zip">Clash</a>)</li>
<li>点击左侧的 <code>配置</code>，在顶部的输入栏中粘贴你复制的Clash订阅地址后点击 <code>下载</code>, 显示绿色的成功之后，点击名字为 <code>Sockboom</code> 的地方</li>
<li>点击左侧的<code>代理</code>，点击上方的<code>Rule</code>，一般选择<code>手动选择</code>内的节点即可，不同的节点名称代表不同地区的服务器，可以点右上方的WiFi图标进行测速，哪个延迟低选哪个节点（用ChatGPT不能选香港），节点有失效可能，注意切换</li>
<li>点击左侧的<code>主页</code>，打开下面<code>系统代理</code>的开关，即<strong>成功挂上梯子</strong>，左上角会显示实时流量，建议在<code>设置&gt;快捷键</code>中将系统代理设为<code>Ctrl+W</code>, 按需随时开关，节约流量<br>（注意：在<code>系统代理</code>的开关打开的情况下关闭软件，将会<em>出现电脑连不上网的情况</em>，此时重新打开Clash即可解决，因此，建议也打开<code>开机自启动</code>的开关，保持Clash后台常驻）</li>
</ol>
<p>至此，即可流畅登录Github上传或下载代码，以及在终端中安装各种工具包，避免了换源等繁琐操作</p>
<h3 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h3><p><a href="https://github.com/Fndroid/clash_for_windows_pkg/releases/download/0.20.29/Clash.for.Windows-0.20.29-x64-linux.tar.gz">Clash.for.Windows-0.20.29-x64-linux.tar.gz </a>可用于Ubuntu，<br>解压缩，进入文件夹终端，运行<code>.cfw</code>,即可打开软件<br>Ubuntu设置-网络代理设为手动，将http&#x2F;https代理指向clash默认端口7890：<code>HTTP代理：127.0.0.1 7890</code> <code>HTTPS代理：127.0.0.1 7890</code></p>
<p>创建软件快捷方式(Optional)</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">wget https://cdn.jsdelivr.net/gh/Dreamacro/clash@master/docs/logo.png    <span class="comment"># 下载clash icon做为桌面图标</span></span><br><span class="line">vim clash.desktop</span><br><span class="line"><span class="comment"># 输入下面的内容</span></span><br><span class="line">[Desktop Entry]</span><br><span class="line"> Name=clash</span><br><span class="line"> Comment=Clash</span><br><span class="line"> Exec=/home/.../clash/cfw</span><br><span class="line"> Icon=/home/.../clash/logo.png</span><br><span class="line"> Type=Application</span><br><span class="line"> Categories=Development;</span><br><span class="line"> StartupNotify=<span class="literal">true</span></span><br><span class="line"> NoDisplay=<span class="literal">false</span></span><br><span class="line"></span><br><span class="line">sudo <span class="built_in">mv</span> clash.desktop /usr/share/applications/</span><br></pre></td></tr></table></figure>
<p>最终就能实现通过图标打开</p>
<p><strong>使用手机或其他USB设备提供网络</strong><br>VirtualBox设置 &gt; USB设备 &gt; 添加对应的USB口 &gt; 重新启动虚拟机 &gt; 右上角网络 &gt; 连接USB以太网</p>
<h2 id="VScode"><a href="#VScode" class="headerlink" title="VScode"></a>VScode</h2><p>vscode远程访问：1.安装remote插件 2.连接服务器<code>ssh ywang85@she1-w50502</code> 3.connect，打开terminal</p>
<p>上传文件：vscode直接拖拽到目录<br>下载文件：右键download</p>
<p>vscode插件离线安装：如装python插件，直接进<a href="https://marketplace.visualstudio.com/vscode"> marketplace </a>下好拖到扩展位置</p>
<p><strong>快捷键</strong><br>打开vscode左下角键盘快捷键设置，找到copy line down，即可查看当前默认快捷键为<code>shift + Alt + ↓</code>，双击快捷键，输入自己想要的快捷组合，如Ctrl+D，然后回车即可设置自己的组合了</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">Ctrl + /    <span class="comment">#注释</span></span><br><span class="line">alt + ↑/↓   <span class="comment">#移动行</span></span><br><span class="line">alt + ←/→   <span class="comment">#光标跳到上/下一个单词</span></span><br><span class="line">Ctrl + L    <span class="comment">#选择整行</span></span><br><span class="line">Ctrl + X    <span class="comment">#删除整行</span></span><br><span class="line">Ctrl + ~    <span class="comment">#控制台终端显示与隐藏：</span></span><br></pre></td></tr></table></figure>


<h2 id="Debug"><a href="#Debug" class="headerlink" title="Debug"></a>Debug</h2><ul>
<li>Could not get lock &#x2F;var&#x2F;lib&#x2F;dpkg&#x2F;lock – open &gt; 执行 <code>sudo rm -rf /var/lib/dpkg/lock</code></li>
<li>共享文件夹ubuntu中不显示 &gt; 重新安装VMware tools</li>
<li><code>sudo apt-get install</code> failed &gt; 换源</li>
<li>若打不开终端：系统设置修改语言后重新登陆</li>
</ul>
<p> </p>
<h1 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h1><p>下载 <a href="https://git-scm.com/downloads">Git</a>，与 <a href="https://tortoisegit.org/download/">TortoiseGit</a> 小乌龟配合使用可以少记很多指令，在目标文件夹右键可执行push、clone、commit等操作</p>
<h2 id="主要流程"><a href="#主要流程" class="headerlink" title="主要流程"></a>主要流程</h2><ol>
<li><code>git clone &lt;X&gt;</code> &#x2F;&#x2F; 到本地</li>
<li><code>git checkout -b xxx</code> 切换至新分支xxx，相当于复制了remote的仓库到本地的xxx分支上</li>
<li>修改或者添加本地代码（部署在硬盘的源文件上）</li>
<li><code>git diff</code> 查看自己对代码做出的改变</li>
<li><code>git add .</code> 上传所有代码至暂存区 也可把 . 换成指定文件</li>
<li><code>git commit</code> 可以将暂存区里更新后的代码更新到本地git</li>
<li><code>git push origin xxx</code> 将本地的xxx git分支上传至github上的git</li>
</ol>
<p>如果在写自己的代码过程中发现远端GitHub上代码出现改变</p>
<ol>
<li><code>git checkout main</code> 切换回main分支</li>
<li><code>git pull origin master(main)</code> 将远端修改过的代码再更新到本地</li>
<li><code>git checkout xxx</code> 回到xxx分支</li>
<li><code>git rebase main</code> 我在xxx分支上，先把main移过来，然后根据我的commit来修改成新的内容<br>（中途可能会出现，rebase conflict –&gt; 手动选择保留哪段代码）</li>
<li><code>git push -f origin xxx</code> 把rebase后并且更新过的代码再push到远端github上（-f –&gt; 强行）</li>
<li>原项目主人采用pull request 中的 squash and merge 合并所有不同的commit</li>
</ol>
<p>远端完成更新后</p>
<ol>
<li><code>git branch -d xxx</code> 删除本地的git分支</li>
<li><code>git pull origin master</code> 再把远端的最新代码拉至本地</li>
</ol>
<p><a href="https://www.bilibili.com/video/BV19e4y1q7JJ">十分钟学会正确的github工作流，和开源作者们使用同一套流程</a></p>
<img data-src="https://www.ruanyifeng.com/blogimg/asset/2015/bg2015120901.png" width="100%">


<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#branch 与 tag：</span></span><br><span class="line">git tag/branch  <span class="comment">#查本地仓库所有的tag或branch -r:远程分支 -a:所有分支</span></span><br><span class="line">git checkout [tag/branch]   <span class="comment">#已有仓库切换 tag/branch</span></span><br><span class="line">git checkout -b [branch]    <span class="comment">#新建一个分支，并切换到该分支</span></span><br><span class="line">git merge [branch]  <span class="comment">#合并指定分支到当前分支</span></span><br><span class="line">git tag [tag]       <span class="comment">#新建一个 tag 在当前 commit</span></span><br><span class="line">git describe --tag  <span class="comment">#查当前tag</span></span><br><span class="line">git show [tag]      <span class="comment">#查看tag信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#tag 对应某次 commit, 是一个点，是不可移动的。</span></span><br><span class="line"><span class="comment">#branch 对应一系列 commit，是很多点连成的一根线，有一个HEAD 指针，是可以依靠 HEAD 指针移动的。</span></span><br><span class="line"><span class="comment">#两者的区别决定了使用方式，改动代码用 branch ,不改动只查看用 tag</span></span><br></pre></td></tr></table></figure>


<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#常用指令：</span></span><br><span class="line">git init [project-name] <span class="comment">#新建一个目录，将其初始化为Git代码库</span></span><br><span class="line">git status  <span class="comment">#显示有变更的文件</span></span><br><span class="line">git <span class="built_in">log</span>     <span class="comment">#显示当前分支的版本历史 commit id</span></span><br><span class="line">git diff    <span class="comment">#显示暂存区和工作区的差异</span></span><br><span class="line">git reset --hard [commitId]  <span class="comment"># 进行回溯</span></span><br><span class="line"></span><br><span class="line">git config --list   <span class="comment">#检查当前配置</span></span><br><span class="line"><span class="comment"># 配置全局信息 无global则是在项目中配置</span></span><br><span class="line">git config --global user.name <span class="string">&quot;[name]&quot;</span></span><br><span class="line">git config --global user.email <span class="string">&quot;[email address]&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h1><p><code>Docker</code>是一种开源的容器化平台，可以帮助开发者更高效地打包、部署和运行应用程序。它基于 <code>Linux</code> 容器（LXC）技术，通过将应用程序及其所有依赖项打包到一个容器中，从而消除了应用程序在不同环境之间迁移所面临的问题。使用Docker，开发者可以快速构建、测试和部署应用程序，减少了与操作系统和基础设施相关的问题，从而提高了开发、测试和发布的速度。</p>
<p><a href="https://www.bilibili.com/video/BV1MR4y1Q738/">🐳Docker概念，工作流和实践</a><br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Linux1.png" width="80%"><br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Linux2.png" width="80%"></p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看当前运⾏的docker实例状态</span> </span><br><span class="line">sudo docker ps -a </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在上⼀条指显示结果列表中，查看openharmony的STATUS</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如为 Exited，则需要执⾏下⾯这条指令，再次启动</span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如为 Up，则跳过下⾯这条指令</span> </span><br><span class="line">sudo docker start openharmony</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进⼊docker编译环境</span> </span><br><span class="line">sudo docker exec -it openharmony bash </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">执⾏后，出现类似如下信息，说明再次进⼊成功</span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">root@bae85ba0f77c:/home/openharmony<span class="comment">#</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">退出</span></span><br><span class="line">exit</span><br></pre></td></tr></table></figure>
<p>启动docker的samba服务：<code>service smbd restart</code><br>查看ip：<code>ifconfig</code><br>连接：<code>\\192.168.174.128\docker</code></p>
<h2 id="文件拷贝"><a href="#文件拷贝" class="headerlink" title="文件拷贝"></a>文件拷贝</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">1、主机拷贝文件到docker编译环境里：</span></span><br><span class="line">sudo docker cp源文件openharmony:/目标文件</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">参数解析：</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">源文件：主机上的，可为文件或者目录</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">盘目标文件：docker编译环境里的，通常为目录，表示将文件拷贝到该目录</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">2、docker编译环境拷贝文件到主机：</span></span><br><span class="line">sudo docker cp openharmony:/源文件目标文件</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">源文件：docker编译环境里的，可为文件或者目录</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">目标文件：主机上的，通常为目录，表示将文件拷贝到该目录下</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">删除docker编译环境【谨慎操作，不可恢复】</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">查看当前运行的docker实例状态</span></span><br><span class="line">sudo docker ps -a</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">在上一条指显示结果列表中，查看openharmony的STATUS</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">如为Up，则需要执行下面这条指令，停止其运行</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">如为Exited，则跳过下面这条指令</span></span><br><span class="line">sudo docker stop openharmony</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">删除</span></span><br><span class="line">sudo docker rm openharmony</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">提醒：</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">删除前，请确保该运行环境内的有效数据都已拷贝到主机上</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">删除后，该运行环境内的所有数据将被移除，不可恢复</span></span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Paper：科研方法总结</title>
    <url>/Paper/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>记录了SCI论文工作流，包括论文的查找翻译及润色工具、文献管理神器Zotero、LaTex排版指南、论文相关规范。</p>
<span id="more"></span>

<h1 id="论文工具"><a href="#论文工具" class="headerlink" title="论文工具"></a>论文工具</h1><h2 id="1-找论文"><a href="#1-找论文" class="headerlink" title="1.找论文"></a>1.找论文</h2><p>参考目标期刊相似论文的架构，搭框架，填内容</p>
<ul>
<li>订阅论文：<a href="https://www.storkapp.me/?ref=1003">文献鸟</a>，设置关键字订阅相关论文，每周会自动收集好论文发你邮箱</li>
<li>下载论文：<a href="https://tool.yovisun.com/scihub/">SCI-Hub论文下载</a>, <a href="https://www.drugfuture.com/cnpat/cn_patent.asp">中国专利下载</a>，以及学校的webvpn</li>
<li>查期刊：<a href="https://www.letpub.com.cn/index.php?page=journalapp">LetPub</a>，有期刊的介绍及投稿人的讨论</li>
<li>下载书籍：<a href="https://lib-xg7r2un3uz6rivi4ibrvtfls.resist.tel/">Z-library</a></li>
<li>快速总结论文：Newbing，<a href="https://chatpaper.org/">ChatPaper</a></li>
</ul>
<h2 id="2-中译英"><a href="#2-中译英" class="headerlink" title="2.中译英"></a>2.中译英</h2><p><a href="https://www.deepl.com/translator">DeepL翻译器</a>，在翻译结果中可以选中单词修改为其他近义词或不同的句式<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Paper1.png" alt="图 1">  </p>
<h2 id="3-润色"><a href="#3-润色" class="headerlink" title="3.润色"></a>3.润色</h2><p><a href="https://chat.openai.com/chat">ChatGPT</a>，按照期刊风格润色，<a href="https://ai.newzone.top/">ChatGPT提示词参考</a></p>
<figure class="highlight applescript"><table><tr><td class="code"><pre><span class="line">I want you <span class="keyword">to</span> act <span class="keyword">as</span> an academic journal editor. </span><br><span class="line">Please [proofreading] <span class="keyword">the</span> [<span class="built_in">paragraph</span>] <span class="keyword">from</span> an academic angle based <span class="keyword">on</span> <span class="keyword">the</span> writing style <span class="keyword">of</span> <span class="keyword">the</span> [CVPR]: </span><br><span class="line">[<span class="built_in">text</span>]</span><br></pre></td></tr></table></figure>
<img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Paper2.png" width = "90%" />

<h2 id="4-语法"><a href="#4-语法" class="headerlink" title="4.语法"></a>4.语法</h2><p><a href="https://www.grammarly.com/office-addin">Grammarly</a>，安装Word插件，快速修正语法<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Paper3.png" alt="图 3">  </p>
<h1 id="文献管理神器：Zotero"><a href="#文献管理神器：Zotero" class="headerlink" title="文献管理神器：Zotero"></a>文献管理神器：Zotero</h1><p>Zotero能管理个人文献库、插入引文、论文阅读和记笔记、pdf下载和翻译、查看论文分区、影响因子等信息</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><a href="https://www.zotero.org/">Zotero官网</a>，安装软件后再装浏览器插件，可以很方便的在浏览器随时导入文献到Zotero；<br>顺便在官网注册一个账号，用于同步,且可登陆官网在线查看自己的论文库；<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Paper4.png" width = "100%" /></p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">编辑 &gt; 首选项 &gt; 同步 &gt; 登陆</span><br><span class="line">编辑 &gt; 首选项 &gt; 高级 &gt; 文件和文件夹 &gt; 更改数据储存路径  <span class="comment">#自选合适的位置存储本地文件</span></span><br></pre></td></tr></table></figure>

<h3 id="SCI-Hub抓取pdf"><a href="#SCI-Hub抓取pdf" class="headerlink" title="SCI-Hub抓取pdf"></a>SCI-Hub抓取pdf</h3><ol>
<li>打开Zotero 首选项-&gt;高级-&gt;设置编辑器</li>
<li>搜索 <code>extensions.zotero.findPDFs.resolvers</code> ，搜到之后双击打开，在对话框中填入：<figure class="highlight livescript"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;name&quot;</span>:<span class="string">&quot;Sci-Hub&quot;</span>,</span><br><span class="line">    <span class="string">&quot;method&quot;</span>:<span class="string">&quot;GET&quot;</span>,</span><br><span class="line">    <span class="string">&quot;url&quot;</span>:<span class="string">&quot;https://sci-hub.ren/&#123;doi&#125;&quot;</span>,</span><br><span class="line">    <span class="string">&quot;mode&quot;</span>:<span class="string">&quot;html&quot;</span>,</span><br><span class="line">    <span class="string">&quot;selector&quot;</span>:<span class="string">&quot;<span class="subst">#pdf</span>&quot;</span>,</span><br><span class="line">    <span class="string">&quot;attribute&quot;</span>:<span class="string">&quot;src&quot;</span>,</span><br><span class="line">    <span class="string">&quot;automatic&quot;</span>:<span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>实现抓取任意文献pdf（知网和部分新文献不行）</li>
</ol>
<h3 id="配置搜索引擎"><a href="#配置搜索引擎" class="headerlink" title="配置搜索引擎"></a>配置搜索引擎</h3><ol>
<li><a href="https://www.123pan.com/s/goS7Vv-kIKbd.html">下载 engines.json 文件</a></li>
<li>打开数据储存路径,替换同名文件 <code>...\Zotero\locate\engines.json</code></li>
<li>重启Zotero后，实现多搜索引擎，很方便</li>
</ol>
<h3 id="安装Word插件"><a href="#安装Word插件" class="headerlink" title="安装Word插件"></a>安装Word插件</h3><p>安装word插件，实现高效插入引文，并且可以根据期刊选择参考文献样式<br><code>编辑 &gt; 首选项 &gt; 引用 &gt; 文字处理软件 &gt; 安装word插件</code><br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Paper5.png" width = "80%" /></p>
<h3 id="插件，让Zotero真正好用"><a href="#插件，让Zotero真正好用" class="headerlink" title="插件，让Zotero真正好用"></a>插件，让Zotero真正好用</h3><p><a href="https://www.zotero.org/support/plugins">官方插件下载地址</a><br>安装方法：下载.xpi 文件到你的电脑。然后，在 Zotero，点击“工具→插件（附加组件）”，然后拖动插件的 .xpi 文件到插件窗口打开，或者选择从文件中安装。安装完成后重新启动软件即可。</p>
<p><strong>下面是强烈推荐的5个必备插件，嫌麻烦可以直接安装我上传的，都是2023年最新的，5个全部拖进去即可，地址：<a href="https://www.123pan.com/s/goS7Vv-mIKbd.html">推荐插件下载</a></strong></p>
<p><strong>Zotero PDF Translate</strong><br><a href="https://github.com/windingwind/zotero-pdf-translate/releases">zotero-pdf-translate 下载</a>，实现划词翻译，但是需要在插件设置里配置翻译引擎（免费,推荐腾讯云），<a href="https://doc.tern.1c7.me/zh/folder/setting/#%E8%85%BE%E8%AE%AF%E4%BA%91">配置方法文档</a> </p>
<p><strong>Jasminum - 茉莉花</strong><br><a href="https://github.com/l0o0/jasminum/releases">Jasminum 下载</a>，一个简单的 Zotero 中文插件, 优化知网中文论文管理体验</p>
<p><strong>Zotero DOI Manager</strong><br><a href="https://github.com/bwiernik/zotero-shortdoi/releases">Zotero DOI Manager 下载</a>，更新抓取文献的DOI号，提高SCI-Hub抓取pdf成功率</p>
<p><strong>Zotero IF</strong><br><a href="https://pan.baidu.com/s/1-nIHvhRj_wK_t6LMsNF6rg#list/path=%2Fsharelink3626522904-21879764921069%2FZotero%20IF&parentPath=%2Fsharelink3626522904-21879764921069">Zotero IF百度云地址，提取码: apeh</a>,为所抓取的文献更新影响因子</p>
<p><strong>Zotero Citation Counts Manager</strong><br><a href="https://github.com/eschnett/zotero-citationcounts/releases">Zotero Citation Counts Manager 下载</a>，抓取引用数，一般选Semantic Scholar citation count</p>
<details>
  <summary>清空其它信息:工具-->开发者-->Run Javascript-->运行以下代码</summary>

<pre><code>var fieldName = &quot;extra&quot;;
var newValue = &quot;&quot;;

var fieldID = Zotero.ItemFields.getID(fieldName);
var s = new Zotero.Search();
s.libraryID = ZoteroPane.getSelectedLibraryID();
var ids = await s.search();
if (!ids.length) &#123;
return &quot;No items found&quot;;
&#125;
await Zotero.DB.executeTransaction(async function () &#123;
for (let id of ids) &#123;
let item = await Zotero.Items.getAsync(id);
let mappedFieldID = Zotero.ItemFields.getFieldIDFromTypeAndBase(item.itemTypeID, fieldName);
item.setField(mappedFieldID ? mappedFieldID : fieldID, newValue);
await item.save();
&#125;
&#125;);
return ids.length + &quot; item(s) updated&quot;;
</code></pre>
</details>

<h1 id="LaTex，论文通用排版系统"><a href="#LaTex，论文通用排版系统" class="headerlink" title="LaTex，论文通用排版系统"></a>LaTex，论文通用排版系统</h1><p>在线编辑器：<a href="https://www.overleaf.com/">Overleaf</a>，可以多人在线共同编辑、批注；<br>将期刊提供的LaTex模板导入编辑器，直接在模板里写；<br>查询文档：<a href="https://github.com/sailist/LaTeXdoc">LaTeXdoc</a>, <a href="https://zhuanlan.zhihu.com/p/508559139">Latex简明速查手册(8页)</a></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><h3 id="单篇论文的参考文献引用"><a href="#单篇论文的参考文献引用" class="headerlink" title="单篇论文的参考文献引用"></a>单篇论文的参考文献引用</h3><ol>
<li>使用<code>\begin&#123;thebibliography&#125;</code>开始参考文献的引用；使用<code>\end&#123;thebibliography&#125;</code>结束；</li>
<li>使用<code>\bibitem&#123;&#125;</code>添加文献，在文章中只用使用<code>\cite&#123;&#125;</code>引用，需要自己改，比较麻烦；</li>
</ol>
<h3 id="overleaf-zotero多篇论文-bib文件引用"><a href="#overleaf-zotero多篇论文-bib文件引用" class="headerlink" title="overleaf+zotero多篇论文.bib文件引用"></a>overleaf+zotero多篇论文.bib文件引用</h3><ol>
<li>使用zotero，将文章所需参考文献导入至一个新文件夹中，选择导出分类，选择文件夹导出<code>ref.bib</code>格式的文件；</li>
<li>打开overleaf，将刚刚的<code>ref.bib</code>文件导入</li>
<li>接下来就是在文章中导入参考文献:<ul>
<li>首先导入如下命令，注意是在文章最后，<code>\end&#123;document&#125;</code>之前：  <figure class="highlight fsharp"><table><tr><td class="code"><pre><span class="line">\<span class="keyword">bibliographystyle</span>&#123;ieicetr&#125; <span class="operator">%</span>引文期刊格式</span><br><span class="line">\<span class="keyword">bibliography</span>&#123;<span class="built_in">ref</span>&#125; <span class="operator">%.</span>bib文件名</span><br></pre></td></tr></table></figure></li>
<li>编译后还不能显示你的参考文献列表，只有在文章中添加了<code>\cite&#123;&#125;</code>之后才可以正常显示<br>  <img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Paper6.png" alt="图 6"><br>  <code>\cite&#123;liu_ceam-yolov7_2022&#125;</code></li>
</ul>
</li>
</ol>
<h2 id="公式图表代码"><a href="#公式图表代码" class="headerlink" title="公式图表代码"></a>公式图表代码</h2><h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><figure class="highlight llvm"><table><tr><td class="code"><pre><span class="line">\<span class="keyword">begin</span>&#123;equation&#125;</span><br><span class="line">  <span class="keyword">x</span>+y</span><br><span class="line">\<span class="keyword">end</span>&#123;equation&#125;</span><br></pre></td></tr></table></figure>
<h3 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h3><figure class="highlight css"><table><tr><td class="code"><pre><span class="line">%h: 表示浮动体尽可能地放在当前位置。t: 表示浮动体放在页面顶部。b: 表示浮动体放在页面底部。p: 表示浮动体放在一个单独的页面上。</span><br><span class="line">\begin&#123;<span class="selector-tag">figure</span>&#125;<span class="selector-attr">[htb]</span></span><br><span class="line">\begin&#123;center&#125;</span><br><span class="line">\includegraphics<span class="selector-attr">[width=1\columnwidth]</span>&#123;FEY-YOLOv7/<span class="selector-tag">figure</span>/fig.<span class="number">1</span><span class="selector-class">.png</span>&#125;</span><br><span class="line">\end&#123;center&#125;</span><br><span class="line">\<span class="selector-tag">caption</span>&#123;RGB-infrared Datasets.&#125;</span><br><span class="line">\<span class="selector-tag">label</span>&#123;fig:<span class="number">1</span>&#125;</span><br><span class="line">\end&#123;<span class="selector-tag">figure</span>&#125;</span><br><span class="line">%跨栏显示：&#123;<span class="selector-tag">figure</span>*&#125;</span><br><span class="line">%若编译不显示图片：菜单&gt;选择用XELATEX编译</span><br><span class="line">%引用：shown in Fig.\ref&#123;fig:<span class="number">2</span>&#125;</span><br></pre></td></tr></table></figure>
<h3 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h3><figure class="highlight livescript"><table><tr><td class="code"><pre><span class="line"><span class="string">\begin&#123;table&#125;[htb]</span></span><br><span class="line"><span class="string">\caption&#123;Effect</span> <span class="keyword">of</span> different datasets <span class="literal">on</span> the YOLOv7.&#125;</span><br><span class="line"><span class="string">\label&#123;table:4&#125;</span></span><br><span class="line"><span class="string">\begin&#123;center&#125;</span></span><br><span class="line"><span class="string">\begin&#123;tabular&#125;&#123;ll&#125;</span></span><br><span class="line">    <span class="string">\hline</span></span><br><span class="line">    <span class="string">\textbf&#123;Dataset&#125;</span> &amp; <span class="string">\textit&#123;mAP&#125;\\</span></span><br><span class="line">    <span class="string">\hline</span></span><br><span class="line">    IR dataset &amp; <span class="number">0.85</span> <span class="string">\\</span></span><br><span class="line">    RGB dataset &amp; <span class="number">0.758</span> <span class="string">\\</span></span><br><span class="line">    RGB-infrared dataset &amp; <span class="number">0.962</span> <span class="string">\\</span></span><br><span class="line">    <span class="string">\hline</span></span><br><span class="line"><span class="string">\end&#123;tabular&#125;%</span></span><br><span class="line"><span class="string">\end&#123;center&#125;</span></span><br><span class="line"><span class="string">\end&#123;table&#125;</span> </span><br></pre></td></tr></table></figure>

<h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>IEICE期刊论文投稿的LaTeX模板中提供了EUC、SJIS和UTF三种不同的模板。这些模板的区别在于它们所使用的字符编码方式，其中UTF-8是其中最为常用的一种编码方式。<br><code>Ctrl + S 编译</code></p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">%分点</span><br><span class="line">\begin&#123;itemize&#125;</span><br><span class="line">  \item [1.]</span><br><span class="line">  \item </span><br><span class="line">  \item </span><br><span class="line">\end&#123;itemize&#125;</span><br><span class="line"></span><br><span class="line">空行：\\  下划线：\_&#123;&#125;  百分号:\%  加粗：\textbf&#123;&#125;</span><br><span class="line">斜体：\textit&#123;&#125; 或 $x$  或公式斜体：\mathit&#123;&#125;  </span><br><span class="line">高亮：\usepackage &#123;soul&#125;  \hl &#123;xxx&#125;</span><br><span class="line"></span><br><span class="line">%文章结构</span><br><span class="line">章：\section&#123;&#125;   节：\subsection&#123;&#125;   条：\subsubsection&#123;&#125;</span><br></pre></td></tr></table></figure>

<h1 id="论文规范"><a href="#论文规范" class="headerlink" title="论文规范"></a>论文规范</h1><h2 id="缩写"><a href="#缩写" class="headerlink" title="缩写"></a>缩写</h2><ol>
<li><p>一个词或词组在文中出现三次或以上才可以用缩写，否则需要写出全称。也就是说如果只出现一次或两次，每次都要写出全称，而缩略语就不需要给出了。<br>出现的次数是在摘要、正文(从前言到讨论)、每个图注以及每个表注中分别计算的。如果一个词在摘要中出现一次，正文中出现多次，图注中又出现一次，那么摘要中只要用全称，不要用缩写；正文中第一次出现时用全称，后面用缩写； 图注中用全称，不用缩写。<br>一些期刊只允许对正文中出现3次以上的术语使用缩写。<br>一些期刊禁止在标题和摘要中使用任何缩写词。<br>从写作风格上讲，大多数期刊都建议作者不要在句首使用缩写词。在这种情况下，作者要详细说明术语的含义，或者改用别的措辞重写句子。<br>在论文中，将摘要看做是独立的部分。因此，如果您需要在摘要和论文中同时使用一个缩写词时，您需要两次介绍这个缩写词：首次在摘要中使用时介绍一次；首次 在论文中使用时介绍一次。然而 ，如果您发现在摘要中介绍的一个缩写词仅仅出现了一两次，没必要缩写——可以考虑避免缩写，每次都用全拼代替（除非您认为读者更了解缩写词）</p>
</li>
<li><p>缩略语在文中第一次出现时需要定义。这里所谓第一次，也是摘要、正文(从前言到讨论)、每个图注以及每个表注中分别计算的。如果摘要和正文都符合上面所说使用缩写的规则，那么摘要中需要定义一次，正文中还需要定义一次。定义就是写出全称并在括号中给出缩写。<br>The patient was diagnosed with pulmonary arterial hypertension (PAH).<br>如果图例或表格中为了节省空间出现了缩写，那么在图注或表注中要解释缩写。<br>PH: pulmonary hypertension, PAH: pulmonary arterial hypertension.<br>写全称时一般是全部用小写，除非是人名、地名等专有名词，而缩写一般是全大写，除非是约定俗成的写法，比如用ChIP 代表chromatin immunoprecipitation。<br>应该考虑到也许有其他领域或者新进入这门领域的读者会阅读这篇论文，所以期刊都规定缩略语要先定义再使用。有些期刊会列出该期刊中不需要定义就可以直接使用的缩略语，比如Journal of Clinical Investigation 就给出<a href="http://www.jci.org/kiosk/publish/abbreviations">标准缩略语列表</a>并指出这些缩略语可以直接使用。如果期刊没有特别规定，应该每个缩略语使用时都要先定义。定义之后记得要一直使用缩略语，不要再使用全称。</p>
</li>
<li><p>尽量避免使用缩略语。一篇文章完全不使用缩略语不见得可行，但是尽量少用为好，而且只使用比较常见的缩略语。使用缩略语的目的是为了便于读者对文章的阅读理解，比较长的词组如果反复出现，不便于读者的快速阅读，所以用缩写更合适。有些词可能多数读者对缩写更熟悉，这些词用缩写也便于读者理解。如果很多比较短的词也用缩写，而且不是普遍使用的缩写，读者就经常要停下来去查找缩写代表的词意，思路就会被打断，所以会影响对文章的理解。</p>
</li>
<li><p>一个缩略语代表一个词或词组。在一篇文章中，一个缩略语只能代表一个词（组），而且一个词（组）也只能用一个缩略语代表。</p>
</li>
<li><p>缩写相关的复数和冠词的用法。多数情况下，我们会用缩略语来代替一个名词，所以如果是可数名词也是有复数的。一般来说，缩略语代表的是名词的单数形式，我们在后面加s（或者加es如果缩略语的最后一个字母是s）来表示复数。定义缩写的时候通常就可以看出来，比如 endothelial cells (ECs)。<br>如果缩略语名词前面需要加不定冠词，要根据缩写的读音来决定是用a还是an。例如human immunodeficiency virus缩写成HIV时我们是念这三个字母，第一个字母H虽然是个辅音字母，念这个字母时却是元音开头，所以HIV这个词的第一个音是元音，前面要用an，例如 an HIV-positive test result。</p>
</li>
<li><p>在技术写作中最常见的关于首字母缩写和缩略词的错误，就是大部分人以为缩写中出现的大写字母在缩写前的词组中也需要大写，实际上并不是这样的。例如，作为商业管理哲学的「total quality management」缩写为「TQM」，但它的全称却常常被错误地写成「Total Quality Management」，这有可能是因为部分作者忘记了这个词本身并不是专有名词，甚至是更糟糕的情况──作者根本从一开始就没有完全了解这个词的含意！请记住，在文章中缩写术语之前，首先了解该术语的含意和来源是很重要的，因为作者在论文中处理首字母缩写和缩略词的态度与方法常常反映了他们自己对于运用术语的了解程度。<br>掌握了上面这几个原则，我们就可以通过恰当地使用缩略语来增加文章的可读性。</p>
</li>
</ol>
<h2 id="字母"><a href="#字母" class="headerlink" title="字母"></a>字母</h2><ol>
<li><p>什么时候用斜体(italic)？</p>
<blockquote>
<p>1)变量(variables)应该用斜体表示：例如<em>T</em>表示温度(temperature)，<em>r</em>表示速率(rate)，<em>x</em>代表摩尔分数(molar fraction).<br>2)坐标轴(axes)：the <em>y</em> axis.<br>3)平面(planes)：plane <em>P</em>.<br>4)行列式(determinants)和矩阵(matrices)中的元素：<em>gn</em>.<br>5)常数(constants)符号: <em>g</em>, 重力加速度<br>6)描述变量的函数:<em>f(x)</em>.</p>
</blockquote>
</li>
<li><p>什么时候不用斜体，而采用罗马字体(Roman)?<br>这里的Roman字体指非特殊性字体，与每个期刊的正文字体一致</p>
<blockquote>
<p>1)数字；<br>2)标点符号和括号；<br>3)大多数运算符；<br>4)量度单位和时间单位：毫克, mg; 帕斯卡, Pa; 毫米汞柱, mmHg.<br>5)非数学符号和数量：s, 原子轨道(atomic orbital);<br>6)变量的多字符缩写<br>7)数学常量(mathematical constants):自然对数,e;复数的虚部,i;圆周率,π.<br>8)矩阵的转置(transposes), <strong>A</strong>T(T是矩阵<strong>A</strong>的转置)<br>9)点(point)和线(line): point A, line AB.<br>10)行列式(determinants)： A是矩阵<strong>A</strong>的行列式<br>11)三角函数和其他数学函数:正弦函数sin;最大值max,极限lim;对数log等.</p>
</blockquote>
</li>
<li><p>什么时候用粗体(Boldface)？</p>
<blockquote>
<p>1）向量(vectors)；<br>2）张量(tensors)；<br>3）矩阵(matrics);<br>4）多维物理量：磁场强度，<strong>H</strong>.</p>
</blockquote>
</li>
<li><p>什么时候使用希腊字母(Greek letters)?</p>
<blockquote>
<p>任何拉丁字母可以使用的地方都可以，包括变量，常量，向量等。</p>
</blockquote>
</li>
<li><p>当上下标本身就是代表物理量或数字的符号时，用斜体(italic),如果上下标是缩写或者不是符号时，不用斜体，采用罗马字体(Roman).</p>
<blockquote>
<p>举例：<em>Cp</em> for heat capacity at constant pressure常压比热（<em>p</em>是压力的符号，采用斜体）<br><em>C</em>B for heat capacity of substance B 物质B的比热（B不是符号，不用斜体）<br><em>C</em>g where g is gas 气体比热 (g是gas的缩写，不用斜体)<br><em>Ei</em> for energy of the <em>i</em>th level, where <em>i</em> is a number(<em>i</em>是代表数字的符号)<br><em>g</em>n where n is normal(n是气体normal的缩写，不用斜体)</p>
</blockquote>
</li>
</ol>
<p><a href="https://journal.cricaas.com.cn/attached/file/20210517/20210517161953_566.pdf">[GB 3102.11-93] 物理科学和技术中使用的数学符号</a></p>
]]></content>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title>Photograph：个人摄影笔记</title>
    <url>/Photograph/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>包括读《摄影笔记》的总结，专题拍摄笔记，摄影名词解释及相机知识，调色笔记更新中。</p>
<span id="more"></span>

<p><a href="https://kdocs.cn/l/cq3CTsJgxvNv?f=201">索尼APP使用教程</a></p>
<h1 id="调色笔记"><a href="#调色笔记" class="headerlink" title="调色笔记"></a>调色笔记</h1><img alt="图 7" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Photograph-rrl.png" width=50%/>  
右右左，适当降低饱和度

<h1 id="摄影笔记"><a href="#摄影笔记" class="headerlink" title="摄影笔记"></a>摄影笔记</h1><p>四要素：<code>取景，曝光，虚实，构图</code></p>
<h2 id="取景"><a href="#取景" class="headerlink" title="取景"></a>取景</h2><p>互动：选取合适的前景与背景相互配合，有好的背景,就等合适的主体，注意人和景色的互动<br>云：镜头正对方向与云运动方向平行时，会呈放射状；大面积的卷云会在日落后形成火烧云<br>色温：日出日落时色温低，画面暖，之后一会儿就色温高画面冷<br>时间：阴天，多云，或日出后1h，日落前1h拍摄<br>拍摄浅色主体时，建议选择深色背景，反之亦然</p>
<p><strong>光线：</strong></p>
<blockquote>
<p>不要在强光下拍风光片(建筑例外),必要则顺光拍；<br>早晚、阴天：顺逆光皆可；<br>侧逆光能更好地表现食物的立体感（柔光板辅助）；<br>上午朝西，下午朝东，顺光、色彩较好。</p>
</blockquote>
<h2 id="曝光"><a href="#曝光" class="headerlink" title="曝光"></a>曝光</h2><p>白天一般不增减曝光补偿,夜晚减一点让画面更干净<br>室外晴天的感光度通常在300以内，在室内可能达到500-800，甚至1000+；<br>在拍摄高速运动的对象时需要1&#x2F;1000以上的快门速度，否则无法捕捉到瞬间画面；<br>逆光&#x2F;大光比：前期包围曝光，后期多张混合（灰度蒙版，手动笔刷）<br>曝光时间——500法则：$$快门速度（s）&#x3D;\frac{500}{镜头等效焦距（mm）}$$</p>
<h2 id="虚实"><a href="#虚实" class="headerlink" title="虚实"></a>虚实</h2><p>不能一味追求大光圈和虚化，要清晰<br>慢门创造虚：1.光圈优先，记下数据；2.切手动，设记下的光圈，ISO，快门延长1000倍（超过30s用B门）<br>水：快门优先,水面雾化,映照色彩<br>后期混合景深<br>两只眼睛都要实<br>先对焦、再构图</p>
<p><strong>摇摄</strong>：将相机调为S档，根据实际情况选择1&#x2F;15~1&#x2F;30秒之间的快门速度，选择AF-C连续对焦、广域对焦区域及高速连拍即可开始拍摄。<br>拍摄时，将对焦点选择在人物身上，然后随着人物运动的方向以与人物相同的角速度转动镜头并同时按下快门连拍，即可获得人物主体清晰、背景模糊的效果。摇摄在拍摄快速移动的物体时除了能够虚化掉杂乱的背景，还能极大地增强画面动感。</p>
<h2 id="构图"><a href="#构图" class="headerlink" title="构图"></a>构图</h2><ol>
<li>身体 4321 160</li>
<li>三分法 99%的时候都有效</li>
<li>层次 前中后景</li>
<li>留白 前景 线条</li>
</ol>
<p><strong>三分法</strong>：人们的目光总是自然落在画面三分之二处的位置上，尽量使主要的被摄体位于画面三等分线的焦点上，效果会比位于中心位置更好。<br>大主体可以居中,否则放在三分焦点；<br>表达广阔场景时,将重要线条往三分线下压；<br>对称主体可居中,独立主体放左&#x2F;右三分线</p>
<p>善用框景<br>“如果你拍的不够好,是因为你站得不够近”<br>注意纵深感，立体感<br>引入线条：利用慢门的轨迹创造线条。<br>美食&#x2F;美颜：45°角，留白<br>参考当地明信片的名景点的拍摄位置和构图</p>
<p><strong>人像</strong>：</p>
<blockquote>
<p>人头不放正中间，要偏上一点,否则显矮；10°俯拍<br>人脸朝左,人物置右;人脸朝右,人物置左;面朝前时,将一只眼睛置竖直中线<br>手脚、关节不能裁切<br>让人物身体和面部与镜头呈45,立体、显瘦<br>头顶、脚下预留空间<br>只考虑人物：竖构图; 交代环境：横构图</p>
</blockquote>
<h2 id="修图"><a href="#修图" class="headerlink" title="修图"></a>修图</h2><p>修图流程：lr定调（不加颗粒）、 液化、 修瑕疵、 磨皮、 其它、 锐化<br>液化：压力8-16<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Photograph1.png" alt="图 1"><br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Photograph2.png" alt="图 2">  </p>
<p><strong>直方图</strong><br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Photograph3.jpeg" width=70%></p>
<p>以12位图像为例，12位图像记录的单个通道色彩深度是4096，但是4096并不是平均分布在直方图上，而是从亮部到暗部倍减。<br>所以在不过曝的前提下，尽量把图像信息向直方图的右边推进，直方图越向右，记录的色调信息就越多；进光量越充分，画质越好。</p>
<h2 id="文字"><a href="#文字" class="headerlink" title="文字"></a>文字</h2><blockquote>
<p>摄影是人类对静止最深刻的表达方式之一。摄影使我们把时间握在手中，与时间融合，这是任何其他形式都不具备的。<br>相片本身会传达出作者注入其中的情感。<br> “为什么这样做”高于“如何实现”，我认为这应当是一切艺术的目标。<br> 我们的世界有许多美的事物。尽管摄影天生是一项孤独的活动，但我们可以通过照片感受到彼此之间的联系，从而让我们获得慰籍。<br> ——丹.温特斯《观看之路》</p>
</blockquote>
<p> 如果你带着相机四下转悠，那就有义务保持警醒。  ——胡里奥·科塔萨尔《放大》</p>
<p> 我曾以为照片能留住那刻的画面，然而拍照的时候我已经失去了那片刻的时光。<br> 我常想，如果我拍了足够的照片，我就不会再失去任何人。事实上，我的照片让我看到了我失去了多少。<br> 摄影不是一项业余爱好，而是一种生活方式。<br> 摄影不只是拍照，更是一种生活哲学。</p>
<blockquote>
<p>高光亮而不溢；暗部黑而不死；过度锐而不硬；画质润而不腻。焦内如刀割般犀利，焦外如奶油般化开。作品给人以美的感受，色彩清新脱俗，构图别出心裁，雅俗共赏。欣赏学习好友佳作，欢迎回访指导。</p>
<p>主体清晰通透，动态多样，抓拍到位，表情自然，活泼靓丽。深邃的思想，惊世的影调，高超的构图，无与伦比的后期。充满了人文的气息，接地气的大片，期待您的佳作。毒，德味，大片，学习了!</p>
</blockquote>
<h1 id="专题拍摄"><a href="#专题拍摄" class="headerlink" title="专题拍摄"></a>专题拍摄</h1><p>主题拍摄：一直拍一种，最后放在一起</p>
<h2 id="人像写真"><a href="#人像写真" class="headerlink" title="人像写真"></a>人像写真</h2><p>人像摄影三要素：情绪，姿态，故事性<br>主题性的拍摄：</p>
<ol>
<li><strong>风格</strong><br>清新、复古、古风、私房…<br>重点：依据主题去确立服饰，妆容，场景的匹配</li>
<li><strong>场景</strong><br>海边：百搭场景，安静，放松，舒服<br>大自然，草坪：场景开阔，构图简单，有利于引导模特姿态<br>室内环境：容易做到出彩的背景色，强烈的主题风格<br>校园场景：制服，便服，清新活泼</li>
<li><strong>服装搭配</strong><pre class="mermaid">graph LR
A(依据主题风格选择服装搭配) -->B(依据搭配选场景)
C(依据模特确定主题) -->|简单大方清新自然不造作| A
B-->C</pre>
各要素相辅相成</li>
<li><strong>模特</strong><br>各有特点，契合气质定主题</li>
<li><strong>与模特的沟通</strong><br>确定主题，表达想法；<br>建立模特对你拍摄技术的信心，给看样片；<br>放松自然（放音乐），不停和模特进行沟通，不冷场，多夸夸；<br>引导与<code>场景，服装，道具</code>的互动；<br>摆拍结合抓拍，引导自然的情绪流露。</li>
</ol>
<h2 id="星空摄影"><a href="#星空摄影" class="headerlink" title="星空摄影"></a>星空摄影</h2><p>使用光污染地图：<a href="www.lightpollutionmap.info">light pollution map</a>，<a href="https://www.darkmap.cn/">天文通-全球光污染地图</a><br>对焦：手动对焦最亮的星星（无穷远），直到最小、最实</p>
<ul>
<li>银河：春夏秋，银河核心会出现在夜空中用，最大光圈</li>
<li>月亮：农历的月初月末无月，月中满月，0点沉入地平线</li>
<li>流星：找辐射点，用最大光圈（拍星轨稍小），短曝光时间</li>
</ul>
<h2 id="夜景烟火人像"><a href="#夜景烟火人像" class="headerlink" title="夜景烟火人像"></a>夜景烟火人像</h2><ol>
<li>手动设置M档，大光圈，1&#x2F;80s快门，1000左右ISO，钨丝灯白平衡，连拍模式</li>
<li>不要将仙女棒拿着脸一侧，容易出现“阴阳脸”，2-3根的亮度较合适</li>
<li>尽量使用RAW格式</li>
</ol>
<p>焰火：不长不短的曝光时间，拍1张焰火+1张背景，后期混合模式：变亮</p>
<h1 id="摄影名词解释"><a href="#摄影名词解释" class="headerlink" title="摄影名词解释"></a>摄影名词解释</h1><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Photograph4.png" width="75%">

<p><a href="http://iphoto.blog.163.com/blog/static/174404124200832982358951/?utm_source=qq&utm_medium=social">几个必要的摄影名词解释</a></p>
<h2 id="感光度-ISO"><a href="#感光度-ISO" class="headerlink" title="感光度 ISO"></a>感光度 ISO</h2><p>International Standards Organization，曾是制订胶卷的生产标准，现为CCD&#x2F;CMOS（或胶卷）对光线的敏感程度。</p>
<blockquote>
<p>设 ISO100，$T_{正确曝光} &#x3D; 2s$<br>则 ISO200，$T_{正确曝光} &#x3D; 1s$</p>
</blockquote>
<p>高ISO：速度快，噪点大——弱光场合<br>低ISO：速度慢，画面精细——风光摄影</p>
<h2 id="快门-Shutter"><a href="#快门-Shutter" class="headerlink" title="快门 Shutter"></a>快门 Shutter</h2><p>相机里控制曝光时间的装置。<br>手持相机拍摄的安全速度原则：安全速度是焦距的倒数，如果使用35mm镜头，快门速度不得低于1&#x2F;35秒，使用200mm镜头时速度不得低于1&#x2F;200秒，否则图片就可能糊了。</p>
<h2 id="光圈-Aperture"><a href="#光圈-Aperture" class="headerlink" title="光圈 Aperture"></a>光圈 Aperture</h2><p>镜头里调节进光孔大小的装置。<br>所有相机都基于小孔成像原理：排列组合的几块凹凸镜头+大孔&#x3D;相机镜头<br>常见的光圈值如下： </p>
<figure class="highlight armasm"><table><tr><td class="code"><pre><span class="line"><span class="symbol">F1</span>， <span class="built_in">F1</span>.<span class="number">4</span>， <span class="built_in">F2</span>， <span class="built_in">F2</span>.<span class="number">8</span>， <span class="built_in">F4</span>， <span class="built_in">F5</span>.<span class="number">6</span>， F8， F11， F16， F22， F32， F44， F64。</span><br></pre></td></tr></table></figure>
<p>每两挡相邻光圈值之间进光量相差一倍。<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Photograph5.gif" width="50%"></p>
<p>最佳光圈：镜头在<strong>中等光圈</strong>的时候成像最好（图片最清晰）。<br>光圈大一级则镜片大很多，加工难度高，重量大，价格贵，适合弱光且无三脚架的情况。</p>
<p>光圈有三个作用：</p>
<blockquote>
<ol>
<li>控制进光量，这直接影响到图片是否能正确曝光，是拍摄成功与否的关键；</li>
<li>控制景深，光圈越小，景深越大。虽然焦距和拍摄远近都影响景深，但焦距和被摄物远近的改变同时也会影响构图，如果构图确定，我们能控制景深的武器就只剩下光圈了；</li>
<li>光圈影响图片的清晰度，任何一个镜头都是在中等光圈的时候成像最好（图片最清晰），在最大光圈和最小光圈的时候解像度差。</li>
</ol>
</blockquote>
<p><strong>光圈优先</strong> Aperture Priority: 手动定义光圈的大小，相机会根据这个光圈值确定能正确曝光的快门速度。<br><strong>小光圈</strong>的景深效果在于叙事，在实践中可将其称为叙事性光圈，主要用于需要传达较多照片信息的题材中。<br>为了使夜景画面星光璀璨，可以通过缩小光圈，得到呈星芒效果的灯光。这是因为光圈收缩到一定的程度时，光线就会通过光圈细小的孔洞产生衍射，从而使灯光出现四射的星芒效果，且光圈越小，光线越强烈，星芒效果越明显。</p>
<p>光圈不能无脑开太大，尤其是拍人像时，很容易使面部发虚</p>
<h2 id="曝光-Exposure"><a href="#曝光-Exposure" class="headerlink" title="曝光 Exposure"></a>曝光 Exposure</h2><p>光圈和快门的组合。<br>曝光量：让多少光进入<br>一张正确曝光的图片可以有N种不同的光圈和快门速度组合，若光圈F4快门速度1秒为正确曝光值，那F5.6和2秒以及F8和4秒也同样能得到准确曝光的图片。</p>
<p><strong>总结以上几个名词解释</strong>：有三个因素能影响一张图片是否正确曝光：光圈，快门速度，ISO。其中光圈和速度联合决定进光量，ISO决定CCD&#x2F;CMOS的感光速度。如果进光量不够，我们可以开大光圈或者降低快门速度，还是不够的话就提高ISO。大光圈的缺点是解像度不如中等光圈，快门速度降低则图片可能会糊，提高ISO后图片质量也会下降 。没有完美的方案，如何取舍要灵活决定。</p>
<h2 id="测光与测光模式-Metering"><a href="#测光与测光模式-Metering" class="headerlink" title="测光与测光模式 Metering"></a>测光与测光模式 Metering</h2><p>测定光照强度，相机内的电脑根据电阻值的变化确定光线强度，进而确定曝光值（光圈，快门）。</p>
<p>测光模式主要有点测光，中央重点测光，区域（平均）测光三种。</p>
<blockquote>
<p><strong>点测光</strong>: 只测取景框内一个小点的光线强度（此小点大约为取景框面积的10%到1%，看不同机型）。<br><strong>区域（平均）测光</strong>: 把取景框分为5到63块（看机型不同），分别对每块测光然后再加权平均得到光照强度。<br><strong>中央重点测光</strong>: 简化的区域（平均）测光，只把取景框分为中央圆圈和四周两块，分别测光，然后加权平均（中央圆圈的权重为70%左右）。</p>
</blockquote>
<p>大多数情况下用区域测光即可。在光线明暗反差很大时应该采用点测光。用区域（平均）测光或中央重点也可以，可根据自己的艺术创意进行曝光补偿。</p>
<h2 id="曝光补偿-Exposure-compansation"><a href="#曝光补偿-Exposure-compansation" class="headerlink" title="曝光补偿 Exposure compansation"></a>曝光补偿 Exposure compansation</h2><p>修正（增减）曝光值。<br>照片要能真实反映拍摄时的环境亮度，相机自动测光就是取平均数，最后给出一个让图片达到<strong>中间灰</strong>（柯达灰）的曝光值，相机自动确定的曝光值90%以上是正确的，但纯白和纯黑等场景会测不准，因此需要曝光补偿。</p>
<p>曝光补偿的原则：<strong>白加黑减</strong>。</p>
<blockquote>
<p>如果构图中有大片白色物体或特别明亮的物体，就要相应增加曝光量（增大光圈or&#x2F;and减低快门速度）；<br>如果取景框中有大片黑色的物体，则要减少曝光量。</p>
</blockquote>
<p>如果图片以RAW格式存储的话，其抗过曝&#x2F;欠曝能力是很强的，只要没有曝成完全没层次的一片纯白，过曝&#x2F;欠曝一个EV之内的照片都能在后期电脑处理时调正，而且基本不漏痕迹。但过曝&#x2F;欠曝太多还是不行，如果相差2EV以上，调正后的图片也会很难看。所以掌握曝光补偿白加黑减的原则依然重要。</p>
<h2 id="焦距-Focus"><a href="#焦距-Focus" class="headerlink" title="焦距 Focus"></a>焦距 Focus</h2><p>从镜片（或镜片组）的中心到焦点的距离（mm）。</p>
<ul>
<li>焦距50mm的镜头称为“标准镜头”，简称标头，拍出来的照片类似肉眼平视的感觉（视角为45°左右）；</li>
<li>广角镜头（焦距小于35mm）能够让照相机“看得更宽阔”，因为它视角大；</li>
<li>长焦镜头（焦距大于70mm）能让照相机“看得更远”，但视角窄。</li>
</ul>
<p>光学变焦Optical Zoom：依靠镜片的位移来实现焦距的改变。光学变焦倍数越大，里面的镜片就越多，镜头体积相应较大，画质相对较低，光圈相对较小。$$变焦倍数＝\frac{最大焦距值}{最小焦距值}$$<br>数码变焦只是电子放大，用来骗人的。</p>
<table>
<thead>
<tr>
<th>焦距</th>
<th>镜头类型</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>14-24mm</td>
<td>超广角</td>
<td>视角很大，形变夸张，多用于拍摄建筑与风光</td>
</tr>
<tr>
<td>24-35mm</td>
<td>广角</td>
<td>多用于拍摄建筑与风光以及街头人文抓拍</td>
</tr>
<tr>
<td>35-70mm</td>
<td>中焦</td>
<td>视角接近人眼，多用于人文纪实抓拍</td>
</tr>
<tr>
<td>70-135mm</td>
<td>中长焦</td>
<td>视角比人眼窄，很多人像摄影师喜欢用这个焦段拍半身和头像的特写</td>
</tr>
<tr>
<td>135-300mm</td>
<td>长焦</td>
<td>适合拍摄远距离物体。例如体育摄影、风光特写等等。</td>
</tr>
<tr>
<td>大于300mm</td>
<td>超长焦&#x2F;望远</td>
<td>适合拍摄超远距离物体比如野生动物</td>
</tr>
</tbody></table>
<p><strong>焦距转换系数</strong>:<br>镜头的视角是由镜头焦距和胶卷（或CCD&#x2F;CMOS）尺寸两者联合决定的。尺寸变小相当于焦距变大。大部分数码相机CCD&#x2F;CMOS面积都比原胶片小。<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Photograph6.gif" alt="图 6"><br>假如焦距不变，CCD&#x2F;CMOS越小，镜头视角越小，镜头转换系数只影响视角 。<br>$$镜头转换系数&#x3D;\frac{135胶片对角线长度}{非全幅的CCD&#x2F;CMOS对角线长度}$$<br>佳能：1.6，<br>尼康，Sony：1.5</p>
<p>在其他条件相同的情况下，拍摄时使用的焦距越长，则画面的景深越浅，即可以得到更明显的虚化背景效果；反之，焦距越短视角越广，则画面的景深越大，越容易呈现大景深的画面效果。</p>
<h2 id="景深-Depth-of-field-DOF"><a href="#景深-Depth-of-field-DOF" class="headerlink" title="景深 Depth of field, DOF"></a>景深 Depth of field, DOF</h2><p>照片焦点前后延伸出来的“可接受清晰区域”。</p>
<blockquote>
<p>小（浅）景深 → 背景模糊 → 人像摄影<br>大景深 → 前后都清晰 → 风光摄影</p>
</blockquote>
<p>由三个因素决定：1. 光圈大小；2. 焦距长短；3. 被摄物体的远近。</p>
<ul>
<li>估计景深：<blockquote>
<ol>
<li>光圈越大，景深越小；</li>
<li>焦距越长，景深越小；</li>
<li>离被摄物体越近，景深越小。</li>
</ol>
</blockquote>
</li>
</ul>
<p>需要注意的是，后景深的范围大于前景深的范围。</p>
<h2 id="白平衡-White-Balance"><a href="#白平衡-White-Balance" class="headerlink" title="白平衡 White Balance"></a>白平衡 White Balance</h2><p>照相机对白色的还原准确性。<br>色温color temperature:以开尔文温度表示光线的色彩，单位K。</p>
<table>
<thead>
<tr>
<th>光源</th>
<th>色温（Ｋ）</th>
</tr>
</thead>
<tbody><tr>
<td>蜡烛</td>
<td>2000</td>
</tr>
<tr>
<td>钨丝灯</td>
<td>2500-3200</td>
</tr>
<tr>
<td>荧光灯</td>
<td>4500-6500</td>
</tr>
<tr>
<td>日光（平均）</td>
<td>5400</td>
</tr>
<tr>
<td>有云天气下的日光</td>
<td>6500-7000</td>
</tr>
</tbody></table>
<p>白平衡漂移：根据不同情况，在设置白平衡时设置不同的色调偏好能够得到不同的画面效果。比如在日光下拍摄樱花时，我们可以将日光白平衡中的色调加一些红，这样拍摄出的樱花会更粉嫩。<br>雪景容易偏色<br>物体在不同色温的光源照射下会呈现不同的色调，在日光灯下整体偏白，在普通钨丝白炽灯下整体偏黄。<br>在电脑上可以给RAW图片任意配置色温，彻底解决白平衡问题。</p>
<h2 id="RAW"><a href="#RAW" class="headerlink" title="RAW"></a>RAW</h2><ul>
<li>TIFF：不压缩的格式，单张超过25MB</li>
<li>JPEG：有损压缩格式，占内存小，但有细节损失<blockquote>
<p>CCD&#x2F;CMOS经曝光产生电子图片信号，相机内的小电脑把这些电子信号进行加工处理，再传输给存储卡。这些加工处理包括白平衡配置，颜色饱和度的增减，图片锐度和对比度增减，降低图片噪点等等，最后压缩转换为JPEG格式进行存储。</p>
</blockquote>
</li>
<li>RAW：无损压缩格式，文件较大，后期空间大<blockquote>
<p>CCD&#x2F;CMOS经过曝光产生的图片电子信号直接传给存储卡，文件没有经过相机内部电路的任何图片参数和质量处理。所以RAW文件又被称为数码底片Digital Negatives,几乎所有的参数都可以后期在电脑上调。</p>
</blockquote>
</li>
</ul>
<p>重要图片的拍摄一律存储JPEG+RAW，一般的图片存JPEG。</p>
<h2 id="色彩空间：sRGB-AdobeRGB"><a href="#色彩空间：sRGB-AdobeRGB" class="headerlink" title="色彩空间：sRGB,AdobeRGB"></a>色彩空间：sRGB,AdobeRGB</h2><ul>
<li>AdobeRGB:有更广泛的色彩范围，比sRGB多出35%，照片呈现的色彩更丰富，适合打印，且可转sRGB，但在网络上色彩范围无法完全显示，表现较差；</li>
<li>sRGB：范围较小，但在网络上有不错的表现。</li>
</ul>
<h1 id="相机知识"><a href="#相机知识" class="headerlink" title="相机知识"></a>相机知识</h1><p><a href="http://iphoto.blog.163.com/blog/static/1744041242011721920436">摄影初学者挑选相机的常见问题</a></p>
<img alt="图 6" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Photograph-camera.png"  width="80%"/>  

<h2 id="镜头组合"><a href="#镜头组合" class="headerlink" title="镜头组合"></a>镜头组合</h2><p>来自相机笔记的『几个镜头组合方案』</p>
<ul>
<li>日常家用：16-50&#x2F;18-55 + 55-210（优先覆盖焦距）</li>
<li>旅游拍摄：腾龙18-200（大变焦，相对便携）</li>
<li>人像拍摄：添置35&#x2F;1.8或50&#x2F;1.8</li>
<li>风景拍摄：10-18 + 55-210（可考虑三阳8mm鱼眼）</li>
<li>扫街纪实：适马19&#x2F;2.8</li>
<li>真爱且APS-C不换粉：10-18 + 16-70ZA + FE70-200G（焦距密不透风，且均为F4恒定），再加蔡司Touit三只，原厂24&#x2F;1.8 ZA和55&#x2F;1.8 ZA。</li>
</ul>
<p>如果你对摄影的确是有很浓厚的兴趣，而且预算又有余，那么可以在第一支 标准变焦镜头的基础上，再购买一支定焦镜头，它是：50&#x2F; F1.8。 不管对哪个品牌来说，50 &#x2F; F1.8都是一支经典镜头，极低廉的价格，却享有不 逊牛头的出色画质，1.8的大光圈可以让你更好的体会光圈的影响和作用，是 一支学习摄影的经典镜头 </p>
<h2 id="相机清洁"><a href="#相机清洁" class="headerlink" title="相机清洁"></a>相机清洁</h2><ul>
<li><p>灰尘：</p>
<blockquote>
<p>气吹：先空吹几下<br>  棉签：确保无硬质灰尘，若有，用棉签定点清除<br>  擦镜布：顺一个方向，不要反复擦，由内向外螺旋擦拭</p>
</blockquote>
</li>
<li><p>指纹油脂：</p>
<blockquote>
<p>将镜头清洁液滴一滴在擦镜布上<br>  先挥发一下，再开始擦拭，手法同上</p>
</blockquote>
</li>
<li><p>雨水&#x2F;海水：</p>
<blockquote>
<p>气吹猛吹<br>  大量清洁液滴在布上，不等挥发直接快速擦<br>  用布较干的地方擦干净，再用毛巾&#x2F;软布擦干机身<br>  海水需要多擦几遍</p>
</blockquote>
</li>
<li><p>其他工具：</p>
<blockquote>
<p>毛刷：刷机身灰尘<br>  棉签：死角、取景器（加液）、镜头卡口<br>  节电导通剂：清洁镜头触点</p>
</blockquote>
</li>
<li><p>传感器：</p>
<blockquote>
<p>1.找灰尘：用小光圈（F16+）试拍白纸，手动对焦无穷远，灰尘位置上下左右颠倒（或有手动清洁模式）<br>  2.气吹<br>  3.传感器清洁棒：从左到右反复，柔和均匀<br>  4.传感器清洁液：等挥发</p>
</blockquote>
</li>
</ul>
<p>最后记得清洁镜头盖；<br>换镜头时要找背风处，动作要快。</p>
<h2 id="拥有相机后快速入门"><a href="#拥有相机后快速入门" class="headerlink" title="拥有相机后快速入门"></a>拥有相机后快速入门</h2><ol>
<li>读一本好书《美国纽约摄影学院摄影教材（上）》《美国摄影用光教程》</li>
<li>阅读相机说明书熟悉常用设置和操作</li>
<li>大量拍摄和学习</li>
<li>关注几个严肃摄影媒体建立正确的价值观和审美情趣</li>
<li>提高艺术文化修养，你的照片反映了你是什么样的人</li>
</ol>
]]></content>
      <tags>
        <tag>技术</tag>
        <tag>摄影</tag>
      </tags>
  </entry>
  <entry>
    <title>Py：Anaconda，Pycharm，Pytorch</title>
    <url>/Pytorch/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>深度学习相关工具使用笔记，包括Anaconda, Pycharm, Jupyter notebook, Google Colab，以及Pytorch，项目地址：<a href="https://github.com/Arrowes/DLpractice">DLpractice</a></p>
<span id="more"></span>

<p>视频：<a href="https://www.bilibili.com/video/BV1hE411t7RN/">PyTorch深度学习快速入门教程</a></p>
<h1 id="Anaconda"><a href="#Anaconda" class="headerlink" title="Anaconda"></a>Anaconda</h1><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">conda create -n pytorch python=3.8  <span class="comment">#Anaconda创建环境</span></span><br><span class="line">conda update python <span class="comment">#更新py</span></span><br><span class="line">conda activate pytorch  <span class="comment">#激活环境</span></span><br><span class="line">conda deactivate    <span class="comment">#退出虚拟环境</span></span><br><span class="line">conda remove pytorch --all  <span class="comment">#删除环境 或conda env remove -n XXX</span></span><br><span class="line">conda list          <span class="comment">#查看环境中的所有包</span></span><br><span class="line">conda install XXX   <span class="comment">#安装 XXX 包</span></span><br><span class="line">conda remove XXX    <span class="comment">#删除 XXX 包</span></span><br><span class="line">conda <span class="built_in">env</span> list      <span class="comment">#列出所有环境</span></span><br><span class="line">conda create -n XXX jupyter notebook  <span class="comment">#创建环境并安装Jupyter Notebook</span></span><br><span class="line">jupyter notebook    <span class="comment">#打开Jupyter Notebook</span></span><br><span class="line"></span><br><span class="line">pip install XXX==2.0 (pip uninstall XXX) <span class="comment">#安装特定版本/卸载</span></span><br><span class="line">pip install --upgrade pip <span class="comment">#更新pip</span></span><br><span class="line">pip config <span class="built_in">set</span> global.index-url https://pypi.tuna.tsinghua.edu.cn/simple  <span class="comment">#pip换源</span></span><br><span class="line">pip config list -v  <span class="comment">#查源</span></span><br><span class="line">pip install opencv-python -i https://pypi.tuna.tsinghua.edu.cn/simple/  <span class="comment">#pip换源安装</span></span><br><span class="line">conda config --remove-key channels    <span class="comment">#换回默认源</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>国内镜像源:<br>  清华大学：<a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a><br>  阿里云：<a href="http://mirrors.aliyun.com/pypi/simple">http://mirrors.aliyun.com/pypi/simple</a><br>  豆瓣：<a href="http://pypi.douban.com/simple">http://pypi.douban.com/simple</a></p>
</blockquote>
<p>修改conda环境保存路径(windows):<br>1.找到用户目录下的.condarc文件（C:\Users\username）。<br>2.打开.condarc文件之后，添加或修改.condarc 中的 env_dirs 设置环境路径:<code>envs_dirs:  - D:\Anaconda3\envs</code></p>
<h1 id="Pycharm"><a href="#Pycharm" class="headerlink" title="Pycharm"></a>Pycharm</h1><p>创建新项目，手动导入已存在的anaconda创建的环境<code>D:\Anaconda3\envs\pytorch\python.exe</code></p>
<p>配置或更改环境：				<code>Settings &gt; Project &gt; interpreter &gt; 齿轮 &gt; Add &gt; System Interpreter &gt; 手动添加</code></p>
<p>Terminal：<code>File &gt; Settings &gt; Tools &gt; Terminal &gt; Shell path</code> <code>C:\Windows\System32\cmd.exe</code></p>
<p><strong>Pycharm使用</strong></p>
<blockquote>
<p>新建.py文件(用作工程) &gt; 右上角配置py解释器 &gt; 运行&#x2F;直接右键运行<br>控制台(用作调试，查参数) &gt; Shift+enter：输入多行 &gt; “↑”重新编辑<br>Ctrl + D 快速复制至下一行<br>Ctrl + R 批量修改<br>Ctrl + &#x2F; 批量注释<br>调试：打断点 &gt; debug &gt; 使用工具栏内的“下一步”或“计算器内输入表达式”进行调试</p>
</blockquote>
<p>调试时使用Console的python调试台，print指令<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Pytorch1.png" alt="图 1"> <img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Pytorch2.png" alt="图 2">  </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Debug:</span></span><br><span class="line">ModuleNotFoundError: No module named <span class="string">&#x27;xxx&#x27;</span></span><br><span class="line"><span class="comment">#需要导入项目根目录：</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&#x27;/home/ywang85/edgeai-yolox&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h1 id="Jupyter-notebook"><a href="#Jupyter-notebook" class="headerlink" title="Jupyter notebook"></a>Jupyter notebook</h1><p>激活pytorch环境后: <code>conda install nb_conda</code><br>打开 jupyter notebook: <code>New &gt; 选择环境：Python [conda env:pytorch]</code><br>IDLE Ctrl+N 编辑多行代码<br>输入一半按tab可以补全</p>
<p>打包与解压缩</p>
<ul>
<li><p>打包：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line">zip_ref = zipfile.ZipFile(<span class="string">&#x27;WYJ.zip&#x27;</span>, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">zip_ref.extractall(<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">zip_ref.close()</span><br></pre></td></tr></table></figure>
</li>
<li><p>解压：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">zip_folder</span>(<span class="params">folder_path, zip_path</span>):</span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(zip_path, <span class="string">&#x27;w&#x27;</span>, zipfile.ZIP_DEFLATED) <span class="keyword">as</span> zipf:</span><br><span class="line">        <span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(folder_path):</span><br><span class="line">            <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">                file_path = os.path.join(root, file)</span><br><span class="line">                zipf.write(file_path, os.path.relpath(file_path, folder_path))</span><br><span class="line"></span><br><span class="line">folder_path = <span class="string">&#x27;./output&#x27;</span> <span class="comment"># 指定要下载的文件夹路径</span></span><br><span class="line">zip_path = <span class="string">&#x27;./output.zip&#x27;</span> <span class="comment"># 指定要保存的zip文件路径</span></span><br><span class="line">zip_folder(folder_path, zip_path)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> FileLink</span><br><span class="line">FileLink(zip_path) <span class="comment"># 生成下载链接</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="Google-Colab"><a href="#Google-Colab" class="headerlink" title="Google Colab"></a><a href="https://colab.research.google.com/">Google Colab</a></h1><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#设置并查看GPU 修改&gt;笔记本设置&gt;GPU</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tf.test.gpu_device_name()</span><br><span class="line"></span><br><span class="line">!/opt/<span class="built_in">bin</span>/nvidia-smi <span class="comment">#详情</span></span><br></pre></td></tr></table></figure>

<p><strong>基本指令</strong></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">!unzip /content/XX.<span class="built_in">zip</span> -d /content/XX <span class="comment">#解压</span></span><br><span class="line">%cd /content/XX   <span class="comment">#进入</span></span><br><span class="line">!pip install -r requirements.txt    <span class="comment">#安装requirements</span></span><br><span class="line">!python XX.py --rect    <span class="comment">#运行</span></span><br><span class="line">!rm -rf /content/XX/mydata  <span class="comment">#删除</span></span><br><span class="line"></span><br><span class="line">%load_ext tensorboard   <span class="comment">#加载tensorboard</span></span><br><span class="line">%tensorboard --logdir=runs/train    <span class="comment">#执行tensorboard</span></span><br></pre></td></tr></table></figure>

<p><strong>云盘</strong></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#先装载谷歌云盘，在云盘里运行以防数据丢失，指定Google Drive云端硬盘的根目录，名为drive</span></span><br><span class="line">!mkdir -p drive</span><br><span class="line">!google-drive-ocamlfuse drive</span><br><span class="line"></span><br><span class="line"><span class="comment">#connect to self drive</span></span><br><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line">drive.mount(<span class="string">&#x27;/content/drive&#x27;</span>)</span><br><span class="line"><span class="comment">#云训练时还是要将盘里的文件拿出来再开始，否则容易直接断连!</span></span><br></pre></td></tr></table></figure>
<p>续航插件：<a href="https://chrome.google.com/webstore/detail/colab-alive/eookkckfbbgnhdgcbfbicoahejkdoele?hl=zh-CN">Colab Alive</a>, 防止训练时掉线</p>
<h1 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h1><p>要调用GPU进行训练的话，需要安装显卡驱动对应的CUDA</p>
<ol>
<li><code>nvidia-smi</code> 查询支持CUDA版本, 显卡驱动程序显示的cuda版本为电脑最高可适配的cuda版本<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Pytorch3.png" alt="图 3">  </li>
<li>到 <a href="https://pytorch.org/get-started/locally/">Pytorch官网</a> 复制对应code进行安装<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Pytorch4.png" alt="图 4"></li>
</ol>
<p><strong>查GPU</strong></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">python -c <span class="string">&quot;import torch;print(torch.cuda.is_available())&quot;</span>   <span class="comment">#返回True说明GPU可以被使用</span></span><br><span class="line">torch.__version__ <span class="comment">#查pytorch版本</span></span><br><span class="line">nvidia-smi -l <span class="number">2</span>   <span class="comment">#查GPU CUDA, &#x27;-l 2&#x27;:每2s更新一次</span></span><br><span class="line">python –-version  <span class="comment">#查python版本</span></span><br><span class="line">conda install python=<span class="number">3.8</span>  <span class="comment">#升级(覆盖安装)python</span></span><br></pre></td></tr></table></figure>

<h2 id="库"><a href="#库" class="headerlink" title="库"></a>库</h2><p><strong>两大查询函数</strong>：<br>dir() 函数，能让我们知道工具箱以及工具箱中的分隔区有什么东西。<br>help() 函数，能让我们知道每个工具是如何使用的，工具的使用方法。<br><code>Jupyter&gt;XX??</code><br><code>Pycharm&gt;ctrl+左键(查原函数)	ctrl+p(查询输入参数，有等号的可忽略)</code><br>多查 <a href="https://pytorch.org/docs/stable/index.html">pytorch官方文档</a></p>
<ul>
<li>文件<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">../XXX <span class="comment">#上一层</span></span><br><span class="line">root=“D:\\desktop”  <span class="comment">#window下绝对路径使用双斜杠\\避免转义：</span></span><br><span class="line">root=r“D:\\desktop” <span class="comment">#或统一加上r取消转义</span></span><br></pre></td></tr></table></figure></li>
<li>计时		<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time 	</span><br><span class="line">start=time.time()     end=time.time()     <span class="built_in">print</span>(start-end)</span><br></pre></td></tr></table></figure></li>
<li>组 <figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a = (<span class="number">1</span>, <span class="number">2</span>)    <span class="comment"># 元组 tuple</span></span><br><span class="line">b = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>] <span class="comment"># 数组 list</span></span><br><span class="line">c = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;wyj&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="string">&#x27;23&#x27;</span>&#125;  <span class="comment"># 字典 dict</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(b[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(c[<span class="string">&#x27;name&#x27;</span>])</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="Tensorboard可视化"><a href="#Tensorboard可视化" class="headerlink" title="Tensorboard可视化"></a>Tensorboard可视化</h3><p>pytorch下安装 <code>pip install tensorboard (conda install tensorboard)</code></p>
<ul>
<li>使用<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line">writer=SummaryWriter(“logs“)</span><br><span class="line">writer.add_image(<span class="string">&quot;name“，parameter，组内步数)</span></span><br><span class="line"><span class="string">writer.close() #关闭读写 </span></span><br></pre></td></tr></table></figure></li>
<li>打开<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">tensorboard --logdir=logs(文件夹路径) --port=<span class="number">6006</span>（<span class="number">6007</span>） <span class="comment">#注意路径</span></span><br><span class="line">tensorboard --logdir runs/train  （YOLO）</span><br></pre></td></tr></table></figure>
地址	localhost:6006<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#debug</span></span><br><span class="line">AttributeError: <span class="built_in">type</span> <span class="built_in">object</span> <span class="string">&#x27;h5py.h5.H5PYConfig&#x27;</span> has no attribute <span class="string">&#x27;__reduce_cython__&#x27;</span></span><br><span class="line">pip uninstall h5py</span><br><span class="line"></span><br><span class="line">AttributeError: module <span class="string">&#x27;distutils&#x27;</span> has no attribute <span class="string">&#x27;version&#x27;</span></span><br><span class="line">pip install setuptools==<span class="number">59.5</span><span class="number">.0</span></span><br><span class="line">pip install brotli</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="Transform"><a href="#Transform" class="headerlink" title="Transform"></a>Transform</h3><p>transforms.py图像处理工具箱</p>
<ol>
<li>调用工具tool&#x3D;transforms.XXX()	</li>
<li>使用 result&#x3D;tool(input)<br>如: Totensor&gt;转向量; Normalize&gt;归一化; Resize&gt;缩放; Compose&gt;组合; RandomCrop&gt;随机裁剪</li>
</ol>
<h3 id="ToTensor"><a href="#ToTensor" class="headerlink" title="ToTensor"></a>ToTensor</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#（桥梁，很多输入都要求tensor类型）</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line">tensor_tool=transforms.ToTensor()</span><br><span class="line">tensor_result=tensor_tool(img)</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Person</span>:</span><br><span class="line">	_ _call_ _(self,name)	</span><br><span class="line">可直接调用&gt;person=Person(“wyj<span class="string">&quot;)</span></span><br></pre></td></tr></table></figure>
<h3 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h3><p>torchvision.datasets 数据集处理</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>, train=<span class="literal">True</span>, <span class="comment">#训练集</span></span><br><span class="line">transform=dataset_transform, download=<span class="literal">True</span>)</span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>, train=<span class="literal">False</span>, <span class="comment">#测试集</span></span><br><span class="line">transform=dataset_transform, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Dataloader"><a href="#Dataloader" class="headerlink" title="Dataloader"></a>Dataloader</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line">test_loader = DataLoader(dataset=test_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>, drop_last=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>例</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">writer = SummaryWriter(<span class="string">&quot;dataloader&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> test_loader: <span class="comment">#读数据</span></span><br><span class="line">        imgs, targets = data</span><br><span class="line">        writer.add_images(<span class="string">&quot;Epoch: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch), imgs, step)</span><br><span class="line">        step = step + <span class="number">1</span></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<h2 id="TORCH-NN"><a href="#TORCH-NN" class="headerlink" title="TORCH.NN"></a>TORCH.NN</h2><p><strong>Module 所有神经网络模块的基类</strong></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module): <span class="comment"># 继承Module模板（父类），在该模板基础上进行修改</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>): <span class="comment">#初始化</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__() 	<span class="comment">#父类初始化</span></span><br><span class="line">        self.conv1 = Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>) <span class="comment">#2维卷积</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):<span class="comment">#前向传播 x即input</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> x </span><br><span class="line">model=Model() 		<span class="comment">#创建神经网络</span></span><br><span class="line">output=model(x)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>
<p><strong>maxpool 下采样</strong><br><code>self.maxpool1 = MaxPool2d(kernel_size=3, ceil_mode=False) #ceil_mode=True为向上取整</code><br><strong>Non-linear Activations</strong><br><code>m = nn.ReLU() # inplace=True为直接替换input</code><br><strong>批标准化</strong><br><code>m = nn.BatchNorm2d(100)</code><br><strong>线性层</strong></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>，</span><br><span class="line">			transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui, self).__init__()</span><br><span class="line">        self.linear1 = Linear(<span class="number">196608</span>, <span class="number">10</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.linear1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"> </span><br><span class="line">tudui = Tudui()</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)</span><br><span class="line">    output = torch.flatten(imgs) <span class="comment">#数据摊平</span></span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br><span class="line">    output = tudui(output)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure>
<h2 id="CIFAR-10-model结构"><a href="#CIFAR-10-model结构" class="headerlink" title="CIFAR 10 model结构"></a>CIFAR 10 model结构</h2><p><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Network1.png" alt="图 1">  </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WYJ</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(WYJ,self).__init__()</span><br><span class="line">        self.conv1=Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,padding=<span class="number">2</span>)</span><br><span class="line">        self.maxpool1=MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.conv2 = Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        self.maxpool2 = MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.conv3 = Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        self.maxpool3 = MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.flatten=Flatten()</span><br><span class="line">        self.linear1=Linear(<span class="number">1024</span>,<span class="number">64</span>)</span><br><span class="line">   self.linear2 = Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.maxpool1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.maxpool2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.maxpool3(x)</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.linear1(x)</span><br><span class="line">        x = self.linear2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"> </span><br><span class="line">wyj = WYJ()</span><br><span class="line"><span class="built_in">print</span>(wyj)</span><br><span class="line"><span class="built_in">input</span> = torch.ones((<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">output = wyj(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"> </span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;../logs_seq&quot;</span>)</span><br><span class="line">writer.add_graph(wyj, <span class="built_in">input</span>)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>])</span><br><span class="line">y = torch.tensor([<span class="number">1</span>])</span><br><span class="line">x = torch.reshape(x, (<span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line">loss_cross = nn.CrossEntropyLoss()    <span class="comment">#注意输入输出形状</span></span><br><span class="line">result_cross = loss_cross(x, y)</span><br></pre></td></tr></table></figure>
<p><strong>反向传播</strong><br><code>result_loss.backward()</code></p>
<h2 id="优化器-梯度下降"><a href="#优化器-梯度下降" class="headerlink" title="优化器(梯度下降)"></a>优化器(梯度下降)</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> dataset:</span><br><span class="line">    optimizer.zero_grad()              <span class="comment">#梯度清零</span></span><br><span class="line">    output = model(<span class="built_in">input</span>)</span><br><span class="line">    loss = loss_fn(ou tput, target)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<h2 id="网络模型"><a href="#网络模型" class="headerlink" title="网络模型"></a>网络模型</h2><ol>
<li><p>添加<br><code>vgg16_true.classifier.add_module(&#39;add_linear&#39;, nn.Linear(1000, 10)) #在vgg16网络中的classifier 模块中加入一个linear层</code></p>
</li>
<li><p>修改<br><code>vgg16_false.classifier[6] = nn.Linear(4096, 10) #修改vgg16网络中的classifier 第6层</code></p>
</li>
<li><p>保存</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">vgg16 = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存方式1,模型结构+模型参数</span></span><br><span class="line">torch.save(vgg16, <span class="string">&quot;vgg16_method1.pth&quot;</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 保存方式2，模型参数（官方推荐）</span></span><br><span class="line">torch.save(vgg16.state_dict(), <span class="string">&quot;vgg16_method2.pth&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li><p>加载</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 方式1</span></span><br><span class="line">model = torch.load(<span class="string">&quot;vgg16_method1.pth&quot;</span>)</span><br><span class="line"> <span class="comment">#若为自定义网络模型，加载时需要引入定义：</span></span><br><span class="line"><span class="keyword">from</span> model_save <span class="keyword">import</span> *  </span><br><span class="line"><span class="comment">#一定要在同一个文件夹下</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 方式2，加载模型</span></span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line">vgg16.load_state_dict(torch.load(<span class="string">&quot;vgg16_method2.pth&quot;</span>)) <span class="comment">#读取字典中的参数</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="CPU训练"><a href="#CPU训练" class="headerlink" title="CPU训练"></a>CPU训练</h2> <figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> …</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> * <span class="comment">#引入CIFAR 10 model网络定义</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">#准备数据集 </span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line">test_data = torchvision.datasets.CIFAR10(root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"> <span class="comment">#length 数据集长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练数据集的长度为：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(train_data_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试数据集的长度为：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_data_size))</span><br><span class="line"> </span><br><span class="line"> <span class="comment">#利用 DataLoader 来加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=<span class="number">64</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"> </span><br><span class="line"> <span class="comment">#创建网络模型</span></span><br><span class="line">tudui = Tudui()</span><br><span class="line"> <span class="comment">#损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"> <span class="comment">#优化器</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">optimizer = torch.optim.SGD(tudui.parameters(), lr=learning_rate)</span><br><span class="line"> </span><br><span class="line"> <span class="comment">#设置训练网络的一些参数</span></span><br><span class="line"> <span class="comment">#记录训练的次数</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line"> <span class="comment">#记录测试的次数</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line"> <span class="comment">#训练的轮数</span></span><br><span class="line">epoch = <span class="number">10</span></span><br><span class="line"> </span><br><span class="line"> <span class="comment">#添加tensorboard</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;../logs_train&quot;</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;------第 &#123;&#125; 轮训练开始------&quot;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>))</span><br><span class="line">    <span class="comment">#训练步骤开始</span></span><br><span class="line">    tudui.train() <span class="comment">#网络设为训练模式</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = tudui(imgs)</span><br><span class="line">        loss = loss_fn(outputs, targets)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 优化器优化模型</span></span><br><span class="line">        optimizer.zero_grad() <span class="comment">#梯度清零</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"> </span><br><span class="line">        total_train_step = total_train_step + <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>: <span class="comment">#逢百进一</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;训练次数：&#123;&#125;, Loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_train_step, loss.item()))</span><br><span class="line">            writer.add_scalar(<span class="string">&quot;train_loss&quot;</span>, loss.item(), total_train_step)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 验证步骤开始</span></span><br><span class="line">    tudui.<span class="built_in">eval</span>() <span class="comment">#网络设为验证模式</span></span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    total_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): <span class="comment">#避免验证过程加入梯度，以节约内存</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader:</span><br><span class="line">            imgs, targets = data</span><br><span class="line">            outputs = tudui(imgs)</span><br><span class="line">            loss = loss_fn(outputs, targets)</span><br><span class="line">            total_test_loss = total_test_loss + loss.item()</span><br><span class="line">            accuracy = (outputs.argmax(<span class="number">1</span>) == targets).<span class="built_in">sum</span>() <span class="comment">#argmax(1)：横轴取最大值并输出序号[0,1,0,0,…]</span></span><br><span class="line">            total_accuracy = total_accuracy + accuracy</span><br><span class="line"> </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的Loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的正确率: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_accuracy/test_data_size))</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_loss&quot;</span>, total_test_loss, total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_accuracy&quot;</span>, total_accuracy/test_data_size, total_test_step)</span><br><span class="line">    total_test_step = total_test_step + <span class="number">1</span></span><br><span class="line">    <span class="comment">#每epoch保存一个</span></span><br><span class="line">    torch.save(tudui, <span class="string">&quot;tudui_&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;模型已保存&quot;</span>) </span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<h2 id="GPU训练"><a href="#GPU训练" class="headerlink" title="GPU训练"></a>GPU训练</h2><p>改cuda：网络、损失函数、data<br>① .cuda()</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    tudui = tudui.cuda()</span><br></pre></td></tr></table></figure>
<p>② .to(device)</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>) <span class="comment">#只需修改此处在CPU,GPU之间切换	“cpu”</span></span><br><span class="line">tudui = tudui.to(device)</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> “cpu”)</span><br></pre></td></tr></table></figure>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">model = torch.load(<span class="string">&quot;tudui_29_gpu.pth&quot;</span>, map_location=torch.device(<span class="string">&#x27;cpu&#x27;</span>)) </span><br><span class="line"><span class="built_in">print</span>(model) 	<span class="comment">#训练好的权重代入模型</span></span><br><span class="line">image = torch.reshape(image, (<span class="number">1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    output = model(image)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Project：多个项目汇总</title>
    <url>/Project/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>FEY-YOLOv7，CEAM-YOLOv7，基于 STM32 的智能空压机状态监测系统，SE智厨，风功率密度便携式测量仪。</p>
<span id="more"></span>

<h1 id="FEY-YOLOv7：基于面部小目标动态追踪的驾驶员疲劳检测算法"><a href="#FEY-YOLOv7：基于面部小目标动态追踪的驾驶员疲劳检测算法" class="headerlink" title="FEY-YOLOv7：基于面部小目标动态追踪的驾驶员疲劳检测算法"></a>FEY-YOLOv7：基于面部小目标动态追踪的驾驶员疲劳检测算法</h1><ul>
<li>SCI四区论文：<a href="https://search.ieice.org/bin/summary_advpub.php?id=2023EDP7093&category=D&lang=E&abst=">A Driver Fatigue Detection Algorithm Based on Dynamic Tracking of Small Facial Targets Using YOLOv7</a></li>
<li>Github: <a href="https://github.com/Arrowes/FEY-YOLOv7">FEY-YOLOv7</a></li>
</ul>
<h2 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h2><p>在车辆安全技术中，驾驶员疲劳检测应用广泛，其准确性和实时性至关重要。在本文中，我们提出了一种基于人脸眼睛和嘴巴打哈欠动态追踪的YOLOv7驾驶员疲劳检测算法，其中YOLOv7针对眼睛和嘴巴小目标进行了优化，结合PERCLOS算法，称为FEY-YOLOv7。在YOLOv7中插入Coordinate Attention(CA)模块，将重点放在坐标信息上，以提高动态追踪的准确性；增加一个小目标检测头，使网络能够提取小目标特征，增强了对眼睛、嘴巴的检测性能，提高了检测的精度。对YOLOv7的网络架构进行了显著简化，以减少计算量，提高检测速度。通过提取视频中每一帧驾驶员睁眼、闭眼、张嘴、闭嘴四种面部行为状态，利用 PERYAWN 判定算法对驾驶员状态进行标注及检测。在RGB-infrared Datasets上使用Guided Image Filtering图像增强算法，并进行混合训练及验证，验证结果表明，FEY-YOLOv7的mAP达到了0.983，FPS达到101，说明FEY-YOLOv7在准确率和速度上都优于最先进的方法，为基于图像信息的驾驶员疲劳检测提供了一个有效和实用的方案。</p>
<h2 id="技术点"><a href="#技术点" class="headerlink" title="技术点"></a>技术点</h2><h3 id="CA注意力机制"><a href="#CA注意力机制" class="headerlink" title="CA注意力机制"></a>CA注意力机制</h3><p>CA模块在空间维度上自适应地对不同位置的特征进行加权，从而使得模型更加关注重要的空间位置，不仅捕获跨通道信息，还捕获方向感知和位置敏感信息，这有助于模型更准确地定位和识别感兴趣的对象。<br>具体来说，Coordinate Attention引入了一个全局自注意力模块，该模块可以对输入特征图的每个位置进行自适应的加权。该加权由两个步骤完成：Coordinate信息嵌入和Coordinate Attention生成。<br>1.	通过两个全局平均池化操作，分别计算输入特征图在通道维度上和空间维度上的均值。这两个均值分别表示了输入特征图在每个通道和每个空间位置的重要性。<br>2.	将通道维度上的均值与空间维度上的均值进行相乘，得到一个权重矩阵，该权重矩阵表示了每个位置在通道和空间维度上的重要性，并将其应用于输入特征图中。最终，每个位置的特征将与其在通道和空间维度上的重要性相关联，从而使得模型更加关注重要的空间位置。<br><img alt="图 43" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project-CA.png" width='38%'/>  </p>
<h3 id="PERYAWN疲劳评估算法"><a href="#PERYAWN疲劳评估算法" class="headerlink" title="PERYAWN疲劳评估算法"></a>PERYAWN疲劳评估算法</h3><p>$PERYAWN &#x3D; E&#x2F;(N - w · M)$</p>
<p>其中，E是闭眼帧数，N是单位时间内总帧数，M代表“张嘴”的帧数，w是加权因子。为了量化四个驾驶员特征（睁眼、闭眼、张嘴和闭嘴）的检测结果，我们采用公式4和5，并将权重w设置为0.15。基于本研究中数据集标签的实际情况，我们确定当单位时间内的PERYAWN值超过0.20时，驾驶员处于疲劳状态。</p>
<p>为了更直观地评估该算法在检测驾驶员状态方面的有效性，我们使用提出的PERYAWN参数作为定量指标来评估多个10秒的测试视频（每秒24帧）。如下图所示，如果PERYAWN值超过0.2，则将驾驶员分类为疲劳状态。如果该值超过0.5，则认为驾驶员处于严重疲劳状态。我们将检测到的状态与驾驶员的实际状态进行比较，并计算准确性。除了一些视频检测结果出现偏差外，所有结果都能够准确检测到三种状态：’正常’，’疲劳’和’严重疲劳’。<br><img alt="图 44" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project-PERYAWN.png" width='60%'/>  </p>
<h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><ul>
<li><p>引导滤波算法<br>引导滤波就是基于局部线性回归，用引导图像的信息来指导输入图像的滤波过程，通常用于图像处理中的去噪、平滑、增强等任务。<br>引导滤波器的基本思想是，对于输入图像p中的每个像素，使用引导图像I中与该像素相关的信息来进行滤波, 引导滤波器将输入图像p的每个像素表示为一个线性组合, 利用线性岭回归模型对线性系数ak,bk进行求解；本文利用原图的灰度图实现了图像的细节增强，优化了对眼睛、嘴巴的目标检测效果，实现过程如下图：</p>
<img alt="图 45" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project-GuidedImageFilter.png" width='80%'/>  
</li>
<li><p>Static Crop+Mosaic预处理<br>对图像进行35-59% Horizontal Region, 25-75% Vertical Region的拆分，并进行4合1的Mosaic拼接，间接实现了面部特征的放大，将数据集重点偏向眼睛、嘴巴这类小目标，优化了算法对小目标的检测性能和鲁棒性。</p>
<img alt="图 46" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project-Mosaic.png" width='80%'/></li>
</ul>
<h2 id="实现效果"><a href="#实现效果" class="headerlink" title="实现效果"></a>实现效果</h2><img alt="图 48" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project-FEYdetectResult.png" width='80%'/>  

<img alt="图 47" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project-FEYresult.png" width='80%'/>  


<h1 id="CEAM-YOLOv7：基于通道扩展注意机制的改进YOLOv7驾驶员行为检测算法"><a href="#CEAM-YOLOv7：基于通道扩展注意机制的改进YOLOv7驾驶员行为检测算法" class="headerlink" title="CEAM-YOLOv7：基于通道扩展注意机制的改进YOLOv7驾驶员行为检测算法"></a>CEAM-YOLOv7：基于通道扩展注意机制的改进YOLOv7驾驶员行为检测算法</h1><ul>
<li><a href="https://ieeexplore.ieee.org/document/9980374/metrics">CEAM-YOLOv7:Improved YOLOv7 Based on Channel Expansion Attention Mechanism for Driver behavior detection</a></li>
<li>Github: <a href="https://github.com/Arrowes/CEAM-YOLOv7">CEAM-YOLOv7</a></li>
</ul>
<h2 id="项目简介-1"><a href="#项目简介-1" class="headerlink" title="项目简介"></a>项目简介</h2><p>驾驶员的不规范行为易引发交通事故，因此，为规范驾驶员行为，减少交通事故的发生，对驾驶员行为进行检测至关重要。本文提出了一种改进的目标检测模型CEAM-YOLOv7，该模型利用GAM注意力模块和通道扩展数据增强算法来减少特征图生成过程中的信息丢失，提高检测精度。将YOLOv7架构的Backbone和Head部分加入GAM注意力模块，减少信息损失的同时放大全局维度交互特征，同时，使用剪枝算法，在保证实时检测的前提下，提高了YOLOv7网络的检测性能。此外，使用更适合于实际驾驶场景的红外图像数据集进行训练，结合inversion和CLAHE图像增强方法，提出了一种基于通道扩展的红外图像数据增强算法，改善针对红外图像的目标检测效果。经大量实验结果表明，与YOLOv7相比，CEAM-YOLOv7的map提升了20.26%，FPS达到了156，本文方法的有效性和优越性得到了验证。 </p>
<h2 id="技术点-1"><a href="#技术点-1" class="headerlink" title="技术点"></a>技术点</h2><h3 id="通道扩展算法"><a href="#通道扩展算法" class="headerlink" title="通道扩展算法"></a>通道扩展算法</h3><p><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project1.png" alt="图 1">  </p>
<figure class="highlight avrasm"><table><tr><td class="code"><pre><span class="line"><span class="symbol">inversion:</span>通过域迁移的思想使得网络能够更加适应处理后的红外图像。一般用于目标检测所用的 RGB 图像都是白天所摄，通常情况是背景较亮，目标较暗。但驾驶员环境通常较暗，且红外图像成像为辐射特性，背景辐射较弱而目标辐射较强，因此选用 inversion 操作；</span><br><span class="line"><span class="symbol">CLAHE:</span>由于红外图像的对比度比较低，其灰度分布通常都是分布在较窄的区域，采用自适应直方图均衡化能够使红外图像的灰度分布更均匀，增强对比度的同时抑制噪声，从而达到增加图像细节信息的作用。</span><br></pre></td></tr></table></figure>
<h3 id="GAM注意力机制"><a href="#GAM注意力机制" class="headerlink" title="GAM注意力机制"></a>GAM注意力机制</h3><p><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project2.png" alt="图 2"><br>GAM是一种能够捕捉所有三个维度的显著特征的注意机制，采用了CBAM中的顺序通道-空间注意机制，对通道-空间注意力子模块进行了重新设计，通道注意力子模块使用3D置换来跨三维保持信息，使用用两层感知器MLP（Multi-Layer Perceptron）放大跨维信道-空间相关性；空间注意力子模块采用了两个卷积层进行空间信息融合。由此，通过减少信息丢失和放大全局交互特征来提高深度神经网络的性能，提高了对于红外图像目标的识别能力，在识别速度和精度之间进行了有效的权衡，也与数据增强处理中的通道扩展算法相对应。</p>
<h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project3.png" alt="图 3">  </p>
<h2 id="实现效果-1"><a href="#实现效果-1" class="headerlink" title="实现效果"></a>实现效果</h2><p> <img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project4.png" alt="图 4">  </p>
<p><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project5.png" alt="图 5">  </p>
<h1 id="基于-STM32-的智能空压机状态监测系统"><a href="#基于-STM32-的智能空压机状态监测系统" class="headerlink" title="基于 STM32 的智能空压机状态监测系统"></a><a href="https://oshwhub.com/Arrows/esp-kong-zhi-ji-dian-qi">基于 STM32 的智能空压机状态监测系统</a></h1><p>2020-2021<br>优秀毕设</p>
<h2 id="项目简介-2"><a href="#项目简介-2" class="headerlink" title="项目简介"></a>项目简介</h2><p>空气压缩机是一种把气体压缩，从而提升气体压力的设备，使用范围广，其运行时的稳定性与可靠性将直接影响到企业生产时的生命与财产安全。本设计以往复式活塞压缩机为研究对象，基于因特网实现了对空压机运行状态的实时监控。</p>
<h2 id="技术点-2"><a href="#技术点-2" class="headerlink" title="技术点"></a>技术点</h2><p>本文所设计的监控系统以STM32F103C8T6单片机为主控芯片，利用BMP280温压传感器及ADXL335 三轴加速度传感器分别测得温度、气压以及XYZ三轴方向上的振动信号，实现了空压机工作数据的采集，并通过OLED显示屏显示，以便现场工作人员监测并及时检修；同时，用ESP8266 WiFi模块，将数据上传至阿里云，通过因特网传送至手机端及PC端，实现了运行状态的实时监测，为避免空压机状态异常进而运行失控造成严重后果，本设计增加了状态异常报警、停机功能，同时工作人员在手机app及PC端网页上可进行开、关机操作。本设计可基本实现对空气压缩机的实时状态监控功能。</p>
<h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><p><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project6.png" alt="图 6">  </p>
<h3 id="电路设计"><a href="#电路设计" class="headerlink" title="电路设计"></a>电路设计</h3><p><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project7.png" alt="图 7">  </p>
<h3 id="程序设计"><a href="#程序设计" class="headerlink" title="程序设计"></a>程序设计</h3><p><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project8.png" alt="图 8">  </p>
<h2 id="实物制作"><a href="#实物制作" class="headerlink" title="实物制作"></a>实物制作</h2><p><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project9.png" alt="图 9">  </p>
<h1 id="SE智厨：一种新型循环节能与安全防控的厨房智能仪器"><a href="#SE智厨：一种新型循环节能与安全防控的厨房智能仪器" class="headerlink" title="SE智厨：一种新型循环节能与安全防控的厨房智能仪器"></a>SE智厨：一种新型循环节能与安全防控的厨房智能仪器</h1><p>2019-2020<br>湖南科技大学第六届“互联网+”大学生创新创业大赛 铜奖</p>
<h2 id="项目简介-3"><a href="#项目简介-3" class="headerlink" title="项目简介"></a>项目简介</h2><p>一款循环节能与安全防控的厨房智能产品，本装置利用炒菜或用火过程中炉火与外界的温差以及用太阳能板来发电储能在蓄电池中，以达到节约能源的目的；以STC15F2K60S2为主控，利用烟雾传感器MQ-2和温度传感器DS18B20的配合使用，监测现场火灾、煤气泄漏、漏电触电等厨房事故，并且该设备能对事故进行自动化处理，避免危害安全。</p>
<h2 id="技术点-3"><a href="#技术点-3" class="headerlink" title="技术点"></a>技术点</h2><figure class="highlight arduino"><table><tr><td class="code"><pre><span class="line">安全监测：</span><br><span class="line">（<span class="number">1</span>）利用烟雾传感器MQ<span class="number">-2</span>和温度传感器DS18B20的配合使用,当达到预警设定时，立即进入报警处理，并通过电子阀门打开水闸开关，实现灭火处理；</span><br><span class="line">（<span class="number">2</span>）实时检测家中煤气是否泄漏，当出现煤气泄漏时，微处理器会立刻控制煤气电子阀门的关闭，通过<span class="built_in">GSM</span>无线通信模块向用户发送短信，传送当前煤气状态以及厨房是否发生火灾状态，预防事故发生。</span><br><span class="line">（<span class="number">3</span>）当厨房插座存在漏电或人触电时，能及时将电源切断。</span><br><span class="line">节能环保：</span><br><span class="line">（<span class="number">1</span>）系统能够将太阳能电压和温差发电输出的电压经过升压、降压、稳压电路稳定输出<span class="number">15</span>V，并通过充电电路存储在蓄电池中，给厨房智能控制系统和节能灯等工作，如果电压不足则自动切换到交流电供电。</span><br><span class="line">（<span class="number">2</span>）当用户使用炉火时，通过烟雾传感器检测油烟的浓度并实现排风扇的无级调速控制以达到节省用电效果；</span><br><span class="line">（<span class="number">3</span>）当厨房内温度较高时，系统根据DS18B20温度传感器检测的温度高低实现电风扇的无级调速，为厨房工作人员降温，减少能源的浪费；</span><br><span class="line">（<span class="number">4</span>）厨房内通过光敏传感器与红外传感器相结合检测厨房的光线强度和是否有人，当厨房较暗时，则自动调节LED照明亮度，以达到节约能源的目的，同时延长设备的使用寿命。</span><br></pre></td></tr></table></figure>
<h2 id="系统设计-1"><a href="#系统设计-1" class="headerlink" title="系统设计"></a>系统设计</h2><p>系统由K60主控模块、温差发电系统、太阳能发电系统、烟雾检测模块、温度检测模块、红外检测模块、电流检测模块、升压降压稳压模块、语音模块、驱动模块、电压检测模块、GSM模块组成。<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project10.png" alt="图 10">  </p>
<h3 id="电路设计-1"><a href="#电路设计-1" class="headerlink" title="电路设计"></a>电路设计</h3><p><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project11.png" alt="图 11">  </p>
<h3 id="实物制作-1"><a href="#实物制作-1" class="headerlink" title="实物制作"></a>实物制作</h3><p><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project12.png" alt="图 12">  </p>
<h1 id="地球仓"><a href="#地球仓" class="headerlink" title="地球仓"></a>地球仓</h1><p>2019-2020<br>实用新型专利：一种自适应旅居设备及控制系统</p>
<h2 id="项目简介-4"><a href="#项目简介-4" class="headerlink" title="项目简介"></a>项目简介</h2><p>地球仓，一种高度智能化的房子，灵活的用于景区休闲娱乐。本设计基于单片机控制地球仓的底盘控制系统，主要模拟地球仓遇到外界环节环境变化时地球仓底盘自动变化的工程。</p>
<h2 id="技术点-4"><a href="#技术点-4" class="headerlink" title="技术点"></a>技术点</h2><p>根据温度传感器和光敏传感器实时采集的数据，STC89C52 芯片根据温度传感器DS18B20和灵敏型光敏电阻传感器的信号来做出反应。当光线过于强烈，LED灯就会亮起并输入一个脉冲信号给STC89C52让步进电机转动带动底盘旋转 180 度；而温度达到一定值时，也会给STC89C52输入脉冲信号，使步进电机转动带动底盘旋转 90 度，同时数码管也会显示温度值。就是经过这样的底盘旋转控制，让地球仓适应更复杂的自然环境。</p>
<h3 id="电路设计-2"><a href="#电路设计-2" class="headerlink" title="电路设计"></a>电路设计</h3><p> <img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project13.png" alt="图 13">  </p>
<h3 id="实物制作-2"><a href="#实物制作-2" class="headerlink" title="实物制作"></a>实物制作</h3><p><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project14.png" alt="图 14">  </p>
<h1 id="风功率密度便携式测量仪"><a href="#风功率密度便携式测量仪" class="headerlink" title="风功率密度便携式测量仪"></a>风功率密度便携式测量仪</h1><p>2018-2019<br>第九届全国大学生电子商务“创新、创意及创业”挑战赛 校赛一等奖<br>第五届湖科大互联网+大赛 院赛金奖<br>“创青春”湖南科技大学大学生创业大赛 银奖</p>
<h2 id="项目简介-5"><a href="#项目简介-5" class="headerlink" title="项目简介"></a>项目简介</h2><p>本项目成果是一种新型风功率密度测量仪器，主要应用于风电场风功率密度测量，以实现风能电能的准确转化，提高电网运行的稳定性。传统风功率测量系统仍然停留在运用测风塔、雷达测出风速，然后将风速代入建模公式中计算得出结果，其过程繁琐，成本高昂。因此，团队成员在海内外知名教授的指导下设计出直接测量风功率的仪器，在将降低测量成本、提高预测准确度、增大使用便携度等方面具有独具一格的优势。</p>
<figure class="highlight ada"><table><tr><td class="code"><pre><span class="line">传统测风方法</span><br><span class="line">超声波测风：运用超声波测风无法启用风速，不受环境的影响，测量精度高、性能稳定，测量对象为风速、风向、温度，但无法直接计算出风功率，必须将风速带入一系列的数学建模公式中计算得出，过程复杂，耗时漫长。</span><br><span class="line">雷达测风：一次雷测风机具有足够大的发射频率，耗电量大，但是探测距离较近；距离远时回波信号弱；二次雷达具有探测精度高、采样速率快、使用方便等特点，但仍然无法直接测量出风功率。</span><br><span class="line">激光测风：激光测风虽然测量精度高、范围广，且测量过程不受天气的影响，但它目前技术仅仅停留在测量风速水平上，依旧无法打破新型技术壁垒<span class="comment">-----直接测量出瞬时风功率。</span></span><br></pre></td></tr></table></figure>
<h2 id="技术点-5"><a href="#技术点-5" class="headerlink" title="技术点"></a>技术点</h2><p>本实用新型专利是一种集风电场的风速、气压、温度和湿度的综合型风能功率密度便携式测试仪，用于风电场风电机组服役环境下风能功率密度的测量。该风能功率密度测试仪通过测量大气压中的压力、温度、湿度以及风速信号后，由51单片机计算出空气密度，再结合风速计算得到风能功率密度。</p>
<p>技术创新：</p>
<blockquote>
<p>①面向工程实际问题，直接输出风功率密度<br>②数据存储，有利于风电资源评估<br>③提出剔除风速瞬态扰动的有效功率密度处理算法，提高测量准确性<br>④操作简单、功耗较低，可随身携带使用或固定在风电场某处</p>
</blockquote>
<h2 id="信号采集与处理"><a href="#信号采集与处理" class="headerlink" title="信号采集与处理"></a>信号采集与处理</h2><h3 id="信号采集"><a href="#信号采集" class="headerlink" title="信号采集"></a>信号采集</h3><p>测试仪所需要采集的信号为风速信号、大气压力信号、温度信号以及湿度信号，分别利用NPN脉冲型RS485风速传感器(±（0.2+0.03v）m&#x2F;s)、BMP5大气压力传感器(±20Pa)、SHT21温湿度传感器(±0.3℃)(±2%RH)实现测量。<br>风速传感器采用输出为电压信号的机械式传感器，测量范围达到风电场风速变化的要求，需要信号调理电路输入到中央处理模块。大气压力、温湿度采用输出为数字量的模块化传感器对信号进行测量，可直接输入至单片机中进行处理。</p>
<h3 id="信号调理"><a href="#信号调理" class="headerlink" title="信号调理"></a>信号调理</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">①幅值调理电路：由于风速传感器输出的电压信号比较微弱，无法满足后续测试的工作要求，利用运算放大器以及电阻构成的放大网络对输出信号进行幅值调理，将其幅值放大以便满足模-数转换电路的要求。</span><br><span class="line">②信号滤波电路：由于环境以及人为影响，在输出的电压信号中，常伴有不必要的噪声信号。利用运算放大器和电阻等元件组成的有源滤波网络，对输出电压信号中的噪声信号进行剔除。</span><br><span class="line">③模-数转换电路：单片机所能处理的是数字量，而风速传感器输出为模拟电压量。因此，设置一个模-数转换电路，将对应的模拟量转换成数字量并输入至单片机中，对其进行处理及显示。</span><br></pre></td></tr></table></figure>
<h3 id="信号处理"><a href="#信号处理" class="headerlink" title="信号处理"></a>信号处理</h3><p><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project15.png" alt="图 15">  </p>
<h2 id="电路设计-3"><a href="#电路设计-3" class="headerlink" title="电路设计"></a>电路设计</h2><p><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project16.png" alt="图 16"><br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project17.png" alt="图 17">  </p>
<h2 id="实物制作-3"><a href="#实物制作-3" class="headerlink" title="实物制作"></a>实物制作</h2><p><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project18.png" alt="图 18"><br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Project19.png" alt="图 19">  </p>
]]></content>
      <tags>
        <tag>技术</tag>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title>TDA4①：SDK, TIDL, OpenVX</title>
    <url>/TDA4VM/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>TDA4的基本知识，包括数据手册研读，SDK介绍，TIDL概念。</p>
<span id="more"></span>

<p><a href="https://www.ti.com.cn/product/zh-cn/TDA4VM">TDA4VM官网</a>， <a href="https://e2e.ti.com/support/processors-group/processors/f/processors-forum">TI e2e论坛</a><br>下一篇：<a href="https://wangyujie.site/TDA4VM2/">TDA4：环境搭建、模型转换及Demo</a></p>
<h1 id="TDA4VM芯片数据手册研读"><a href="#TDA4VM芯片数据手册研读" class="headerlink" title="TDA4VM芯片数据手册研读"></a>TDA4VM芯片数据手册研读</h1><p><a href="https://www.ti.com.cn/cn/lit/ds/symlink/tda4vm.pdf">TDA4VM数据手册</a><br>适用于 ADAS 和自动驾驶汽车的TDA4VM Jacinto™ 处理器,具有深度学习、视觉功能和多媒体加速器的双核 Arm® Cortex®-A72 SoC 和 C7x DSP.<br>Jacinto 7系列架构芯片含两款汽车级芯片：TDA4VM 处理器和 DRA829V 处理器，前者应用于 ADAS，后者应用于网关系统，以及加速数据密集型任务的专用加速器，如计算机视觉和深度学习。二者都基于J721E平台开发。</p>
<h2 id="多核异构"><a href="#多核异构" class="headerlink" title="多核异构"></a>多核异构</h2><img alt="图 3" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/TDA4VMedit.jpg"/>  

<h2 id="处理器内核"><a href="#处理器内核" class="headerlink" title="处理器内核"></a>处理器内核</h2><ul>
<li><strong>C7x 浮点矢量 DSP，性能高达 1.0GHz、 80GFLOPS、256GOPS</strong>：C7x是TI的一款高性能数字信号处理器，其中的浮点矢量 DSP 可以进行高效的信号处理、滤波和计算，大幅提高神经网络模型的计算效率。<br><em>GHz-每秒钟执行10亿次计算，GFLOPS-每秒10亿次浮点运算，GOPS-每秒10亿次通用操作。</em></li>
<li><strong>深度学习矩阵乘法加速器 (MMA)，性能高达8TOPS (8b)（频率为1.0GHz）</strong>：可以高效地执行矩阵乘法和卷积等运算。<br><em>TOPS-每秒万亿次操作，8b-8位精度的运算。</em></li>
<li><strong>具有图像信号处理器(ISP)和多个视觉辅助加速器的视觉处理加速器（VPAC）</strong>：可以高效地执行图像处理、计算机视觉和感知任务。</li>
<li><strong>深度和运动处理加速器（DMPAC）</strong>：可以高效地执行深度计算和运动估计等任务。</li>
<li><strong>双核 64 位 Arm® Cortex®-A72 微处理器子系统，性能高达 2.0GHz</strong>：可以高效地执行复杂的应用程序。<ul>
<li>每个双核 Cortex®-A72 集群具有 1MB L2 共享缓存 </li>
<li>每个 Cortex®-A72 内核具有 32KB L1 数据缓存 和 48KB L1 指令缓存<br><em>L1缓存（一级缓存）：小而快，缓存CPU频繁使用的数据和指令，以提高内存访问速度；L2：大，帮助CPU更快地访问主内存中的数据。</em></li>
</ul>
</li>
<li><strong>六个 Arm® Cortex®-R5F MCU，性能高达 1.0GHz</strong>：一组小型、低功耗的微控制器单元，用于处理实时任务和控制应用程序<ul>
<li>16K 指令缓存，16K 数据缓存，64K L2 TCM（Tightly-Coupled Memory）</li>
<li>隔离 MCU 子系统中有两个 Arm® Cortex®-R5F MCU</li>
<li>通用计算分区中有四个 Arm® Cortex®-R5F MCU</li>
</ul>
</li>
<li><strong>两个 C66x 浮点 DSP，性能高达 1.35GHz、 40GFLOPS、160GOPS</strong>：另一款高性能数字信号处理器，可以高效地执行信号处理、滤波和计算任务。</li>
<li><strong>3D GPU PowerVR® Rogue 8XE GE8430，性能高达 750MHz、96GFLOPS、6Gpix&#x2F;s</strong>：专门用于图形处理的硬件单元，可以实现高效的图形渲染和计算。<br><em>Gpix&#x2F;s-每秒可以处理10亿像素数</em></li>
<li><strong>定制设计的互联结构，支持接近于最高的处理能力</strong>：处理器内部的互连结构，用于连接各种硬件单元，并支持高效的数据传输和协议。</li>
</ul>
<h1 id="SDK"><a href="#SDK" class="headerlink" title="SDK"></a>SDK</h1><p>Download：<a href="https://www.ti.com.cn/tool/cn/PROCESSOR-SDK-J721E">PROCESSOR-SDK-J721E</a>，提供<strong>两套SDK</strong>（软件架构不同）：</p>
<ol>
<li>PSDK <a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/08_06_00_12/exports/docs/psdk_rtos/docs/user_guide/index.html">RTOS</a> and <a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-linux-rt-jacinto7/08_06_00_11/exports/docs/devices/J7/linux/index.html">Linux</a>，用于J721E-EVM</li>
<li>PSDK Linux for <a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-linux-sk-tda4vm/latest/exports/docs/index.html">Edge AI</a>，用于TDA4VM-SK</li>
</ol>
<blockquote>
<p><em>RTOS and Linux SDK work together as a multi-processor software development kit，用于ADAS领域，更多的外设和驱动放在RTOS端，便于实时处理 ，自定义性更强，开发难度更大<br>Edge AI SDK主要基于Linux开发，用于工业领域，工作量少但实时性差，无法发挥芯片全部性能</em> <sup id="fnref:0"><a href="#fn:0" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[视频：深度学习算法在ADAS处理器TDA4VM的应用与部署](https://www.ti.com.cn/zh-cn/video/6301563648001)">[0]</span></a></sup></p>
</blockquote>
<h2 id="Processor-SDK-RTOS-PSDK-RTOS"><a href="#Processor-SDK-RTOS-PSDK-RTOS" class="headerlink" title="Processor SDK RTOS (PSDK RTOS)"></a>Processor SDK RTOS (PSDK RTOS)</h2><p><strong>PSDK RTOS Block Diagram</strong><br><img alt="图 4" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/TDA4VMSDKedit.png" /><br><strong>Hardware</strong><br>Evaluation Module (EVM):Ti 推出的硬件开发板。用于快速原型设计和新产品开发，可以帮助开发人员在短时间内实现复杂的嵌入式系统功能, <a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/08_06_00_12/exports/docs/psdk_rtos/docs/user_guide/evm_setup_j721e.html">EVM Setup for J721E</a><br>JTAG:debug execution, load program via JTAG-<em>No Boot Mode</em><br>uart:prints status of the application via the uart terminal.<br><strong>Software</strong><br>Recommend IDE:Code Composer Studio (CCS), <a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/08_06_00_12/exports/docs/psdk_rtos/docs/user_guide/ccs_setup_j721e.html#ccs-setup-j721e">CCS Setup for J721E</a><br><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/08_06_00_12/exports/docs/psdk_rtos/docs/user_guide/getting_started_j721e.html#demo-applications"><strong>Demos</strong></a><br>Prebuilt Demos:直装<br>Build Demos from Source: Linux, Windows(很少)</p>
<p><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/08_06_00_12/exports/docs/psdk_rtos/docs/user_guide/sdk_components_j721e.html#vxlib"><strong>SDK Components</strong></a><br>The following table lists <em>part</em> of the top-level folders in the SDK package and the component it represents.</p>
<table>
<thead>
<tr>
<th>Folder</th>
<th>Component</th>
<th>User guide</th>
</tr>
</thead>
<tbody><tr>
<td>vision_apps</td>
<td>Vision Apps</td>
<td><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/08_06_00_12/exports/docs/vision_apps/docs/user_guide/index.html">Demos</a></td>
</tr>
<tr>
<td>pdk_jacinto_*</td>
<td>Platform Development Kit</td>
<td><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/08_06_00_12/exports/docs/pdk_jacinto_08_06_00_31/docs/pdk_introduction.html#Documentation">PDK</a></td>
</tr>
<tr>
<td><del>mcusw</del></td>
<td>MCU Software</td>
<td><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/08_06_00_12/exports/docs/mcusw/mcal_drv/docs/drv_docs/index.html">MCU SW</a></td>
</tr>
<tr>
<td>tidl_j7_*</td>
<td>TI Deep learning Product</td>
<td><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/08_06_00_12/exports/docs/tidl_j721e_08_06_00_10/ti_dl/docs/user_guide_html/index.html">TIDL Product</a></td>
</tr>
<tr>
<td>tiovx</td>
<td>TI OpenVX</td>
<td><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/08_06_00_12/exports/docs/tiovx/docs/user_guide/index.html">TIOVX</a></td>
</tr>
<tr>
<td>tiadalg</td>
<td>TI Autonomous Driving Algorithms</td>
<td><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/08_06_00_12/exports/docs/tiadalg/TIAutonomousDrivingAlgorithmLibrary_ReleaseNotes.html#Documentation">TIADALG</a></td>
</tr>
</tbody></table>
<p>RTOS SDK 中集成了众多的Demo展示TIDL在TDA4处理器上对实时的语义分割和 SSD 目标检测的能力。如下图,	Vision Apps User Guide 中 <a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/07_02_00_06/exports/docs/vision_apps/docs/user_guide/group_apps_dl_demos_app_tidl_avp3.html">AVP Demo</a> 的展示了使用TIDL对泊车点、车辆的检测。<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Deep Learning with Jacinto™ 7 SoCs: TDA4x](https://www.ti.com.cn/cn/lit/ml/slyp667/slyp667.pdf?raw=true) | [当深度学习遇上TDA4](https://e2echina.ti.com/blogs_/b/behindthewheel/posts/tda4)">[1]</span></a></sup></p>
<img alt="图 7" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/TDA4VMAVP.jpg" />  

<h2 id="Processor-SDK-Linux"><a href="#Processor-SDK-Linux" class="headerlink" title="Processor SDK Linux"></a>Processor SDK Linux</h2><details>
  <summary>SDK Components</summary>

<table>
<thead>
<tr>
<th>Folder</th>
<th>Component</th>
</tr>
</thead>
<tbody><tr>
<td>bin</td>
<td>包含用于配置主机系统的帮助程序脚本和目标设备。这些脚本中的大多数都由setup.sh使用脚本。</td>
</tr>
<tr>
<td>board-support</td>
<td>主要包含linux内核源码，uboot源码，及其他组件。</td>
</tr>
<tr>
<td>configs</td>
<td>yocto工具的构建链接（yocto构建大约需要十几个小时，一般情况下不会去编译yocto。）。</td>
</tr>
<tr>
<td>docs</td>
<td>直接打开index.html，即可阅读整个SDK的官方文档。</td>
</tr>
<tr>
<td>example-applications</td>
<td>包含一些benchmarks等app demo。</td>
</tr>
<tr>
<td>filesystem</td>
<td>存放默认、最小的文件系统。</td>
</tr>
<tr>
<td>linux-devkit</td>
<td>交叉编译工具链和库以加快目标设备的开发速度。</td>
</tr>
<tr>
<td>Makefile</td>
<td>顶级编译脚本（make）。</td>
</tr>
<tr>
<td>patches</td>
<td>补丁、预留目录。</td>
</tr>
<tr>
<td>Rules.make</td>
<td>设置顶级生成文件使用的默认值以及子组件生成文件。</td>
</tr>
<tr>
<td>setup .sh</td>
<td>配置用户主机系统和目标开发系统。</td>
</tr>
<tr>
<td>yocto-build</td>
<td>此目录允许重建SDK组件和使用Yocto Bitbake的文件系统。</td>
</tr>
</tbody></table>
</details>

<p>Linux SDK最主要是用于A72核心上的启动引导、操作系统、文件系统，一般只有在修改到这部分的时候才会使用到Linux SDK。</p>
<h2 id="PSDK-Linux-for-Edge-AI"><a href="#PSDK-Linux-for-Edge-AI" class="headerlink" title="PSDK Linux for Edge AI"></a>PSDK Linux for Edge AI</h2><p>对于Edge AI，无需对深度学习算法进行深入了解，使用python或C++即可进行部署，不支持的算法可以放在ARM端计算和实施推理，TI会自动生成推理文件，如下图；<br><img alt="图 sdk" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/TDA4VMsdk.png" />  </p>
<p>而对于ADAS领域，要把深度学习算法都放在TIDL端，最大化利用算力，需要手写加速算子进行自定义层的设计；</p>
<p>两套SDK部署深度学习算法的区别如下：<br><img alt="图 compare" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/TDA4VMcompare.png" /><br><img alt="图 compare" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/TDA4VMcompare2.png" /> </p>
<h1 id="TDA4VM-SK开发板"><a href="#TDA4VM-SK开发板" class="headerlink" title="TDA4VM-SK开发板"></a>TDA4VM-SK开发板</h1><p>TDA4VM processor starter kit for edge AI vision systems</p>
<img alt="图 TDA4VM-SK" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/TDA4VM-SK.png" /> 

<p><a href="https://www.ti.com/tool/SK-TDA4VM">SK-TDA4VM Evaluation board | TI.com</a><br><a href="https://www.ti.com.cn/cn/lit/ug/zhcu912c/zhcu912c.pdf?ts=1688968746311">SK-TDA4VM 用户指南</a>, 提供了 SK-TDA4VM 的功能和接口详细信息<br><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-linux-sk-tda4vm/latest/exports/docs/running_simple_demos.html">Processor SDK Linux for Edge AI Documentation</a><br><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-linux-edgeai/TDA4VM/08_06_01/exports/docs/common/inference_models.html">Processor SDK Linux for SK-TDA4VM Documentation</a></p>
<p>TI 的 TDA4VM SoC 包含双核 A72、高性能视觉加速器、视频编解码器加速器、最新的 C71x 和 C66x DSP、 用于捕获和显示的高带宽实时 IP、GPU、专用安全岛和安全加速器。 SoC 经过功率优化，可为机器人、工业 和汽车应用中的感知、传感器融合、定位和路径规划任务提供一流的性能。</p>
<p>TDA4VM Edge AI Starter Kit (SK) 是一款低成本、小尺寸板，功耗大约20W，能提供8TOPS深度学习算力，支持Tensorflow Lite,ONNX,TVM,GStreamer接口</p>
<p><strong>Features</strong></p>
<ul>
<li><strong>性能</strong> - TDA4VM处理器提供8 TOPS的深度学习性能，并以低功耗实现硬件加速的边缘人工智能。</li>
<li><strong>摄像头接口</strong> - 两个与树莓派兼容的CSI-2端口，以及一个高速40针Semtec相机连接器，可连接最多八个相机（需要TIDA-01413传感器融合附加卡）。</li>
<li><strong>连接性</strong> - 三个USB 3.0 Type A端口，一个USB 3.0 Type C端口，一个以太网口，一个M.2 Key E连接器和一个M.2 Key M连接器，四个CAN-FD接口，通过一个USB桥接器支持四个UART终端。</li>
<li><strong>内存</strong> - DRAM，LPDDR4-4266，总计4GB内存，支持行内ECC(Error Checking and Correcting”)。</li>
<li><strong>显示</strong> - DisplayPort支持最高4K分辨率和MST功能，以及1080p HDMI。</li>
</ul>
<h1 id="TIDL"><a href="#TIDL" class="headerlink" title="TIDL"></a>TIDL</h1><p><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/06_01_01_12/exports/docs/tidl_j7_01_00_01_00/ti_dl/docs/user_guide_html/index.html">TIDL</a>（TI Deep Learning Library）是TI平台基于深度学习算法的<em>软件生态系统</em>，其特性和支持<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Embedded low-power deep learning with TIDL](https://www.ti.com.cn/cn/lit/wp/spry314/spry314.pdf?raw=true)">[2]</span></a></sup>可以将一些常见的深度学习算法模型快速的部署到TI嵌入式平台。</p>
<figure class="highlight mathematica"><table><tr><td class="code"><pre><span class="line"><span class="variable">Features</span><span class="operator">:</span> <span class="variable">Interoperability</span><span class="operator">,</span> <span class="variable">High</span> <span class="variable">Compute</span><span class="operator">,</span> <span class="variable">High</span> <span class="variable">Memory</span> <span class="variable">Bandwidth</span><span class="operator">,</span> <span class="variable">Scalability</span></span><br><span class="line"><span class="variable">Popular</span> <span class="variable">operators</span> <span class="variable">supported</span><span class="operator">:</span> <span class="variable">Convolution</span><span class="operator">,</span> <span class="variable">Pooling</span><span class="operator">,</span> <span class="built_in">Element</span> <span class="variable">Wise</span><span class="operator">,</span> <span class="built_in">Inner</span><span class="operator">-</span><span class="built_in">Product</span><span class="operator">,</span> <span class="variable">Soft</span><span class="operator">-</span><span class="built_in">Max</span><span class="operator">,</span> <span class="variable">Bias</span> <span class="variable">Add</span><span class="operator">,</span> <span class="variable">Concatenate</span><span class="operator">,</span> <span class="built_in">Scale</span><span class="operator">,</span> <span class="variable">Batch</span> <span class="variable">Normalization</span><span class="operator">,</span> <span class="built_in">Re</span><span class="operator">-</span><span class="variable">size</span><span class="operator">,</span> <span class="built_in">Arg</span><span class="operator">-</span><span class="variable">max</span><span class="operator">,</span> <span class="variable">Slice</span><span class="operator">,</span> <span class="variable">Crop</span><span class="operator">,</span> <span class="built_in">Flatten</span><span class="operator">,</span> <span class="variable">Shuffle</span> <span class="variable">Channel</span><span class="operator">,</span> <span class="variable">Detection</span> <span class="variable">output</span><span class="operator">,</span> <span class="variable">Deconvolution</span><span class="operator">/</span><span class="built_in">Transpose</span> <span class="variable">convolution</span> </span><br></pre></td></tr></table></figure>
<p>Functions: </p>
<ul>
<li><strong>Import</strong> trained network models into <em>.bin</em> files that can be used by TIDL. The following model formats are currently supported:<ul>
<li>Caffe 模型（使用 .caffemodel 和 .prototxt 文件） - 0.17 (caffe-jacinto in gitHub)</li>
<li>Tensorflow 模型（使用 .pb 或 .tflite 文件） - 1.12（TFLite - Tensorflow 2.0-Alpha）</li>
<li><em>ONNX</em> 模型（使用 .onnx 文件 和 .prototxt 文件） - 1.3.0 （官方onnx已经到了1.14）</li>
</ul>
</li>
<li>Run <strong>performance simulation tool</strong> on PC to estimate the expected performace of the network while executing the network for inference on TI Jacinto7 SoC</li>
<li><strong>Execute the network on PC</strong> using the imported files and validate the results.bin</li>
<li><strong>Execute the network on TI</strong> Jacinto7 SoC using the imported files and validate the results.bin</li>
</ul>
<img alt="图 14" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/TDA4VMTIDLppt.jpg" width="80%"/>  

<p>TIDL当前支持的训练框架有Tensorflow、Pytorch、Caffe等，用户可以根据需要选择合适的训练框架进行模型训练。TIDL可以将PC端训练好的模型导入编译生成TIDL可以识别的模型格式，同时在导入编译过程中进行层级合并以及量化等操作，方便导入编译后的模型高效的运行在具有高性能定点数据感知能力TDA4硬件加速器上。</p>
<h2 id="TIDL-Importer"><a href="#TIDL-Importer" class="headerlink" title="TIDL Importer"></a>TIDL Importer</h2><p>RTOS SDK 中的 ti_dl 提供了 <a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/06_01_01_12/exports/docs/tidl_j7_01_00_01_00/ti_dl/docs/user_guide_html/md_tidl_model_import.html">TIDL Importer</a> 模型导入工具，模型可视化工具等，非常便捷地可以对训练好的模型进行导入。</p>
<img alt="图 3" data-src="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/06_01_01_12/exports/docs/tidl_j7_01_00_01_00/ti_dl/docs/user_guide_html/TIDL_blockDiagram.png" />  
<img alt="图 4" data-src="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/06_01_01_12/exports/docs/tidl_j7_01_00_01_00/ti_dl/docs/user_guide_html/tidl_import_design.jpg" />  

<p><code>RTOSsdk/tidl_j721e/ti_dl/utils/tidlModelImport</code></p>
<ol>
<li>读取导入配置文件；</li>
<li>转换并<strong>导入</strong>网络层和算子（operators）到TIDL net file，计算层大小和缓冲区大小，并尽可能合并层；</li>
<li>生成<strong>量化</strong>配置文件，调用量化工具（quant tool）进行范围采集，并更新TIDL net file；</li>
<li>生成用于网络<strong>编译</strong>器（network compiler）的配置文件，并调用编译器进行性能优化；</li>
<li><em>[Optional]</em> 调用GraphVisualiser来生成网络图；</li>
<li>导入工具将在最后结束检查模型；</li>
<li>最后，如果没有错误，可以用于<strong>部署</strong>。</li>
</ol>
<p>总的来说，导入工具将在内部运行quantization, network compilation, performance simulation internally, 并生成文件：</p>
<blockquote>
<p>Compiled network and I&#x2F;O files used for inference<br>Performance simulation results for network analysis in .csv</p>
</blockquote>
<h2 id="TIDL-Quantization-量化方法"><a href="#TIDL-Quantization-量化方法" class="headerlink" title="TIDL Quantization 量化方法"></a><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/06_01_01_12/exports/docs/tidl_j7_01_00_01_00/ti_dl/docs/user_guide_html/md_tidl_fsg_quantization.html">TIDL Quantization</a> 量化方法</h2><p><a href="https://github.com/TexasInstruments/edgeai-tidl-tools/blob/master/docs/tidl_fsg_quantization.md">Tidl tools quantization</a>，<a href="https://github.com/TexasInstruments/edgeai-torchvision/blob/master/docs/pixel2pixel/Quantization.md">torchvision-Quantization</a><br>把浮点计算（Float）转换成定点（Int）计算，是一种基于计算机存储和计算过程特点，提升（端侧）模型推理速度，并维持稳定精度的模型压缩方法。</p>
<ul>
<li>浮点计算在成本和功耗效率方面不高。这些浮点计算可以用定点计算(8 or 16 bit)来代替，同时不会丢失推理精度。</li>
<li>J7平台的矩阵乘法加速器(MMA)支持深度学习模型的8位、16位和32位推理。</li>
<li>当进行64x64矩阵乘法时，8位推理支持4096 MACs(<em>Multiply–Accumulate Operations</em>) per cycle的乘法器吞吐量。因此，8位推理适用于J7平台。 (16位和32位推理会显著消耗性能。 16位推理的乘法器吞吐量为每个周期1024个MAC。 所需的内存I&#x2F;O会很高。)</li>
</ul>
<p>TIDL中需要量化的层：Convolution Layer、De-convolution Layer、Inner-Product Layer、Batch Normalization (Scale&#x2F;Mul, Bias&#x2F;Add, PReLU)</p>
<ul>
<li>Quantization options：<ul>
<li>Post Training Quantization (PTQ, 训练后量化、离线量化)</li>
<li>Training for Quantization (QAT，训练时量化，伪量化，在线量化)</li>
<li>Quantization aware Training<img data-src="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/06_01_01_12/exports/docs/tidl_j7_01_00_01_00/ti_dl/docs/user_guide_html/TIDL_Quant_Options.png" width='70%'></li>
</ul>
</li>
</ul>
<p><a href="https://zhuanlan.zhihu.com/p/639245713">从零开始玩转TDA4之模型量化</a></p>
<h2 id="TI’s-Edge-AI"><a href="#TI’s-Edge-AI" class="headerlink" title="TI’s Edge AI"></a>TI’s Edge AI</h2><p>TIDL is a fundamental software component of <a href="https://www.ti.com/edgeai">TI’s Edge AI solution</a>.在TIDL上，深度学习网络应用开发主要分为三个大的步骤: </p>
<ol>
<li>基于Tensorflow、Pytorch、Caffe 等训练框架，训练模型</li>
<li>基于TDA4VM处理器导入模型： 训练好的模型，需要使用TIDL Importer工具导入成可在TIDL上运行的模型。导入的主要目的是对输入的模型进行量化、优化并保存为TIDL能够识别的网络模型和网络参数文件</li>
<li>基于TI Jacinto7TM SDK 验证模型，并在应用里面部署模型：<ul>
<li>PC 上验证并部署<ul>
<li>在PC上使用TIDL推理引擎进行模型测试。</li>
<li>在PC上使用OpenVX框架开发程序，在应用上进行验证。</li>
</ul>
</li>
<li>EVM上验证并部署<ul>
<li>在EVM上使用TIDL推理引擎进行模型测试。</li>
<li>在EVM上使用OpenVX框架开发程序，在应用上进行验证<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[基于Pytorch训练并在TDA4上部署ONNX模型](https://www.ti.com/cn/lit/an/zhcabs1/zhcabs1.pdf?raw=true)">[3]</span></a></sup></li>
</ul>
</li>
</ul>
</li>
</ol>
<img data-src="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/08_06_00_12/exports/docs/tidl_j721e_08_06_00_10/ti_dl/docs/user_guide_html/dnn-workflow.png">

<p><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/08_06_00_12/exports/docs/tidl_j721e_08_06_00_10/ti_dl/docs/user_guide_html/md_tidl_overview.html">TIDL Runtime</a>（TIDL-RT）是运行在TDA4端的实时推理单元，同时提供了TIDL的运行环境，对于input tensor，TIDL TIOVX Node 调用TIDL 的深度学习加速库进行感知，并将结果进行输出。<br><img alt="图 9" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/TDA4VMworkflow.png" width="60%"/>  </p>
<p><a href="https://github.com/TexasInstruments/edgeai"><strong>TI’s EdgeAI Tools</strong></a>:Training and quantization tools,make DNNs more suitable for TI devices.</p>
<ul>
<li><p><a href="https://github.com/TexasInstruments/edgeai-modelzoo">Model ZOO</a>:A large collection of pre-trained models for data scientists,其中有<a href="https://github.com/TexasInstruments/edgeai-modelzoo/tree/master/models/vision/detection">YOLO例程</a></p>
</li>
<li><p><a href="https://github.com/TexasInstruments/edgeai-tidl-tools#edgeai-tidl-tools">Edge AI TIDL Tools</a>:used for model compilation on X86. Artifacts from compilation process can used for Model inference， which can happen on X86 machine or on development board with TI SOC.</p>
</li>
<li><p><a href="https://github.com/TexasInstruments/edgeai-benchmark">Edge AI Benchmark</a>:provides higher level scripts for model compilation,and perform accuracy and performance benchmark.</p>
</li>
<li><p><a href="https://dev.ti.com/edgeai/">Edge AI Studio</a>:Integrated development environment for development of AI applications for edge processors.（需授权）</p>
</li>
<li><p><a href="https://github.com/TexasInstruments/edgeai-modelmaker">EdgeAI-ModelMaker</a>: Command line Integrated environment for training &amp; compilation，集成了edgeai-modelzoo, edgeai-torchvision, edgeai-mmdetection, edgeai-benchmark, edgeai-modelmaker</p>
<img alt="图 9" data-src="https://raw.gitmirror.com/TexasInstruments/edgeai/blob/master/assets/workblocks_tools_software.png?raw=true" width="90%"></li>
</ul>
<h2 id="OpenVX"><a href="#OpenVX" class="headerlink" title="OpenVX"></a>OpenVX</h2><p><a href="https://www.khronos.org/openvx/">OpenVX</a> 视觉加速中间件是芯片内部的硬件加速器与视觉应用间的桥梁(中间件:用于简化编程人员开发复杂度、抽象软硬件平台差异的软件抽象层)，是个由Khronos定义的API框架，包括：宏的定义与含义，结构体的定义与含义，函数的定义与行为。</p>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p><code>Vx_context</code>：一个context就是一个运行环境，包含多种不同的功能，在不同场景下被调度。一般很少有使用到多context的场景；<br><code>Vx_graph</code>：一个graph就是一个功能，是由多个步骤连接在一起的完整功能；<br><code>Vx_node</code>：一个node就是一个最小的调度单元，可以是图像预处理算法，可以是边缘检测算法。<br>每个进程内可以有多个context（上下文），每个context内可以有多个graph（图，或连接关系），每个graph内可以有多个node（节点）。<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[TIOVX – TI’s OpenVX Implementation](https://www.ti.com/content/dam/videos/external-videos/2/3816841626001/5624955361001.mp4/subassets/openvx-implementation-on-ti-tda-adas-socs-presentation.pdf)">[4]</span></a></sup></p>
<img alt="图 4" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/TDA4VMopenvxflow.png" />  

<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//Example Program</span></span><br><span class="line">vx_context context = vxCreateContext(); <span class="comment">//创建 OpenVX 上下文,即整个应用程序的运行环境</span></span><br><span class="line">vx_graph graph = vxCreateGraph( context ); <span class="comment">//在上下文中创建一个图，表示图像处理的流程</span></span><br><span class="line">vx_image input = vxCreateImage( context, <span class="number">640</span>, <span class="number">480</span>, VX_DF_IMAGE_U8 );</span><br><span class="line">vx_image output = vxCreateImage( context, <span class="number">640</span>, <span class="number">480</span>, VX_DF_IMAGE_U8 ); <span class="comment">//创建输入和输出图像，分别用于存储输入和处理后的图像。</span></span><br><span class="line">vx_image intermediate = vxCreateVirtualImage( graph, <span class="number">640</span>, <span class="number">480</span>, VX_DF_IMAGE_U8 ); <span class="comment">//创建虚拟图像，用于存储第一次处理后的结果，供下一步处理使用</span></span><br><span class="line">vx_node F1 = vxF1Node( graph, input, intermediate );</span><br><span class="line">vx_node F2 = vxF2Node( graph, intermediate, output ); <span class="comment">//创建两个节点，分别表示两个不同的图像处理操作，并将它们添加到图中</span></span><br><span class="line">vxVerifyGraph( graph ); <span class="comment">//验证图的正确性</span></span><br><span class="line">vxProcessGraph( graph ); <span class="comment">//执行图像处理</span></span><br></pre></td></tr></table></figure>
<img alt="图 5" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/TDA4VMopenvxexample.png" width="80%"/>  

<h3 id="基本数据结构"><a href="#基本数据结构" class="headerlink" title="基本数据结构"></a>基本数据结构</h3><p><code>Vx_image, Vx_tensor, Vx_matrix, Vx_array, Vx_user_object_data</code><br>OpenVX规范了标准化的数据结构，基本满足了嵌入式系统的主要需求，尤其是这种数据结构的描述方法对嵌入式系统非常友好：支持虚拟地址、物理地址等异构内存；提供了数据在多种地址之间映射的接口；提供了统一化的自定义结构体的描述方法。</p>
<h3 id="TIOVX"><a href="#TIOVX" class="headerlink" title="TIOVX"></a>TIOVX</h3><p><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/08_06_00_12/exports/docs/tiovx/docs/user_guide/index.html">TIOVX</a> 是TI公司对OpenVX的实现。<br><img data-src="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/08_06_00_12/exports/docs/tiovx/docs/user_guide/tiovx_block_diagram_j7.png" width="80%"></p>
<p>TIOVX Platform提供了特定硬件(如TDAx, AM65x)的操作系统(如TI-RTOS, Linux)调用API。TIOVX Framework包含了官方OpenVX的标准API和TI扩展的API，其中包括</p>
<figure class="highlight mathematica"><table><tr><td class="code"><pre><span class="line"><span class="variable">public</span><span class="operator">:</span> <span class="built_in">Context</span><span class="operator">,</span> <span class="built_in">Parameter</span><span class="operator">,</span> <span class="variable">Kernel</span><span class="operator">,</span> <span class="variable">Node</span><span class="operator">,</span> <span class="built_in">Graph</span> <span class="built_in">Array</span><span class="operator">,</span> <span class="built_in">Image</span><span class="operator">,</span> <span class="variable">Scalar</span><span class="operator">,</span> <span class="built_in">Pyramid</span><span class="operator">,</span> <span class="variable">ObjectArray</span> ；</span><br><span class="line"><span class="variable">TI</span><span class="operator">:</span> <span class="variable">Target</span><span class="operator">,</span> <span class="variable">Target</span> <span class="variable">Kernel</span><span class="operator">,</span> <span class="variable">Obj</span> <span class="variable">Desc</span>。</span><br></pre></td></tr></table></figure>

<p><strong>优势</strong></p>
<ul>
<li>TI官方提供OpenVX的支持，提供标准算法的硬件加速实现，提供各个功能的Demo，能够简化开发调试工作。</li>
<li>简化多核异构的开发，可以在X86模拟运行，所有的板级开发和调试都位于A72 Linux端，减少了对RTOS调试的工作量。</li>
<li>OpenVX提供了数据流调度机制，能够支持流水线运行，简化了多线程和并行调度的工作。结合RTOS的实时特性，减少Linux非实时操作系统带来的负面影响<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[OpenVX视觉加速中间件与TDA4VM平台上的应用](https://zhuanlan.zhihu.com/p/423179832) | [TDA4横扫行泊一体市场与其背后的OpenVX](https://zhuanlan.zhihu.com/p/606584605)">[5]</span></a></sup><img alt="图 6" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/TDA4VMtiovx.png" width="80%"/></li>
</ul>
<p><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/08_06_00_12/exports/docs/tiovx/docs/user_guide/PYTIOVX.html">PyTIOVX</a>: Automated OpenVX “C” Code Generation</p>
<hr>
<blockquote>
<p>TDA4系列文章：<br><a href="https://wangyujie.site/TDA4VM/">TDA4①：SDK, TIDL, OpenVX</a><br><a href="https://wangyujie.site/TDA4VM2/">TDA4②：环境搭建、模型转换、Demo及Tools</a><br><a href="https://wangyujie.site/TDA4VM3/">TDA4③：YOLOX的模型转换与SK板端运行</a><br><a href="https://wangyujie.site/TDA4VM4/">TDA4④：部署自定义模型</a><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:0"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">0.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.ti.com.cn/zh-cn/video/6301563648001">视频：深度学习算法在ADAS处理器TDA4VM的应用与部署</a><a href="#fnref:0" rev="footnote"> ↩</a></span></li><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.ti.com.cn/cn/lit/ml/slyp667/slyp667.pdf?raw=true">Deep Learning with Jacinto™ 7 SoCs: TDA4x</a> | <a href="https://e2echina.ti.com/blogs_/b/behindthewheel/posts/tda4">当深度学习遇上TDA4</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.ti.com.cn/cn/lit/wp/spry314/spry314.pdf?raw=true">Embedded low-power deep learning with TIDL</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.ti.com/cn/lit/an/zhcabs1/zhcabs1.pdf?raw=true">基于Pytorch训练并在TDA4上部署ONNX模型</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.ti.com/content/dam/videos/external-videos/2/3816841626001/5624955361001.mp4/subassets/openvx-implementation-on-ti-tda-adas-socs-presentation.pdf">TIOVX – TI’s OpenVX Implementation</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://zhuanlan.zhihu.com/p/423179832">OpenVX视觉加速中间件与TDA4VM平台上的应用</a> | <a href="https://zhuanlan.zhihu.com/p/606584605">TDA4横扫行泊一体市场与其背后的OpenVX</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li></ol></div></div></p>
</blockquote>
]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>嵌入式</tag>
      </tags>
  </entry>
  <entry>
    <title>TDA4③：YOLOX的模型转换与SK板端运行</title>
    <url>/TDA4VM3/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>以目标检测算法YOLOX为例，记录模型从权重文件转换为ONNX，再使用TIDL(Importer&#x2F;Tolls)编译为可执行文件，最后于SK板运行及评估的开发流程。</p>
<span id="more"></span>

<p>接上一篇：<a href="https://wangyujie.site/TDA4VM2/">TDA4②：环境搭建、模型转换、Demo及Tools</a></p>
<h1 id="YOLOX部署TDA4VM-SK流程"><a href="#YOLOX部署TDA4VM-SK流程" class="headerlink" title="YOLOX部署TDA4VM-SK流程"></a>YOLOX部署TDA4VM-SK流程</h1><p>TI官方在<a href="https://github.com/TexasInstruments/edgeai-modelzoo"> ModelZOO </a>中提供了一系列预训练模型可以直接拿来转换，也提供了<a href="https://github.com/TexasInstruments/edgeai-yolov5"> edgeai-YOLOv5 </a>与<a href="https://github.com/TexasInstruments/edgeai-yolox"> edgeai-YOLOX </a>等优化的开源项目，可以直接下载提供的YOLOX_s的<a href="http://software-dl.ti.com/jacinto7/esd/modelzoo/latest/models/vision/detection/coco/edgeai-yolox/yolox-s-ti-lite_39p1_57p9.onnx"> onnx文件 </a>和<a href="http://software-dl.ti.com/jacinto7/esd/modelzoo/latest/models/vision/detection/coco/edgeai-yolox/yolox_s_ti_lite_metaarch.prototxt"> prototxt文件 </a>，也可以在官方项目上训练自己的模型后再导入。</p>
<p>这里尝试跑通全流程，在 edgeai-YOLOX 项目中训练，得到 <code>.pth</code> 权重文件，使用 export_onnx.py 文件转换为 <code>.onnx</code> 模型文件和 <code>.prototxt</code> 架构配置文件，并导入TIDL，得到部署用的 <code>.bin</code> 文件。<br>主要参考<a href="https://github.com/TexasInstruments/edgeai-yolox/blob/main/README_2d_od.md"> edgeai-YOLOX文档 </a>以及<a href="https://blog.csdn.net/AIRKernel/article/details/126222505"> YOLOX模型训练结果导入及平台移植应用 </a></p>
<img alt="picture 1" data-src="https://github.com/TexasInstruments/edgeai-yolox/raw/main/yolox/utils/figures/Focus.png"/>  

<h2 id="1-使用edgeai-yolox训练自制数据集"><a href="#1-使用edgeai-yolox训练自制数据集" class="headerlink" title="1. 使用edgeai-yolox训练自制数据集"></a>1. 使用edgeai-yolox训练自制数据集</h2><p>目标检测文档：<a href="https://github.com/TexasInstruments/edgeai-yolox/blob/main/README_2d_od.md">edgeai-yolox-2d_od</a></p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">conda create -n pytorch python=3.6</span><br><span class="line">./setup.sh  <span class="comment">#若pytorch环境已建好，就不用全部跑通，后面运行时一个个装</span></span><br><span class="line"><span class="comment">#运行demo，pth在文档中下载</span></span><br><span class="line">python tools/demo.py image -f exps/default/yolox_s_ti_lite.py -c yolox-s-ti.pth --path assets/dog.jpg --conf 0.25 --nms 0.45 --tsize 640 --save_result --device gpu --dataset coco</span><br><span class="line"><span class="comment">#报错，注释掉135行self.cad_models = model.head.cad_models，成功</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#自建数据集，COCO格式，放在datasets文件夹</span></span><br><span class="line">    COCO </span><br><span class="line">    ├── train2017   <span class="comment">#训练jpg图片</span></span><br><span class="line">    ├── val2017     <span class="comment">#验证jpg图片</span></span><br><span class="line">    └── annotations <span class="comment">#标签json文件</span></span><br><span class="line">        ├── instances_train2017.json</span><br><span class="line">        └── instances_val2017.json</span><br><span class="line"></span><br><span class="line">yolox/data/datasets/coco_classes.py <span class="comment">#修改类别名称</span></span><br><span class="line">yolox/exp/yolox_base.py中的self.num_classes   <span class="comment">#类别数量</span></span><br><span class="line">yolox/data/datasets/coco.py  <span class="comment">#改size</span></span><br><span class="line">exps/default/yolox_s_ti_lite.py <span class="comment">#模型配置文件，在里面修改参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#运行训练：</span></span><br><span class="line">python -m yolox.tools.train -n yolox-s-ti-lite -d 0 -b 16 --fp16 -o --cache</span><br><span class="line"><span class="comment">#Save weights to ./YOLOX_outputs/yolox_s_ti_lite</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="1-模型文件转ONNX"><a href="#1-模型文件转ONNX" class="headerlink" title="1. 模型文件转ONNX"></a>1. 模型文件转ONNX</h2><p><del>pycharm进入edgeai-yolox项目，根据提示额外安装requirements</del><br>Window中配置该环境需要安装visual studio build tools，而且很多包报错，因此转ubuntu用vscode搭pytorch环境，非常顺利（vscode插件离线安装：如装python插件，直接进<a href="https://marketplace.visualstudio.com/vscode"> marketplace </a>下好拖到扩展位置）拓展设置中把Python Default Path改成创建的环境 <code>/home/wyj/anaconda3/envs/pytorch/bin/python</code>，最后用vscode打开项目，F5运行py程序，将.pth转为 <code>.onnx, .prototxt</code> 文件。</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">pip3 install -U pip &amp;&amp; pip3 install -r requirements.txt</span><br><span class="line">pip3 install -v -e .  <span class="comment"># or  python3 setup.py develop</span></span><br><span class="line"><span class="comment">#安装pycocotools</span></span><br><span class="line">pip3 install cython</span><br><span class="line">pip3 install <span class="string">&#x27;git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI&#x27;</span></span><br><span class="line"><span class="comment">#下载ti的yolox-s-ti-lite.pth放入项目文件夹，运行export，</span></span><br><span class="line">python3 tools/export_onnx.py --output-name yolox_s_ti_lite.onnx -f exps/default/yolox_s_ti_lite.py -c yolox-s-ti-lite.pth</span><br><span class="line"></span><br><span class="line"><span class="comment">#Debug：</span></span><br><span class="line">TypeError: Descriptors cannot not be created directly. &gt; pip install protobuf==3.19.6;</span><br><span class="line">AttributeError: module <span class="string">&#x27;numpy&#x27;</span> has no attribute <span class="string">&#x27;object&#x27;</span>. &gt; pip install numpy==1.23.4</span><br><span class="line"><span class="comment">#成功，生成onnx文件</span></span><br><span class="line"> __main__:main:245 - generated onnx model named yolox_s_ti_lite.onnx</span><br><span class="line"> __main__:main:261 - generated simplified onnx model named yolox_s_ti_lite.onnx</span><br><span class="line"> __main__:main:264 - generated prototxt yolox_s_ti_lite.prototxt</span><br></pre></td></tr></table></figure>
<details>
<summary>yolox_s_ti_lite.prototxt</summary>

<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">name: <span class="string">&quot;yolox&quot;</span></span><br><span class="line">tidl_yolo &#123;</span><br><span class="line">  yolo_param &#123;</span><br><span class="line">    input: <span class="string">&quot;/head/Concat_output_0&quot;</span></span><br><span class="line">    anchor_width: 8.0</span><br><span class="line">    anchor_height: 8.0&#125;</span><br><span class="line">  yolo_param &#123;</span><br><span class="line">    input: <span class="string">&quot;/head/Concat_3_output_0&quot;</span></span><br><span class="line">    anchor_width: 16.0</span><br><span class="line">    anchor_height: 16.0&#125;</span><br><span class="line">  yolo_param &#123;</span><br><span class="line">    input: <span class="string">&quot;/head/Concat_6_output_0&quot;</span></span><br><span class="line">    anchor_width: 32.0</span><br><span class="line">    anchor_height: 32.0&#125;</span><br><span class="line">detection_output_param &#123;</span><br><span class="line">    num_classes: 80</span><br><span class="line">    share_location: <span class="literal">true</span></span><br><span class="line">    background_label_id: -1</span><br><span class="line">    nms_param &#123;</span><br><span class="line">      nms_threshold: 0.4</span><br><span class="line">      top_k: 500&#125;</span><br><span class="line">    code_type: CODE_TYPE_YOLO_X</span><br><span class="line">    keep_top_k: 200</span><br><span class="line">    confidence_threshold: 0.4&#125;</span><br><span class="line">  name: <span class="string">&quot;yolox&quot;</span></span><br><span class="line">  in_width: 640</span><br><span class="line">  in_height: 640</span><br><span class="line">  output: <span class="string">&quot;detections&quot;</span>&#125;</span><br></pre></td></tr></table></figure>
</details>           

<hr>
<p><a href="https://github.com/TexasInstruments/edgeai-yolox/tree/main/demo/ONNXRuntime#yolox-onnxruntime-in-python">ONNXRuntime inference</a></p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> &lt;YOLOX_HOME&gt;</span><br><span class="line">python3 demo/ONNXRuntime/onnx_inference.py -m yolox_s_ti_lite.onnx -i assets/dog.jpg -o output -s 0.3 --input_shape 640,640</span><br><span class="line"><span class="comment">#成功基于ONNXRuntime输出预测结果</span></span><br></pre></td></tr></table></figure>
<img alt="图 1" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/TDA4VM3onnxinference.jpg" width="50%"/>  

<h2 id="2-使用TIDL编译ONNX并运行"><a href="#2-使用TIDL编译ONNX并运行" class="headerlink" title="2. 使用TIDL编译ONNX并运行"></a>2. 使用TIDL编译ONNX并运行</h2><p>本节使用了两种不同的方法完成PC端TIDL的编译运行：</p>
<ol>
<li>TIDL Importer: 使用RTOS SDK中提供的导入工具，提供了很多例程（8.6中没有，copy 8.5的），方便快捷；</li>
<li>TIDL Tools：TI提供的工具，见github <a href="https://github.com/TexasInstruments/edgeai-tidl-tools">edgeai-tidl-tools</a>，或在RTOS SDK也内置了，灵活度高，不支持的算子分配到ARM核，支持的会使用TIDL加速运行，增加了深度学习模型开发和运行的效率。但要求平台有onnx运行环境</li>
</ol>
<h3 id="a-使用TIDL-Importer-by-RTOS-SDK"><a href="#a-使用TIDL-Importer-by-RTOS-SDK" class="headerlink" title="a. 使用TIDL Importer (by RTOS SDK)"></a>a. 使用<a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/06_01_01_12/exports/docs/tidl_j7_01_00_01_00/ti_dl/docs/user_guide_html/md_tidl_model_import.html">TIDL Importer</a> (by RTOS SDK)</h3><ol>
<li>模型文件配置：拷贝 .onnx, .prototxt 文件至&#x2F;ti_dl&#x2F;test&#x2F;testvecs&#x2F;models&#x2F;public&#x2F;onnx&#x2F;，<strong>yolox_s_ti_lite.prototxt</strong>中改in_width&amp;height，根据情况改nms_threshold: 0.4，confidence_threshold: 0.4</li>
<li>编写转换配置文件：在&#x2F;testvecs&#x2F;config&#x2F;import&#x2F;public&#x2F;onnx下新建（或复制参考目录下yolov3例程）<strong>tidl_import_yolox_s.txt</strong>，参数配置见<a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/06_01_01_12/exports/docs/tidl_j7_01_00_01_00/ti_dl/docs/user_guide_html/md_tidl_model_import.html">文档</a>, 元架构类型见 <a href="https://github.com/TexasInstruments/edgeai-tidl-tools/blob/master/docs/tidl_fsg_od_meta_arch.md">Object detection meta architectures</a>，<code>inData</code>处修改自定义的数据输入</li>
</ol>
<p><em>转换配置文件tidl_import_yolox_s.txt</em></p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">modelType       = 2     <span class="comment">#模型类型，0: Caffe, 1: TensorFlow, 2: ONNX, 3: tfLite</span></span><br><span class="line">numParamBits    = 8     <span class="comment">#模型参数的位数，Bit depth for model parameters like Kernel, Bias etc.</span></span><br><span class="line">numFeatureBits  = 8     <span class="comment">#Bit depth for Layer activation</span></span><br><span class="line">quantizationStyle = 3   <span class="comment">#量化方法，Quantization method. 2: Linear Mode. 3: Power of 2 scales（2的幂次）</span></span><br><span class="line">inputNetFile    = <span class="string">&quot;../../test/testvecs/models/public/onnx/yolox-s-ti-lite.onnx&quot;</span> <span class="comment">#Net definition from Training frames work</span></span><br><span class="line">outputNetFile   = <span class="string">&quot;../../test/testvecs/config/tidl_models/onnx/yolo/tidl_net_yolox_s.bin&quot;</span>   <span class="comment">#Output TIDL model with Net and Parameters</span></span><br><span class="line">outputParamsFile = <span class="string">&quot;../../test/testvecs/config/tidl_models/onnx/yolo/tidl_io_yolox_s_&quot;</span>  <span class="comment">#Input and output buffer descriptor file for TIDL ivision interface</span></span><br><span class="line">inDataNorm      = 1     <span class="comment">#1 Enable / 0 Disable Normalization on input tensor.</span></span><br><span class="line">inMean          = 0 0 0 <span class="comment">#Mean value needs to be subtracted for each channel of all input tensors</span></span><br><span class="line">inScale         = 1.0 1.0 1.0   <span class="comment">#Scale value needs to be multiplied after means subtract for each channel of all input tensors，yolov3例程是0.003921568627 0.003921568627 0.003921568627</span></span><br><span class="line">inDataFormat    = 1     <span class="comment">#Input tensor color format. 0: BGR planar, 1: RGB planar</span></span><br><span class="line">inWidth         = 1024  <span class="comment">#each input tensors Width (可以在.prototxt文件中查找到)</span></span><br><span class="line">inHeight        = 512   <span class="comment">#each input tensors Height</span></span><br><span class="line">inNumChannels   = 3     <span class="comment">#each input tensors Number of channels</span></span><br><span class="line">numFrames       = 1     <span class="comment">#Number of input tensors to be processed from the input file</span></span><br><span class="line">inData          =   <span class="string">&quot;../../test/testvecs/config/detection_list.txt&quot;</span> <span class="comment">#Input tensors File for Reading</span></span><br><span class="line">perfSimConfig   = ../../test/testvecs/config/import/device_config.cfg   <span class="comment">#Network Compiler Configuration file</span></span><br><span class="line">inElementType   = 0     <span class="comment">#Format for each input feature, 0 : 8bit Unsigned, 1 : 8bit Signed</span></span><br><span class="line">metaArchType    = 6     <span class="comment">#网络使用的元架构类型，Meta Architecture used by the network，ssd mobilenetv2 = 3, yolov3 = 4, efficientdet tflite = 5, yolov5 yolox = 6</span></span><br><span class="line">metaLayersNamesList =  <span class="string">&quot;../../test/models/pubilc/onnx/yolox_s_ti_lite.prototxt&quot;</span> <span class="comment">#架构配置文件，Configuration files describing the details of Meta Arch</span></span><br><span class="line">postProcType    = 2     <span class="comment">#后处理，Post processing on output tensor. 0 : Disable, 1- Classification top 1 and 5 accuracy, 2 – Draw bounding box for OD, 3 - Pixel level color blending</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li><p>模型导入<br>使用TIDL import tool，得到可执行文件 <code>.bin</code></p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;TIDL_INSTALL_PATH&#125;</span>/ti_dl/utils/tidlModelImport</span><br><span class="line">./out/tidl_model_import.out <span class="variable">$&#123;TIDL_INSTALL_PATH&#125;</span>/ti_dl/test/testvecs/config/import/public/onnx/tidl_import_yolox_s.txt</span><br><span class="line"><span class="comment">#successful Memory allocation</span></span><br><span class="line"><span class="comment">#../../test/testvecs/config/tidl_models/onnx/生成的文件分析：</span></span><br><span class="line">tidl_net_yolox_s.bin        <span class="comment">#Compiled network file 网络模型数据</span></span><br><span class="line">tidl_io_yolox_s_1.bin       <span class="comment">#Compiled I/O file 网络输入配置文件</span></span><br><span class="line">tidl_net_yolox_s.bin.svg    <span class="comment">#tidlModelGraphviz tool生成的网络图</span></span><br><span class="line">tidl_out.png, tidl_out.txt  <span class="comment">#执行的目标检测测试结果，与第三步TIDL运行效果一致 txt:[class, source, confidence, Lower left point(x,y), upper right point(x,y) ]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Debug，本来使用官方的yolox_s.pth转成onnx后导入，发现报错：</span></span><br><span class="line">Step != 1 is NOT supported <span class="keyword">for</span> Slice Operator -- /backbone/backbone/stem/Slice_3 </span><br><span class="line"><span class="comment">#因为&quot;the slice operations in Focus layer are not embedded friendly&quot;，因此ti提供yolox-s-ti-lite，优化后的才能直接导入</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>TIDL运行(PC inference)</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#在文件ti_dl/test/testvecs/config/config_list.txt顶部加入:</span></span><br><span class="line">1 testvecs/config/infer/public/onnx/tidl_infer_yolox.txt</span><br><span class="line">0</span><br><span class="line"></span><br><span class="line"><span class="comment">#新建tidl_infer_yolox.txt:</span></span><br><span class="line">inFileFormat    = 2</span><br><span class="line">numFrames       = 1</span><br><span class="line">netBinFile      = <span class="string">&quot;testvecs/config/tidl_models/onnx/yolo/tidl_net_yolox_s.bin&quot;</span></span><br><span class="line">ioConfigFile    = <span class="string">&quot;testvecs/config/tidl_models/onnx/yolo/tidl_io_yolox_s_1.bin&quot;</span></span><br><span class="line">inData  =   testvecs/config/detection_list.txt</span><br><span class="line">outData =   testvecs/output/tidl_yolox_od.bin</span><br><span class="line">inResizeMode    = 0</span><br><span class="line">debugTraceLevel = 0</span><br><span class="line">writeTraceLevel = 0</span><br><span class="line">postProcType    = 2</span><br><span class="line"></span><br><span class="line"><span class="comment">#运行，结果在ti_dl/test/testvecs/output/</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;TIDL_INSTALL_PATH&#125;</span>/ti_dl/test</span><br><span class="line">./PC_dsp_test_dl_algo.out</span><br></pre></td></tr></table></figure>
<img alt="图 2" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/TDA4VM3sdktidlyolox.png" width="50%"/></li>
</ol>
<h3 id="b-使用TIDL-Tools（by-Edge-AI-Studio）"><a href="#b-使用TIDL-Tools（by-Edge-AI-Studio）" class="headerlink" title="b. 使用TIDL Tools（by Edge AI Studio）"></a>b. 使用TIDL Tools（by <a href="https://dev.ti.com/edgeaistudio/">Edge AI Studio</a>）</h3><p>使用<code>Edge AI Studio &gt; Model Analyzer &gt; Custom models &gt; ONNX runtime &gt; custom-model-onnx.ipynb</code>例程, 并结合 <code>OD.ipynb</code> 例程进行修改</p>
<p><em>YOLOX.ipynb</em></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> onnxruntime <span class="keyword">as</span> rt</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#/notebooks/scripts/utils.py:</span></span><br><span class="line"><span class="keyword">from</span> scripts.utils <span class="keyword">import</span> imagenet_class_to_name, download_model, loggerWritter, get_svg_path, get_preproc_props, single_img_visualise, det_box_overlay</span><br></pre></td></tr></table></figure>
<p>其中scripts.utils中的代码细节在<code>/notebooks/scripts/utils.py</code></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#预处理</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">image_path</span>):</span><br><span class="line">    img = cv2.imread(image_path) <span class="comment"># 使用OpenCV读取图像</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;原始图像：&#x27;</span>, img.shape, img.dtype)</span><br><span class="line">    img = cv2.resize(img, (<span class="number">640</span>, <span class="number">640</span>), interpolation=cv2.INTER_CUBIC)</span><br><span class="line">    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line">    img = img.astype(<span class="string">&#x27;float32&#x27;</span>) / <span class="number">255.0</span></span><br><span class="line">    img = (img * <span class="number">255</span>).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">    img = np.expand_dims(img, axis=<span class="number">0</span>) <span class="comment"># 扩展图片数组维度</span></span><br><span class="line">    img = np.transpose(img, (<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)) <span class="comment"># NHWC 格式（batch_size，height, width，channels）转换为 NCHW 格式</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;处理后的图像：&#x27;</span>, img.shape, img.dtype)</span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure>
<p>图片的预处理十分重要，调试时注意print图片数据，避免处理出错</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#配置</span></span><br><span class="line">images = [</span><br><span class="line"><span class="string">&#x27;WYJ/dog.jpg&#x27;</span>,</span><br><span class="line">]</span><br><span class="line">output_dir = <span class="string">&#x27;WYJ/output&#x27;</span><span class="comment">#优化后的ONNX模型将保存的输出目录</span></span><br><span class="line">onnx_model_path = <span class="string">&#x27;WYJ/yolox_s_lite_640x640_20220221_model.onnx&#x27;</span></span><br><span class="line">prototxt_path = <span class="string">&#x27;WYJ/yolox_s_lite_640x640_20220221_model.prototxt&#x27;</span></span><br><span class="line"><span class="keyword">with</span> loggerWritter(<span class="string">&quot;WYJ/logs&quot;</span>):<span class="comment"># stdout and stderr saved to a *.log file.</span></span><br><span class="line">    compile_options = &#123;</span><br><span class="line">      <span class="string">&#x27;tidl_tools_path&#x27;</span> : os.environ[<span class="string">&#x27;TIDL_TOOLS_PATH&#x27;</span>],</span><br><span class="line">      <span class="string">&#x27;artifacts_folder&#x27;</span> : output_dir,</span><br><span class="line">      <span class="string">&#x27;tensor_bits&#x27;</span> : <span class="number">8</span>,</span><br><span class="line">      <span class="string">&#x27;accuracy_level&#x27;</span> : <span class="number">1</span>,</span><br><span class="line">      <span class="string">&#x27;advanced_options:calibration_frames&#x27;</span> : <span class="built_in">len</span>(images), </span><br><span class="line">      <span class="string">&#x27;advanced_options:calibration_iterations&#x27;</span> : <span class="number">3</span>, <span class="comment"># used if accuracy_level = 1</span></span><br><span class="line">      <span class="string">&#x27;debug_level&#x27;</span> : <span class="number">1</span>, <span class="comment"># 设置调试级别，级别越高提供的调试信息越详细</span></span><br><span class="line">      <span class="comment">#&#x27;advanced_options:output_feature_16bit_names_list&#x27;: &#x27;370, 680, 990, 1300&#x27;,    </span></span><br><span class="line">      <span class="comment">#&#x27;deny_list&#x27;: &#x27;ScatterND&#x27;, #&#x27; Conv, Relu, Add, Concat, Resize&#x27;, # MaxPool</span></span><br><span class="line">      <span class="string">&#x27;object_detection:meta_arch_type&#x27;</span>: <span class="number">6</span>,</span><br><span class="line">      <span class="string">&#x27;object_detection:meta_layers_names_list&#x27;</span>: prototxt_path,    </span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment"># create the output dir if not present &amp; clear the directory</span></span><br><span class="line">os.makedirs(output_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(output_dir, topdown=<span class="literal">False</span>):</span><br><span class="line">    [os.remove(os.path.join(root, f)) <span class="keyword">for</span> f <span class="keyword">in</span> files]</span><br><span class="line">    [os.rmdir(os.path.join(root, d)) <span class="keyword">for</span> d <span class="keyword">in</span> dirs]</span><br></pre></td></tr></table></figure>
<p>object_detection:meta_arch_type、meta_layers_names_list两个参数在OD任务中必须配置，否则内核直接奔溃，参数配置文档中也有说明：<a href="https://github.com/TexasInstruments/edgeai-tidl-tools/blob/master/examples/osrt_python/README.md#object-detection-model-specific-options">object-detection-model-specific-options</a></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#模型转换</span></span><br><span class="line">so = rt.SessionOptions()</span><br><span class="line">EP_list = [<span class="string">&#x27;TIDLCompilationProvider&#x27;</span>,<span class="string">&#x27;CPUExecutionProvider&#x27;</span>]</span><br><span class="line">sess = rt.InferenceSession(onnx_model_path ,providers=EP_list, provider_options=[compile_options, &#123;&#125;], sess_options=so)</span><br><span class="line"><span class="comment"># 获取所有输入输出详细信息</span></span><br><span class="line">input_details = sess.get_inputs()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Model input details:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> input_details:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line">output_details = sess.get_outputs()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Model output details:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> output_details:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line"><span class="comment">#运行</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm.trange(<span class="built_in">len</span>(images)):</span><br><span class="line">    processed_image = preprocess(images[i])</span><br><span class="line">    output=<span class="literal">None</span></span><br><span class="line">    output = <span class="built_in">list</span>(sess.run(<span class="literal">None</span>, &#123;input_details[<span class="number">0</span>].name :processed_image &#125;))</span><br></pre></td></tr></table></figure>
<p>打印输入输出信息，运行编译</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#画框</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageDraw</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&quot;WYJ/dog.jpg&quot;</span>)</span><br><span class="line"></span><br><span class="line">width_scale = <span class="number">640</span> / img.size[<span class="number">0</span>]</span><br><span class="line">height_scale = <span class="number">640</span> / img.size[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 创建ImageDraw对象</span></span><br><span class="line">draw = ImageDraw.Draw(img)</span><br><span class="line"><span class="comment"># 遍历所有边界框，画出矩形</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(output[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>].shape[<span class="number">0</span>])):</span><br><span class="line">    <span class="comment"># 取出顶点坐标和置信度</span></span><br><span class="line">    xmin, ymin, xmax, ymax, conf = <span class="built_in">tuple</span>(output[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>][i].tolist())</span><br><span class="line">    <span class="keyword">if</span>(conf &gt; <span class="number">0.4</span>) :</span><br><span class="line">        cls = <span class="built_in">int</span>(output[<span class="number">1</span>][<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>][i])  <span class="comment"># 取出类别编号</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;class:&#x27;</span>, cls, <span class="string">&#x27;, box:&#x27;</span>,output[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>][i])</span><br><span class="line">        color = (<span class="number">255</span>, cls*<span class="number">10</span>, cls*<span class="number">100</span>)        <span class="comment"># 选择不同颜色表示不同类别</span></span><br><span class="line">        <span class="comment"># 画出矩形框</span></span><br><span class="line">        draw.rectangle(((xmin/ width_scale, ymin/ height_scale), (xmax/ width_scale, ymax/ height_scale)), outline=color, width=<span class="number">2</span>)</span><br><span class="line">img.show()  <span class="comment"># 显示画好的图像</span></span><br></pre></td></tr></table></figure>
<p>画框，引入了缩放比例，否则框的位置不对<br><img alt="图 3" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/TDA4VM3studioyolox.png" width="50%"/>  </p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Subgraphs visualization</span></span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Markdown <span class="keyword">as</span> md</span><br><span class="line"></span><br><span class="line">subgraph_link =get_svg_path(output_dir) </span><br><span class="line"><span class="keyword">for</span> sg <span class="keyword">in</span> subgraph_link:</span><br><span class="line">    hl_text = os.path.join(*Path(sg).parts[<span class="number">4</span>:])</span><br><span class="line">    sg_rel = os.path.join(<span class="string">&#x27;../&#x27;</span>, sg)</span><br><span class="line">    display(md(<span class="string">&quot;[&#123;&#125;](&#123;&#125;)&quot;</span>.<span class="built_in">format</span>(hl_text,sg_rel)))</span><br></pre></td></tr></table></figure>
<p>生成两个.svg网络可视化图的链接</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#模型推理</span></span><br><span class="line">EP_list = [<span class="string">&#x27;TIDLExecutionProvider&#x27;</span>,<span class="string">&#x27;CPUExecutionProvider&#x27;</span>]</span><br><span class="line">sess = rt.InferenceSession(onnx_model_path ,providers=EP_list, provider_options=[compile_options, &#123;&#125;], sess_options=so)</span><br><span class="line"></span><br><span class="line">input_details = sess.get_inputs()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):<span class="comment">#Running inference several times to get an stable performance output</span></span><br><span class="line">    output = <span class="built_in">list</span>(sess.run(<span class="literal">None</span>, &#123;input_details[<span class="number">0</span>].name : preprocess(<span class="string">&#x27;WYJ/dog.jpg&#x27;</span>)&#125;))</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scripts.utils <span class="keyword">import</span> plot_TI_performance_data, plot_TI_DDRBW_data, get_benchmark_output</span><br><span class="line">stats = sess.get_TI_benchmark_data()</span><br><span class="line">fig, ax = plt.subplots(nrows=<span class="number">1</span>, ncols=<span class="number">1</span>, figsize=(<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line">plot_TI_performance_data(stats, axis=ax)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">tt, st, rb, wb = get_benchmark_output(stats)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Statistics : \n Inferences Per Second   : <span class="subst">&#123;<span class="number">1000.0</span>/tt :<span class="number">7.2</span>f&#125;</span> fps&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27; Inference Time Per Image : <span class="subst">&#123;tt :<span class="number">7.2</span>f&#125;</span> ms  \n DDR BW Per Image        : <span class="subst">&#123;rb+ wb : <span class="number">7.2</span>f&#125;</span> MB&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>推理，注意<code>TIDLCompilationProvider</code>和<code>TIDLExecutionProvider</code>的区别<br><img alt="图 2" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/TDA4VM3yoloxs.png" width="90%"/>  </p>
<blockquote>
<p>Statistics :<br>  Inferences Per Second   :  104.44 fps<br>  Inference Time Per Image :    9.57 ms<br>  DDR BW Per Image        :   16.22 MB</p>
</blockquote>
<p><strong>Debug</strong>:</p>
<ul>
<li>将custom-model-onnx 替换为自己的模型后报错，且内核经常挂掉，这不是服务器的问题，而是代码中有错误引发 Jupyter 中的某种内存分配问题并kill内核.（如，索引路径错误，模型不存在，config参数配置错误）—— <a href="https://e2e.ti.com/support/processors-group/processors/f/processors-forum/1214094/tda4vm-inference-with-custom-artifacts-kills-kernel-in-edge-ai-studio/4658432?tisearch=e2e-sitesearch&keymatch=edge%252520ai%252520studio#4658432">E2E:Kills Kernel in Edge AI Studio</a></li>
<li>在My Workspace中， 右上角<code>New &gt; Terminal</code> 可以打开终端，便于进一步的调试</li>
<li>prebuilt-models中的预训练模型每次重启EVM都要先重新解压:<br><code>cd notebooks/prebuilt-models/8bits/</code><br><code>find . -name &quot;*.tar.gz&quot; -exec tar --one-top-level -zxvf &quot;&#123;&#125;&quot; \;</code></li>
<li>内核频繁挂掉：重启EVM</li>
</ul>
<h2 id="3-板端运行-TDA4VM-SK"><a href="#3-板端运行-TDA4VM-SK" class="headerlink" title="3. 板端运行(TDA4VM-SK)"></a>3. 板端运行(TDA4VM-SK)</h2><p><del>连接SK板进入minicom串口通讯传输模型文件(失败)</del>（若能连网线通过jupyternotebook配置更方便，这里网络有限制所以配置都通过SD卡进行）</p>
<p>通过SD卡配置编译生成的模型：</p>
<blockquote>
<p>配置模型文件夹yolox_s_studio放入modelzoo文件夹</p>
<blockquote>
<p>artifacts：存放编译生成的工件，.bin, .txt<br>model：原onnx模型，.onnx, (.prototxt)<br>param.yaml：配置文件, 其中需要修改model_path等参数<br>dataset.yaml：数据集类别对应文件</p>
</blockquote>
</blockquote>
<p>通过SD卡配置object_detection.yaml，在model参数中索引上面建立的模型文件夹</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#通过minicom连接串口</span></span><br><span class="line">sudo minicom -D /dev/ttyUSB2 -c on</span><br><span class="line">root <span class="comment">#登录</span></span><br><span class="line"><span class="comment">#运行yolox_s实例</span></span><br><span class="line"><span class="built_in">cd</span> /opt/edgeai-gst-apps/apps_cpp</span><br><span class="line">./bin/Release/app_edgeai ../configs/object_detection.yaml</span><br></pre></td></tr></table></figure>

<h3 id="修改app-edgeai（optional）"><a href="#修改app-edgeai（optional）" class="headerlink" title="修改app_edgeai（optional）"></a>修改app_edgeai（optional）</h3><p>在<code>opt\edgeai-gst-apps\apps_cpp\</code>完成修改后重新make:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Regular builds (Build_Instructions.txt)</span></span><br><span class="line"><span class="built_in">mkdir</span> build &amp;&amp; <span class="built_in">cd</span> build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br></pre></td></tr></table></figure>

<h2 id="4-性能评估"><a href="#4-性能评估" class="headerlink" title="4. 性能评估"></a>4. 性能评估</h2><p>Docs: <a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-linux-sk-tda4vm/latest/exports/docs/performance_visualizer.html#">Performance Visualization Tool</a><br>运行实例时，会在运行文件的上一级<code>../perf_Logs/</code>中生成 <code>.md</code> 格式的<strong>Performance Logs</strong>，最多15个，运行时会不断覆写</p>
<p>也可以使用Perfstats tool, 把运行状态在terminal print:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#构建工具</span></span><br><span class="line"><span class="built_in">cd</span> /opt/edgeai-gst-apps/scripts/perf_stats</span><br><span class="line"><span class="built_in">mkdir</span> build &amp;&amp; <span class="built_in">cd</span> build</span><br><span class="line">cmake .. &amp;&amp; make</span><br><span class="line"><span class="comment">#运行评估</span></span><br><span class="line"><span class="built_in">cd</span> /opt/edgeai-gst-apps/scripts/perf_stats/build</span><br><span class="line">../bin/Release/perf_stats -l</span><br></pre></td></tr></table></figure>
<p>此外，使用官方提供的可视化工具Visualization tool是最佳选择，但是要装Docker<br><img data-src="https://software-dl.ti.com/jacinto7/esd/processor-sdk-linux-sk-tda4vm/latest/exports/docs/_images/perf_plots.png"></p>
<h1 id="Performance-Logs"><a href="#Performance-Logs" class="headerlink" title="Performance Logs"></a>Performance Logs</h1><h2 id="Summary-of-CPU-load"><a href="#Summary-of-CPU-load" class="headerlink" title="Summary of CPU load"></a>Summary of CPU load</h2><table>
<thead>
<tr>
<th>CPU</th>
<th>TOTAL LOAD %</th>
</tr>
</thead>
<tbody><tr>
<td>mpu1_0</td>
<td>40.83</td>
</tr>
<tr>
<td>mcu2_0</td>
<td>7. 0</td>
</tr>
<tr>
<td>mcu2_1</td>
<td>1. 0</td>
</tr>
<tr>
<td>c6x_1</td>
<td>0. 0</td>
</tr>
<tr>
<td>c6x_2</td>
<td>1. 0</td>
</tr>
<tr>
<td>c7x_1</td>
<td>32. 0</td>
</tr>
</tbody></table>
<h2 id="HWA-performance-statistics"><a href="#HWA-performance-statistics" class="headerlink" title="HWA performance statistics"></a>HWA performance statistics</h2><table>
<thead>
<tr>
<th>HWA（Hardware Accelerator）</th>
<th>LOAD（Million Operations per second）</th>
</tr>
</thead>
<tbody><tr>
<td>MSC0（Multiply and Accumulate）</td>
<td>6.94 % ( 42 MP&#x2F;s )</td>
</tr>
<tr>
<td>MSC1</td>
<td>6.74 % ( 55 MP&#x2F;s )</td>
</tr>
</tbody></table>
<h2 id="DDR-performance-statistics"><a href="#DDR-performance-statistics" class="headerlink" title="DDR performance statistics"></a>DDR performance statistics</h2><table>
<thead>
<tr>
<th>DDR BW</th>
<th>AVG</th>
<th>PEAK</th>
</tr>
</thead>
<tbody><tr>
<td>READ BW</td>
<td>1509 MB&#x2F;s</td>
<td>5713 MB&#x2F;s</td>
</tr>
<tr>
<td>WRITE BW</td>
<td>721 MB&#x2F;s</td>
<td>3643 MB&#x2F;s</td>
</tr>
<tr>
<td>TOTAL BW</td>
<td>2230 MB&#x2F;s</td>
<td>9356 MB&#x2F;s</td>
</tr>
</tbody></table>
<h2 id="Detailed-CPU-performance-memory-statistics"><a href="#Detailed-CPU-performance-memory-statistics" class="headerlink" title="Detailed CPU performance&#x2F;memory statistics"></a>Detailed CPU performance&#x2F;memory statistics</h2><h3 id="CPU-mcu2-0"><a href="#CPU-mcu2-0" class="headerlink" title="CPU: mcu2_0"></a>CPU: mcu2_0</h3><table>
<thead>
<tr>
<th>TASK</th>
<th>TASK LOAD</th>
</tr>
</thead>
<tbody><tr>
<td>IPC_RX</td>
<td>0.34 %</td>
</tr>
<tr>
<td>REMOTE_SRV</td>
<td>0.30 %</td>
</tr>
<tr>
<td>LOAD_TEST</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_CPU_0</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_V1NF</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_V1LDC1</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_V1SC1</td>
<td>3. 9 %</td>
</tr>
<tr>
<td>TIVX_V1MSC2</td>
<td>3.24 %</td>
</tr>
<tr>
<td>TIVXVVISS1</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_CAPT1</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_CAPT2</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_DISP1</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_DISP2</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_CSITX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_CAPT3</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_CAPT4</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_CAPT5</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_CAPT6</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_CAPT7</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_CAPT8</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_DPM2M1</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_DPM2M2</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_DPM2M3</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_DPM2M4</td>
<td>0. 0 %</td>
</tr>
</tbody></table>
<h4 id="CPU-Heap-Table"><a href="#CPU-Heap-Table" class="headerlink" title="CPU Heap Table"></a>CPU Heap Table</h4><table>
<thead>
<tr>
<th>HEAP</th>
<th>Size</th>
<th>Free</th>
<th>Unused</th>
</tr>
</thead>
<tbody><tr>
<td>DDR_LOCAL_MEM</td>
<td>16777216 B</td>
<td>16768256 B</td>
<td>99 %</td>
</tr>
<tr>
<td>L3_MEM</td>
<td>262144 B</td>
<td>261888 B</td>
<td>99 %</td>
</tr>
</tbody></table>
<details>
<summary>CPU: mcu2_1</summary>

<h3 id="CPU-mcu2-1"><a href="#CPU-mcu2-1" class="headerlink" title="CPU: mcu2_1"></a>CPU: mcu2_1</h3><table>
<thead>
<tr>
<th>TASK</th>
<th>TASK LOAD</th>
</tr>
</thead>
<tbody><tr>
<td>IPC_RX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>REMOTE_SRV</td>
<td>0.18 %</td>
</tr>
<tr>
<td>LOAD_TEST</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_CPU_1</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_SDE</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_DOF</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_RX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_TX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_TX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_TX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_TX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_TX</td>
<td>0. 0 %</td>
</tr>
</tbody></table>
<h4 id="CPU-Heap-Table-1"><a href="#CPU-Heap-Table-1" class="headerlink" title="CPU Heap Table"></a>CPU Heap Table</h4><table>
<thead>
<tr>
<th>HEAP</th>
<th>Size</th>
<th>Free</th>
<th>Unused</th>
</tr>
</thead>
<tbody><tr>
<td>DDR_LOCAL_MEM</td>
<td>16777216 B</td>
<td>16773376 B</td>
<td>99 %</td>
</tr>
<tr>
<td>L3_MEM</td>
<td>262144 B</td>
<td>262144 B</td>
<td>100 %</td>
</tr>
</tbody></table>
</details>

<details>
<summary>CPU: c6x_1</summary>

<h3 id="CPU-c6x-1"><a href="#CPU-c6x-1" class="headerlink" title="CPU: c6x_1"></a>CPU: c6x_1</h3><table>
<thead>
<tr>
<th>TASK</th>
<th>TASK LOAD</th>
</tr>
</thead>
<tbody><tr>
<td>IPC_RX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>REMOTE_SRV</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>LOAD_TEST</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_CPU</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_RX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_TX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_TX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_TX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_TX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_TX</td>
<td>0. 0 %</td>
</tr>
</tbody></table>
<h4 id="CPU-Heap-Table-2"><a href="#CPU-Heap-Table-2" class="headerlink" title="CPU Heap Table"></a>CPU Heap Table</h4><table>
<thead>
<tr>
<th>HEAP</th>
<th>Size</th>
<th>Free</th>
<th>Unused</th>
</tr>
</thead>
<tbody><tr>
<td>DDR_LOCAL_MEM</td>
<td>16777216 B</td>
<td>16773376 B</td>
<td>99 %</td>
</tr>
<tr>
<td>L2_MEM</td>
<td>229376 B</td>
<td>229376 B</td>
<td>100 %</td>
</tr>
<tr>
<td>DDR_SCRATCH_MEM</td>
<td>50331648 B</td>
<td>50331648 B</td>
<td>100 %</td>
</tr>
</tbody></table>
</details>

<details>
<summary>CPU: c6x_2</summary>

<h3 id="CPU-c6x-2"><a href="#CPU-c6x-2" class="headerlink" title="CPU: c6x_2"></a>CPU: c6x_2</h3><table>
<thead>
<tr>
<th>TASK</th>
<th>TASK LOAD</th>
</tr>
</thead>
<tbody><tr>
<td>IPC_RX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>REMOTE_SRV</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>LOAD_TEST</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_CPU</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_RX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_TX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_TX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_TX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_TX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_TX</td>
<td>0. 0 %</td>
</tr>
</tbody></table>
<h4 id="CPU-Heap-Table-3"><a href="#CPU-Heap-Table-3" class="headerlink" title="CPU Heap Table"></a>CPU Heap Table</h4><table>
<thead>
<tr>
<th>HEAP</th>
<th>Size</th>
<th>Free</th>
<th>Unused</th>
</tr>
</thead>
<tbody><tr>
<td>DDR_LOCAL_MEM</td>
<td>16777216 B</td>
<td>16773376 B</td>
<td>99 %</td>
</tr>
<tr>
<td>L2_MEM</td>
<td>229376 B</td>
<td>229376 B</td>
<td>100 %</td>
</tr>
<tr>
<td>DDR_SCRATCH_MEM</td>
<td>50331648 B</td>
<td>50331648 B</td>
<td>100 %</td>
</tr>
</tbody></table>
</details>

<h3 id="CPU-c7x-1"><a href="#CPU-c7x-1" class="headerlink" title="CPU: c7x_1"></a>CPU: c7x_1</h3><table>
<thead>
<tr>
<th>TASK</th>
<th>TASK LOAD</th>
</tr>
</thead>
<tbody><tr>
<td>IPC_RX</td>
<td>0. 5 %</td>
</tr>
<tr>
<td>REMOTE_SRV</td>
<td>0. 1 %</td>
</tr>
<tr>
<td>LOAD_TEST</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_C71_P1</td>
<td>31.38 %</td>
</tr>
<tr>
<td>TIVX_C71_P2</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_C71_P3</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_C71_P4</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_C71_P5</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_C71_P6</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_C71_P7</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>TIVX_C71_P8</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_RX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_TX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_TX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_TX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_TX</td>
<td>0. 0 %</td>
</tr>
<tr>
<td>IPC_TEST_TX</td>
<td>0. 0 %</td>
</tr>
</tbody></table>
<h4 id="CPU-Heap-Table-4"><a href="#CPU-Heap-Table-4" class="headerlink" title="CPU Heap Table"></a>CPU Heap Table</h4><table>
<thead>
<tr>
<th>HEAP</th>
<th>Size</th>
<th>Free</th>
<th>Unused</th>
</tr>
</thead>
<tbody><tr>
<td>DDR_LOCAL_MEM</td>
<td>268435456 B</td>
<td>232984320 B</td>
<td>86 %</td>
</tr>
<tr>
<td>L3_MEM</td>
<td>8159232 B</td>
<td>0 B</td>
<td>0 %</td>
</tr>
<tr>
<td>L2_MEM</td>
<td>458752 B</td>
<td>458752 B</td>
<td>100 %</td>
</tr>
<tr>
<td>L1_MEM</td>
<td>16384 B</td>
<td>0 B</td>
<td>0 %</td>
</tr>
<tr>
<td>DDR_SCRATCH_MEM</td>
<td>385875968 B</td>
<td>367400145 B</td>
<td>95 %</td>
</tr>
</tbody></table>
<h2 id="Performance-point-statistics"><a href="#Performance-point-statistics" class="headerlink" title="Performance point statistics"></a>Performance point statistics</h2><h3 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h3><table>
<thead>
<tr>
<th>PERF</th>
<th>avg (usecs)</th>
<th>min&#x2F;max (usecs)</th>
<th>number of executions</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>33352</td>
<td>0 &#x2F; 412578</td>
<td>9556</td>
</tr>
</tbody></table>
<h3 id="FPS"><a href="#FPS" class="headerlink" title="FPS"></a>FPS</h3><table>
<thead>
<tr>
<th>PERF</th>
<th>Frames per sec (FPS)</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>29.98</td>
</tr>
</tbody></table>
<h2 id="Temperature-statistics"><a href="#Temperature-statistics" class="headerlink" title="Temperature statistics"></a>Temperature statistics</h2><table>
<thead>
<tr>
<th>ZONE</th>
<th>TEMPERATURE</th>
</tr>
</thead>
<tbody><tr>
<td>CPU</td>
<td>50.93 Celsius</td>
</tr>
<tr>
<td>WKUP</td>
<td>49.52 Celsius</td>
</tr>
<tr>
<td>C7X</td>
<td>51.86 Celsius</td>
</tr>
<tr>
<td>GPU</td>
<td>51.63 Celsius</td>
</tr>
<tr>
<td>R5F</td>
<td>50.93 Celsius</td>
</tr>
</tbody></table>
<hr>
<blockquote>
<p>TDA4系列文章：<br><a href="https://wangyujie.site/TDA4VM/">TDA4①：SDK, TIDL, OpenVX</a><br><a href="https://wangyujie.site/TDA4VM2/">TDA4②：环境搭建、模型转换、Demo及Tools</a><br><a href="https://wangyujie.site/TDA4VM3/">TDA4③：YOLOX的模型转换与SK板端运行</a><br><a href="https://wangyujie.site/TDA4VM4/">TDA4④：部署自定义模型</a></p>
</blockquote>
]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>嵌入式</tag>
      </tags>
  </entry>
  <entry>
    <title>TDA4②：环境搭建、模型转换、Demo及Tools</title>
    <url>/TDA4VM2/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>TDA4的SDK环境搭建，SK开发板配置，TIDL demo运行，TIDL Tools与Edge AI Studio工具的介绍。</p>
<span id="more"></span>

<p>相关前置知识见上一篇：<a href="https://wangyujie.site/TDA4VM/">TDA4①：SDK, TIDL, OpenVX</a><br>环境搭建需要下载SDK：<a href="https://www.ti.com.cn/tool/cn/PROCESSOR-SDK-J721E">PROCESSOR-SDK-J721E</a></p>
<p>以下两节是EVM板的PSDK RTOS与PSDK Linux的环境搭建，因为暂时没有EVM板所以<em>没有上板测试</em>，只有SK板可以跳到第三节 TDA4VM-SK 配置。</p>
<h1 id="Linux-SDK-环境搭建"><a href="#Linux-SDK-环境搭建" class="headerlink" title="Linux SDK 环境搭建"></a><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-linux-rt-jacinto7/08_06_00_11/exports/docs/devices/J7/linux/index.html">Linux SDK</a> 环境搭建</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">添加执行文件并执行</span></span><br><span class="line">chmod +x ./ti-processor-sdk-linux-j7-evm-08_06_01_02-Linux-x86-Install.bin </span><br><span class="line">./ti-processor-sdk-linux-j7-evm-08_06_01_02-Linux-x86-Install.bin</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">安装依赖的系统软件包和工具，安装过程中跳过需要连EVM的NFS、minicom、TFTP</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">(若Ubuntu版本不匹配 &gt; bin/setup-host-check.sh &gt; <span class="keyword">if</span> [ <span class="string">&quot;<span class="variable">$host</span>&quot;</span> != <span class="string">&quot;bionic&quot;</span> ] 改为 <span class="keyword">if</span> [ <span class="string">&quot;<span class="variable">$host</span>&quot;</span> != <span class="string">&quot;focal&quot;</span> ] )</span></span><br><span class="line">sudo ./setup.sh</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">TISDK setup completed!</span></span><br></pre></td></tr></table></figure>
<p>通过在根目录下make linux或u-boot等各种命令，可以快速的让SDK编译出你所需要的产物。注意需要手工修改Rules.mak文件中的DESTDIR变量为你的TF卡挂载路径。</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#ti-processor-sdk-linux-j7-evm*/board-support/</span></span><br><span class="line">Make linux        <span class="comment">#编译Linux kernel代码和dtb，主要用于内核驱动的修改和裁剪。安装命令可以将内核和驱动模块自动拷贝到TF卡中。</span></span><br><span class="line">Make linux_install  <span class="comment">#生成built-images</span></span><br><span class="line">Make u-boot       <span class="comment">#编译u-boot代码，主要分为两部分：运行在MCU上的r5f部分和运行在A72上的a53部分。此处A72兼容A53指令集。</span></span><br><span class="line">Make sysfw-image  <span class="comment">#生成sysfw固件，主要在修改MSMC大小的时候会用到。</span></span><br></pre></td></tr></table></figure>


<h1 id="RTOS-SDK-环境搭建"><a href="#RTOS-SDK-环境搭建" class="headerlink" title="RTOS SDK 环境搭建"></a><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/08_06_00_12/exports/docs/psdk_rtos/docs/user_guide/index.html">RTOS SDK</a> 环境搭建</h1><blockquote>
<p>下载：<br>ti-processor-sdk-rtos-j721e-evm-08_06_01_03.tar.gz<br>ti-processor-sdk-rtos-j721e-evm-08_06_01_03-prebuilt.tar<br>+两个dataset.tar.gz</p>
</blockquote>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">tar -xf ti-processor-sdk-rtos-j721e-evm-08_06_01_03.tar.gz  <span class="comment">#解压</span></span><br><span class="line"><span class="comment">#配置RTOS和Linux的安装环境变量</span></span><br><span class="line"><span class="built_in">export</span> PSDKL_PATH=/home/wyj/SDK/ti-processor-sdk-linux-j7-evm-08_06_01_02</span><br><span class="line"><span class="built_in">export</span> PSDKR_PATH=/home/wyj/SDK/ti-processor-sdk-rtos-j721e-evm-08_06_01_03</span><br><span class="line"><span class="comment">#拷贝linux系统文件和linux启动文件到psdk rtos文件夹（或从rtos-prebuilt.tar）</span></span><br><span class="line"><span class="built_in">cp</span> <span class="variable">$&#123;PSDKL_PATH&#125;</span>/board-support/prebuilt-images/boot-j7-evm.tar.gz <span class="variable">$&#123;PSDKR_PATH&#125;</span>/</span><br><span class="line"><span class="built_in">cp</span> <span class="variable">$&#123;PSDKL_PATH&#125;</span>/filesystem/tisdk-default-image-j7-evm.tar.xz <span class="variable">$&#123;PSDKR_PATH&#125;</span>/</span><br><span class="line"><span class="comment">#安装依赖库和下载编译器，若安装报错则需换源，有包没安上会影响之后的make</span></span><br><span class="line">./psdk_rtos/scripts/setup_psdk_rtos.sh  <span class="comment">#若卡在git clone则进.sh把git://换成https://</span></span><br><span class="line"><span class="comment">#Packages installed successfully</span></span><br></pre></td></tr></table></figure>


<h2 id="Vision-Apps-Demo-编译"><a href="#Vision-Apps-Demo-编译" class="headerlink" title="Vision Apps Demo 编译"></a><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/08_06_00_12/exports/docs/vision_apps/docs/user_guide/ENVIRONMENT_SETUP.html">Vision Apps Demo</a> 编译</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#修改文件 tiovx/build_flags.mak（没修改过则是默认）</span></span><br><span class="line">BUILD_EMULATION_MODE=no <span class="comment">#非模拟器模式</span></span><br><span class="line">BUILD_TARGET_MODE=<span class="built_in">yes</span></span><br><span class="line">BUILD_LINUX_A72=<span class="built_in">yes</span></span><br><span class="line">PROFILE=release</span><br><span class="line"><span class="comment">#Optional:配置tiovx/build_flags.mak, vision_apps/vision_apps_build_flags.mak</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#开始编译vision apps</span></span><br><span class="line"><span class="built_in">cd</span> vision_apps</span><br><span class="line">make vision_apps -j8    <span class="comment">#若缺少core-secdev-k3包，手动导入(https://git.ti.com/cgit/security-development-tools/core-secdev-k3/snapshot/core-secdev-k3-08.06.00.006.tar.gz)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#编译成功可以看到对应目录下有产出文件，RTOS SDK主要使用了一个开源编译框架concerto，这个框架基于Makefile，他能够自动搜索当前目录内的所有concerto.mak文件，并且分析依赖，一次将各个核心的固件全部编译出来。编译生成的文件位于</span></span><br><span class="line">vision_apps/out/J7/A72/LINUX/<span class="variable">$PROFILE</span></span><br><span class="line">vision_apps/out/J7/R5F/SYSBIOS/<span class="variable">$PROFILE</span></span><br><span class="line">vision_apps/out/J7/C66/SYSBIOS/<span class="variable">$PROFILE</span></span><br><span class="line">vision_apps/out/J7/C71/SYSBIOS/<span class="variable">$PROFILE</span></span><br><span class="line"><span class="comment">##If clean build of vision_apps/clean the full PSDK RTOS</span></span><br><span class="line"><span class="comment">#cd vision_apps, make vision_apps_scrub/make sdk_scrub</span></span><br></pre></td></tr></table></figure>
<details>
<summary>配置SD卡(EVM)</summary>
配置SD卡(EVM)，在TDA4VM的开发过程中，都是使用TF卡进行开发的。在单片机开发平台下，通常是直接用电脑使用USB方式将固件烧写到板卡的eMMC或FLASH中去。在TI平台下，首选的调试方法是使用TF卡：TF卡会被划分为两个分区，一个是 *BOOT* 分区（FAT32），用于存放bootloader如uboot等，另一个是 *rootfs* 分区（ext4），用于存放Linux需要的文件系统。每次Ubuntu编译完成的固件都需要手动拷贝到TF卡中，然后将TF卡插入EVM上电启动。

<p><code>df -h</code>, 查得SD卡设备名 <code>/dev/sdb</code><br>使用RTOS SDK prebuilt中的脚本依次执行：</p>
<table>
<thead>
<tr>
<th>脚本</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>sudo .&#x2F;mk-linux-card.sh &#x2F;dev&#x2F;sdb</td>
<td>用途：将TF卡重新分区、并且格式化</td>
</tr>
<tr>
<td>.&#x2F;install_to_sd_card.sh</td>
<td>将该脚本旁边的文件系统压缩包直接拷贝到&#x2F;media&#x2F;USER&#x2F;BOOT和&#x2F;media&#x2F;USER&#x2F;rootfs中，需要十几分钟，之后该卡就可以启动了。</td>
</tr>
<tr>
<td>.&#x2F;install_data_set_to_sd_card.sh .&#x2F;psdk_rtos_ti_data_set_08_06_00.tar.gz</td>
<td>以及.&#x2F;psdk_rtos_ti_data_set_08_06_00_j721e.tar.gz，将数据集解压到TF卡中对应的位置，这样默认SDK配套的Demo就可以正常运行。</td>
</tr>
</tbody></table>
<p>添加可执行文件至SD card</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;PSDKR_PATH&#125;</span>/vision_apps</span><br><span class="line">make linux_fs_install_sd</span><br></pre></td></tr></table></figure>
<p>然后即可插在EVM端运行，这里没有，跳过。</p>
</details>

<hr>
<p>上面都是EVM板的相关环境配置，后面只拿到了SK板，因此转为SK板的相关配置。</p>
<h1 id="TDA4VM-SK-配置"><a href="#TDA4VM-SK-配置" class="headerlink" title="TDA4VM-SK 配置"></a><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-linux-edgeai/TDA4VM/08_06_01/exports/docs/devices/TDA4VM/linux/getting_started.html">TDA4VM-SK</a> 配置</h1><p>硬件信息：<a href="https://www.ti.com.cn/cn/lit/ug/zhcu912c/zhcu912c.pdf">SK-TDA4VM 用户指南</a><br><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-linux-sk-tda4vm/latest/exports/docs/running_simple_demos.html">Processor SDK Linux for Edge AI Documentation</a><br>配置文档：<a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-linux-edgeai/TDA4VM/08_06_01/exports/docs/devices/TDA4VM/linux/getting_started.html">Processor SDK Linux for SK-TDA4VM Documentation - getting_started</a>，详细说明了如何配置，下面是简要步骤：</p>
<blockquote>
<p>物料准备：<br>SK板，microUSB串口线，USB camera，HDMI&#x2F;DP显示器，≥16GB的内存卡，网线和局域网*，串口电源（5-20V DC ≥20w），散热风扇</p>
</blockquote>
<p>通过USB挂载SD卡到Ubuntu<br>下载<a href="https://www.ti.com/tool/download/PROCESSOR-SDK-LINUX-SK-TDA4VM">SD card .wic image</a><br>使用<a href="https://github.com/balena-io/etcher/releases/tag/v1.7.0">Balena etcher tool 1.7.0</a> 把 SD card .wic image <code>flash</code>到SD卡上<br>然后插入SD卡到SK板，拨码开关拨到数字端，系统从SD卡启动<br>SK板连接显示器，上电，进入界面。</p>
<p>连接串口线，在虚拟机设置中挂载USB串口，使用 <a href="https://help.ubuntu.com/community/Minicom">minicom</a> 串口通讯：<br>(在minicom中自动换行：Ctrl+A Z W)</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sudo apt-get install minicom  <span class="comment">#安装minicom</span></span><br><span class="line">sudo minicom -D /dev/ttyUSB2 -c on</span><br><span class="line"><span class="comment">#输入用户名：root，登录tda4vm-sk</span></span><br><span class="line"><span class="comment">#若连接了USB摄像头此时会显示端口信息，也可以运行 ./init_script.sh 查摄像头端口号：/dev/video2</span></span><br></pre></td></tr></table></figure>

<p>连接显示器后（HDMI&#x2F;DP），可以鼠标点击试运行开箱即用的 GUI 应用程序，也可使用 Python 和C++参考示例开发边缘 AI 应用程序：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#配置</span></span><br><span class="line"><span class="built_in">cd</span> /opt/edgeai-gst-apps/configs/  <span class="comment">#app_config_template.yaml中有参数介绍</span></span><br><span class="line">vi image_classification.yaml  <span class="comment">#flow参数配置为摄像头输入input0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Classification (python)</span></span><br><span class="line"><span class="built_in">cd</span> /opt/edgeai-gst-apps/apps_python</span><br><span class="line">./app_edgeai.py ../configs/image_classification.yaml  <span class="comment">#ctrl+c退出</span></span><br><span class="line"><span class="comment">#替换为configs下其他文件能执行不同任务如object_detection.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Classification (c++)</span></span><br><span class="line"><span class="built_in">cd</span> /opt/edgeai-gst-apps/apps_cpp</span><br><span class="line">./bin/Release/app_edgeai ../configs/image_classification.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment">#视频流车辆检测</span></span><br><span class="line"><span class="built_in">cd</span> /opt/edgeai-gst-apps/scripts/optiflow</span><br><span class="line">`./optiflow.py ../../configs/object_detection.yaml -t`  <span class="comment">#如果没有单引号，终端会将 -t 选项解释为一个单独的参数，而不是作为 optiflow.py 命令的选项之一</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#多flows</span></span><br><span class="line">flows:</span><br><span class="line">    <span class="comment"># flowname : [input,mode1,output,[mosaic_pos_x,mosaic_pos_y,width,height]]</span></span><br><span class="line">    flow0: [input0,model1,output0,[160,90,800,450]]</span><br><span class="line">    flow1: [input0,model2,output0,[960,90,800,450]]</span><br><span class="line">    flow2: [input1,model0,output0,[160,540,800,450]]</span><br><span class="line">    flow3: [input1,model3,output0,[960,540,800,450]]</span><br></pre></td></tr></table></figure>
<p>如果运行过程中突然重启，一般是需要加个<em>风扇</em>增强散热</p>
<p>可选操作：</p>
<ul>
<li>连接网线，ifconfig查询板子ip地址，后面即可使用ssh登陆，可以使用vscode的remote插件来直接ssh登陆到板子，然后可以很方便地修改配置文件</li>
<li>安装tensorflow，onnx，python和c++依赖库 <code>/opt/edge_ai_apps#./setup_script.sh </code></li>
</ul>
<p><strong>Dataflows</strong><br><img data-src="https://software-dl.ti.com/jacinto7/esd/processor-sdk-linux-edgeai/TDA4VM/08_06_01/exports/docs/_images/edgeai_object_detection.png" width='90%'><br>GStreamer input pipeline:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">v4l2src device=/dev/video18 io-mode=2 ! image/jpeg, width=1280, height=720 ! jpegdec ! tiovxdlcolorconvert ! video/x-raw, format=NV12 ! tiovxmultiscaler name=split_01</span><br><span class="line">split_01. ! queue ! video/x-raw, width=320, height=320 ! tiovxdlpreproc data-type=10 channel-order=1 mean-0=128.000000 mean-1=128.000000 mean-2=128.000000 scale-0=0.007812 scale-1=0.007812 scale-2=0.007812 tensor-format=rgb out-pool-size=4 ! application/x-tensor-tiovx ! appsink name=pre_0 max-buffers=2 drop=<span class="literal">true</span></span><br><span class="line">split_01. ! queue ! video/x-raw, width=1280, height=720 ! tiovxdlcolorconvert out-pool-size=4 ! video/x-raw, format=RGB ! appsink name=sen_0 max-buffers=2 drop=<span class="literal">true</span></span><br></pre></td></tr></table></figure>
<p>GStreamer output pipeline:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">appsrc format=GST_FORMAT_TIME is-live=<span class="literal">true</span> block=<span class="literal">true</span> do-timestamp=<span class="literal">true</span> name=post_0 ! tiovxdlcolorconvert ! video/x-raw,format=NV12, width=1280, height=720 ! queue ! mosaic_0.sink_0</span><br><span class="line">appsrc format=GST_FORMAT_TIME block=<span class="literal">true</span> num-buffers=1 name=background_0 ! tiovxdlcolorconvert ! video/x-raw,format=NV12, width=1920, height=1080 ! queue ! mosaic_0.background</span><br><span class="line">tiovxmosaic name=mosaic_0</span><br><span class="line">sink_0::startx=<span class="string">&quot;&lt;320&gt;&quot;</span>  sink_0::starty=<span class="string">&quot;&lt;180&gt;&quot;</span>  sink_0::widths=<span class="string">&quot;&lt;1280&gt;&quot;</span>   sink_0::heights=<span class="string">&quot;&lt;720&gt;&quot;</span></span><br><span class="line">! video/x-raw,format=NV12, width=1920, height=1080 ! kmssink <span class="built_in">sync</span>=<span class="literal">false</span> driver-name=tidss</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/TexasInstruments/edgeai-gst-apps/tree/44f4d44ddcda766d2abb5e89b9b112a1280f99ec">Edge AI application stack</a><br><img data-src='https://software-dl.ti.com/jacinto7/esd/processor-sdk-linux-edgeai/TDA4VM/08_06_01/exports/docs/_images/edgeai-app-stack.jpg' width='80%'></p>
<h1 id="TIDL"><a href="#TIDL" class="headerlink" title="TIDL"></a><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/06_01_01_12/exports/docs/tidl_j7_01_00_01_00/ti_dl/docs/user_guide_html/md_tidl_user_model_deployment.html">TIDL</a></h1><img data-src="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/06_01_01_12/exports/docs/tidl_j7_01_00_01_00/ti_dl/docs/user_guide_html/TIDL_import_process.png" >

<h2 id="Demo-使用TIDL实现多种模型的导入、运行-from-RTOS-SDK"><a href="#Demo-使用TIDL实现多种模型的导入、运行-from-RTOS-SDK" class="headerlink" title="Demo:使用TIDL实现多种模型的导入、运行 (from RTOS SDK)"></a>Demo:使用TIDL实现多种模型的导入、运行 (from RTOS SDK)</h2><p>文档教程：<a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/06_01_01_12/exports/docs/tidl_j7_01_00_01_00/ti_dl/docs/user_guide_html/md_tidl_user_model_deployment.html#importing-mobilenetv2-model-for-image-classification">MobileNetV2 Tensorflow，PeleeNet Caffe，JSegNet21V2 Caffe model</a>，下面以PeleeNet为例</p>
<p><strong>Config</strong> TIDL</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> TIDL_INSTALL_PATH=/home/wyj/SDK/ti-processor-sdk-rtos-j721e-evm-08_06_01_03/tidl_j721e_08_06_00_10</span><br><span class="line"><span class="comment">#配置永久环境变量更方便，sudo gedit /etc/profile，末尾加入如上代码，然后source /etc/profile加载立即生效</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#optional：tidlModelGraphviz tool 模型可视化工具</span></span><br><span class="line">sudo apt install graphviz-dev</span><br><span class="line"><span class="built_in">export</span> TIDL_GRAPHVIZ_PATH=/usr</span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;TIDL_INSTALL_PATH&#125;</span>/ti_dl/utils/tidlModelGraphviz</span><br><span class="line">make</span><br></pre></td></tr></table></figure>

<p><strong>Import</strong>ing PeleeNet model for object detection (caffe)<br><a href="https://drive.google.com/file/d/1KJHKYQ2nChZXlxroZRpg-tRsksTXUhe9/view">下载</a>并提取.caffemodel，deploy.prototxt放入<code>ti_dl/test/testvecs/models/public/caffe/peele/pelee_voc/</code><br>deploy.prototxt中改confidence_threshold: 0.4</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;TIDL_INSTALL_PATH&#125;</span>/ti_dl/utils/tidlModelImport</span><br><span class="line">./out/tidl_model_import.out <span class="variable">$&#123;TIDL_INSTALL_PATH&#125;</span>/ti_dl/test/testvecs/config/import/public/caffe/tidl_import_peeleNet.txt</span><br><span class="line"><span class="comment">#$&#123;TIDL_INSTALL_PATH&#125;/ti_dl/test/下面的配置文件在RTOSsdk8.6中找不到，要从SDK8.5复制！！！</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#successful Memory allocation</span></span><br><span class="line"><span class="comment"># Compiled network and I/O .bin files used for inference</span></span><br><span class="line">    <span class="comment"># Compiled network file in ti_dl/test/testvecs/config/tidl_models/caffe/tidl_net_peele_300.bin</span></span><br><span class="line">    <span class="comment"># Compiled I/O file in ti_dl/test/testvecs/config/tidl_models/caffe/tidl_io_peele_300_1.bin</span></span><br><span class="line"><span class="comment"># Performance simulation results for network analysis in ti_dl/utils/perfsim/tidl_import_peeleNet.txt/tidl_import_peeleNet...csv</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#若是tensorflow例程，.pb需要先运行tensorflow的.local/lib/python3.6/site-packages/tensorflow/python/tools/optimize_for_inference.py工具进行模型推理优化，再导入。</span></span><br></pre></td></tr></table></figure>


<p><strong>Run</strong>ning PeleeNet for object detection</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#在文件ti_dl/test/testvecs/config/config_list.txt顶部加入:</span></span><br><span class="line">1 testvecs/config/infer/public/caffe/tidl_infer_pelee.txt</span><br><span class="line">0</span><br><span class="line"></span><br><span class="line"><span class="comment">#运行，结果在ti_dl/test/testvecs/output/</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;TIDL_INSTALL_PATH&#125;</span>/ti_dl/test</span><br><span class="line">./PC_dsp_test_dl_algo.out</span><br><span class="line"><span class="comment">#若标注框尺寸不匹配，需要改deploy.prototxt文件顶部：dim: 512  dim: 1024</span></span><br></pre></td></tr></table></figure>
<img alt="picture 1" data-src="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/06_01_01_12/exports/docs/tidl_j7_01_00_01_00/ti_dl/docs/user_guide_html/out_ti_lindau_000020.png" width="70%"/>  







<h2 id="TIDL-RT"><a href="#TIDL-RT" class="headerlink" title="TIDL-RT"></a><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/08_06_00_12/exports/docs/tidl_j721e_08_06_00_10/ti_dl/docs/user_guide_html/md_tidl_dependency_info.html">TIDL-RT</a></h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> TIDL_INSTALL_PATH=/home/ywang85/SDK/RTOSSDK/tidl_j721e_08_06_00_10   <span class="comment">#设置环境变量</span></span><br><span class="line"><span class="comment">#TARGET_PLATFORM=PC make gv失败：../../inc/itidl_ti.h:91:21: fatal error: ivision.h: No such file or directory</span></span><br><span class="line"><span class="comment">#跳过，不修改code暂时不要rebuild</span></span><br></pre></td></tr></table></figure>



<h2 id="EdgeAI-TIDL-Tools"><a href="#EdgeAI-TIDL-Tools" class="headerlink" title="EdgeAI TIDL Tools"></a><a href="https://github.com/TexasInstruments/edgeai-tidl-tools">EdgeAI TIDL Tools</a></h2><p>EdgeAI TIDL Tools是TI提供的深度学习开发工具，后续会多次用到。</p>
<p>要求：OS——Ubuntu 18.04，Python Version——3.6<br><img alt="图 9" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/TDA4VM2onnxruntimeflow.png" width="60%"/>  </p>
<ol>
<li>OSRT(Open Source Runtimes:TFLite,ONNX,TVM) 作为用户应用程序的顶级推理 API</li>
<li>将子图卸载到 C7x&#x2F;MMA 以使用TIDL进行加速执行</li>
<li>在 ARM 核心上运行优化代码，以支持 TIDL 不支持的层（<a href="https://github.com/TexasInstruments/edgeai-tidl-tools/blob/master/docs/supported_ops_rts_versions.md">支持情况</a>）</li>
</ol>
<p><a href="https://github.com/TexasInstruments/edgeai-tidl-tools/tree/08_06_00_05#setup">Setup - TexasInstruments&#x2F;edgeai-tidl-tools at 08_06_00_05</a></p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libyaml-cpp-dev</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/TexasInstruments/edgeai-tidl-tools.git <span class="comment">#failed：手动安装证书 git config --global http.sslVerify false，export GIT_SSL_NO_VERIFY=1</span></span><br><span class="line"><span class="built_in">cd</span> edgeai-tidl-tools</span><br><span class="line">git checkout 08_06_00_05</span><br><span class="line"><span class="built_in">export</span> SOC=am68pa</span><br><span class="line"><span class="built_in">source</span> ./setup.sh</span><br><span class="line"><span class="comment">#Docker Based X86_PC Setup 跳过，不用docker装</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#配置变量</span></span><br><span class="line"><span class="built_in">export</span> SOC=am68pa</span><br><span class="line"><span class="built_in">export</span> TIDL_TOOLS_PATH=$(<span class="built_in">pwd</span>)/tidl_tools</span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$LD_LIBRARY_PATH</span>:<span class="variable">$TIDL_TOOLS_PATH</span></span><br><span class="line"><span class="built_in">export</span> ARM64_GCC_PATH=$(<span class="built_in">pwd</span>)/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu</span><br><span class="line"><span class="comment">#配置永久环境变量更方便，sudo gedit /etc/profile，末尾加入如上代码，然后source /etc/profile加载立即生效</span></span><br><span class="line"><span class="built_in">source</span> ./scripts/run_python_examples.sh <span class="comment">#编译运行</span></span><br><span class="line">python3 ./scripts/gen_test_report.py    <span class="comment">#评估</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Compile and Validate on X86_PC for cpp_example</span></span><br><span class="line"><span class="built_in">mkdir</span> build &amp;&amp; <span class="built_in">cd</span> build</span><br><span class="line">cmake ../examples &amp;&amp; make -j &amp;&amp; <span class="built_in">cd</span> ..</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th align="center">Image Classification</th>
<th align="center">Object detection</th>
<th align="center">Semantic Segmentation</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><a href="https://github.com/TexasInstruments/edgeai-tidl-tools/blob/08_06_00_05/docs/out_viz_cls.jpg"><img data-src="https://github.com/TexasInstruments/edgeai-tidl-tools/raw/08_06_00_05/docs/out_viz_cls.jpg"></a></td>
<td align="center"><a href="https://github.com/TexasInstruments/edgeai-tidl-tools/blob/08_06_00_05/docs/out_viz_od.jpg"><img data-src="https://github.com/TexasInstruments/edgeai-tidl-tools/raw/08_06_00_05/docs/out_viz_od.jpg"></a></td>
<td align="center"><a href="https://github.com/TexasInstruments/edgeai-tidl-tools/blob/08_06_00_05/docs/out_viz_ss.jpg"><img data-src="https://github.com/TexasInstruments/edgeai-tidl-tools/raw/08_06_00_05/docs/out_viz_ss.jpg"></a></td>
</tr>
</tbody></table>
<h2 id="Edge-AI-Studio"><a href="#Edge-AI-Studio" class="headerlink" title="Edge AI Studio"></a><a href="https://dev.ti.com/edgeaistudio/">Edge AI Studio</a></h2><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/TDA4VM2studio.png" width="80%"/>  

<p>TI官方提供的云端环境，集成了一系列工具,无需本地搭环境，使用需要申请，提供两个工具：</p>
<ul>
<li><a href="https://dev.ti.com/modelcomposer/">Model Composer</a>： 为 TI 嵌入式处理器训练、优化和编译 AI 模型。支持数据采集，标注，模型训练，以及上板编译，<strong>一步到位</strong>。目前仅支持分类和检测任务，只能使用modelzoo中的模型进行训练，比如OD任务只有yolox模型，灵活度不高，主打方便快捷。</li>
<li><a href="https://dev.ti.com/edgeaisession/">Model Analyzer</a>：远程连接到真实的评估硬件，基于jupyter notebook，在 TI 嵌入式处理器上部署和测试 AI 模型性能，进行多个模型的Benchmark。前身叫做 TI edge AI cloud。</li>
</ul>
<h3 id="Model-Analyzer"><a href="#Model-Analyzer" class="headerlink" title="Model Analyzer"></a>Model Analyzer</h3><p>选TDA4VM设备，能使用3h，文件在顶端My Workspace;<br>进入后分两大板块:</p>
<ul>
<li>Find your model: Compare model performance, 能查看不同模型在板端的表现，用来选择适合自己需求的模型；<img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/TDA4VM2perform.png" width="70%"/>  </li>
<li>Get model benchmarks：<ul>
<li>Model performance 是配置好的jupyter notebook，无需修改一步步运行即可输出结果；</li>
<li>下面重点使用Custom models：</li>
</ul>
</li>
</ul>
<p><strong>Custom models</strong>（onnxRT）</p>
<ul>
<li>编译模型（在异构模型编译期间，支持的层将被装载到<code>TI-DSP</code>，生成推理所需工件（artifacts））</li>
<li>使用生成的工件进行推理</li>
<li><em>执行输入预处理和输出后处理</em></li>
<li>启用调试日志</li>
<li>使用deny-layer编译选项来隔离可能有问题的层并创建额外的模型子图</li>
<li>使用生成的子图工件进行推理</li>
<li><em>执行输入预处理和输出后处理</em></li>
</ul>
<p>Create Onnx runtime with <code>tidl_model_import_onnx</code> library to generate artifacts that offload supported portion of the DL model to the TI DSP.<br>参数配置见<a href="https://github.com/TexasInstruments/edgeai-tidl-tools/blob/master/examples/osrt_python/README.md#user-options-for-tidl-acceleration">User options for TIDL Acceleration</a></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># &#x27;sess&#x27; model compilation options</span></span><br><span class="line">compile_options = &#123;</span><br><span class="line">    <span class="string">&#x27;tidl_tools_path&#x27;</span> : os.environ[<span class="string">&#x27;TIDL_TOOLS_PATH&#x27;</span>], <span class="comment">#tidl tools 路径</span></span><br><span class="line">    <span class="string">&#x27;artifacts_folder&#x27;</span> : output_dir, <span class="comment">#编译输出目录</span></span><br><span class="line">    <span class="string">&#x27;tensor_bits&#x27;</span> : num_bits,    <span class="comment">#量化位数</span></span><br><span class="line">    <span class="string">&#x27;accuracy_level&#x27;</span> : accuracy, <span class="comment">#精度级别，0快但精度低，1慢但精度高</span></span><br><span class="line">    <span class="string">&#x27;advanced_options:calibration_frames&#x27;</span> : <span class="built_in">len</span>(calib_images),  <span class="comment">#设置用于校准模型量化参数的图片</span></span><br><span class="line">    <span class="string">&#x27;advanced_options:calibration_iterations&#x27;</span> : <span class="number">3</span>, <span class="comment">#设置校准迭代次数 used if accuracy_level = 1</span></span><br><span class="line">    <span class="string">&#x27;debug_level&#x27;</span> : <span class="number">1</span>, <span class="comment">#设置调试级别，级别越高提供的调试信息越详细</span></span><br><span class="line">    <span class="string">&#x27;deny_list&#x27;</span> : <span class="string">&quot;MaxPool&quot;</span> <span class="comment">#排除ONNXRT不支持的层</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个会话选项对象，可以设置GPU加速、CPU 线程数、精度模式等会话参数</span></span><br><span class="line">so = rt.SessionOptions() <span class="comment">#此处默认参数</span></span><br><span class="line"><span class="comment"># 设置执行提供者列表，包含 TIDLCompilationProvider 和 CPUExecutionProvider</span></span><br><span class="line">EP_list = [<span class="string">&#x27;TIDLCompilationProvider&#x27;</span>, <span class="string">&#x27;CPUExecutionProvider&#x27;</span>]</span><br><span class="line"><span class="comment"># compile the model with TIDL acceleration by passing required compilation options.</span></span><br><span class="line">sess = rt.InferenceSession(onnx_model_path, providers=EP_list, provider_options=[compile_options, &#123;&#125;], sess_options=so)</span><br><span class="line"><span class="comment"># 载入 ONNX 模型并进行推理。可以使用 sess 对象来进行标准化、预处理、推理等操作，还可以获取模型的输入信息、输出信息、元图信息等</span></span><br><span class="line"><span class="comment"># At the end of model compilation step, model-artifacts for inference will be generated in user specified path.</span></span><br><span class="line"></span><br><span class="line">input_details = sess.get_inputs() <span class="comment"># 获取输入数据信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对校准图片进行预处理并进行推理，并将输出结果存储到 output 列表中</span></span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> tqdm.trange(<span class="built_in">len</span>(calib_images)):</span><br><span class="line">    output = <span class="built_in">list</span>(sess.run(<span class="literal">None</span>, &#123;input_details[<span class="number">0</span>].name : preprocess_for_onnx_resent18v2(calib_images[num])&#125;))[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># Create OSRT inference session with TIDL acceleration option for running inference with generated model artifacts in the above step.</span></span><br></pre></td></tr></table></figure>

<p>Then using Onnx with the libtidl_onnxrt_EP inference library we run the model and collect benchmark data.<br><img alt="图 9" data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/TDA4VM2benchmark.png" width="88%"/>  </p>
<p><a href="https://github.com/TexasInstruments/edgeai-tidl-tools/blob/master/examples/osrt_python/README.md">edgeai-tidl-tools:Python Examples</a><br><a href="https://www.ti.com.cn/cn/lit/ug/zhcu546/zhcu546.pdf">适用于嵌入式应用的深度学习推理参考设计</a></p>
<h1 id="EdgeAI-Benchmark"><a href="#EdgeAI-Benchmark" class="headerlink" title="EdgeAI-Benchmark"></a><a href="https://github.com/TexasInstruments/edgeai-benchmark/tree/master">EdgeAI-Benchmark</a></h1><p>EdgeAI-Benchmark提供了一系列针对不同图像识别任务的脚本，包括分类、分割、检测和关键点检测。（使用<a href="https://github.com/TexasInstruments/edgeai-tidl-tools">edgeai-tidl-tools</a>用于模型编译和推理）</p>
<h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h2><p>文档：<a href="https://github.com/TexasInstruments/edgeai-benchmark/blob/master/docs/setup_instructions.md">setup_instructions</a>，其中<code>pyenv install 3.6</code>可能因为网络原因下载极慢，这时可以先从官网或镜像源下载所需要的包到 ~&#x2F;.pyenv&#x2F;cache 目录下，再执行安装命令<br>此后每次需要激活环境：<code>pyenv activate benchmark</code></p>
<h2 id="edgeai-tidl-tools-docs-custom-model-evaluation-md"><a href="#edgeai-tidl-tools-docs-custom-model-evaluation-md" class="headerlink" title="edgeai-tidl-tools&#x2F;docs&#x2F;custom_model_evaluation.md"></a><a href="https://github.com/TexasInstruments/edgeai-tidl-tools/blob/master/docs/custom_model_evaluation.md">edgeai-tidl-tools&#x2F;docs&#x2F;custom_model_evaluation.md</a></h2><blockquote>
<p>TDA4系列文章：<br><a href="https://wangyujie.site/TDA4VM/">TDA4①：SDK, TIDL, OpenVX</a><br><a href="https://wangyujie.site/TDA4VM2/">TDA4②：环境搭建、模型转换、Demo及Tools</a><br><a href="https://wangyujie.site/TDA4VM3/">TDA4③：YOLOX的模型转换与SK板端运行</a><br><a href="https://wangyujie.site/TDA4VM4/">TDA4④：部署自定义模型</a></p>
</blockquote>
]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>嵌入式</tag>
      </tags>
  </entry>
  <entry>
    <title>TDA4④：部署自定义深度学习模型</title>
    <url>/TDA4VM4/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>自定义深度学习模型的转换、编译及部署流程，使用了三种不同的编译工具：TIDL Importer，Edge AI Studio，EdgeAI-TIDL-Tools</p>
<span id="more"></span>

<p>接上一篇：<a href="https://wangyujie.site/TDA4VM3/">TDA4③：YOLOX的模型转换与SK板端运行</a></p>
<p>TI文档中对yolo、mobilenet、resnet等主流深度学习模型支持十分完善，相关开箱即用的文件在 <a href="https://github.com/TexasInstruments/edgeai-modelzoo">Modelzoo</a> 中，但有关自定义模型的编译和部署内容很少，只能利用例程和提供的工具进行尝试。</p>
<p>深度学习模型基于TI板端运行要有几个组件：</p>
<ol>
<li><strong>model</strong>：这个目录包含了要进行推理的模型（.onnx, *.prototxt）</li>
<li><strong>artifacts</strong>：这个目录包含了模型编译后生成的文件。这些文件可以用Edge AI TIDL Tools来生成和验证</li>
<li><strong>param.yaml</strong>：配置文件，提供了模型的基本信息，以及相关的预处理和后处理参数</li>
<li>*<strong>dataset.yaml</strong>：配置文件，说明了用于模型训练的数据集的细节</li>
<li>*<strong>run.log</strong>：这是模型的运行日志</li>
</ol>
<p><a href="https://github.com/TexasInstruments/edgeai-benchmark">edgeai-benchmark</a>: Custom model benchmark can also be easily done (please refer to the documentation and example).<br>Uses <a href="https://github.com/TexasInstruments/edgeai-tidl-tools">edgeai-tidl-tools</a> for model compilation and inference</p>
<h1 id="网络结构的修改与适配"><a href="#网络结构的修改与适配" class="headerlink" title="网络结构的修改与适配"></a>网络结构的修改与适配</h1><p>edgeai-tidl-tools与edge ai studio的编译结果可以结合onnx模型在arm上运行，因此可以有不支持的网络层（有性能损失），但若使用TIDL Importer编译，则只能转换完全支持TIDL的网络结构，因此前期将网络中不支持的层替换是最好的，</p>
<p>此处以YOLOX的Backbone为例，修改不支持的层：slice, <del>Resize_206, Resize_229</del>(resize在version13不支持，11支持), MaxPool(在11只支持kernel sizes: 3x3,2x2,1x1)</p>
<p>TIDL支持的算子见：<a href="https://github.com/TexasInstruments/edgeai-tidl-tools/blob/master/docs/supported_ops_rts_versions.md">supported_ops_rts_versions</a><br>ONNX算子版本见：<a href="https://github.com/onnx/onnx/blob/main/docs/Operators.md">onnx&#x2F;docs&#x2F;Operators</a></p>
<table>
<thead>
<tr>
<th align="left">TIDL Layer Type</th>
<th align="left">ONNX Ops</th>
<th align="left">TFLite Ops</th>
<th align="left">Notes</th>
</tr>
</thead>
<tbody><tr>
<td align="left">TIDL_SliceLayer</td>
<td align="left">Split</td>
<td align="left">NA</td>
<td align="left">Only channel wise slice is supported</td>
</tr>
<tr>
<td align="left">TIDL_ResizeLayer</td>
<td align="left">UpSample</td>
<td align="left">RESIZE_NEAREST_NEIGHBOR</td>
<td align="left">RESIZE_BILINEAR	Only power of 2 and symmetric resize is supported</td>
</tr>
<tr>
<td align="left">TIDL_PoolingLayer</td>
<td align="left">MaxPool, AveragePool, GlobalAveragePool</td>
<td align="left">MAX_POOL_2D, AVERAGE_POOL_2D, MEAN</td>
<td align="left">Pooling has been validated for the following kernel sizes: 3x3,2x2,1x1, with a maximum stride of 2</td>
</tr>
</tbody></table>
<p>修改网络中三处不支持的层以支持TIDL：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">(<span class="number">1</span>,<span class="number">1</span>,<span class="number">256</span>,<span class="number">128</span>) --&gt; Slice + Concat --&gt; (<span class="number">1</span>,<span class="number">4</span>,<span class="number">128</span>,<span class="number">64</span>)</span><br><span class="line"><span class="comment">#Slice+Concat参照TI_YOLOX, 替换为Conv + Relu</span></span><br><span class="line"></span><br><span class="line">(<span class="number">1</span>,<span class="number">64</span>,<span class="number">8</span>,<span class="number">4</span>)  --&gt; Resize_206 --&gt; (<span class="number">1</span>,<span class="number">64</span>,<span class="number">16</span>,<span class="number">8</span>)</span><br><span class="line">(<span class="number">1</span>,<span class="number">32</span>,<span class="number">16</span>,<span class="number">8</span>) --&gt; Resize_229 --&gt; (<span class="number">1</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">16</span>)</span><br><span class="line"><span class="comment">#resize理论上支持，此处原因待排查</span></span><br><span class="line"><span class="comment">#原因是onnx转换时opset=13，应为opset=11，网络无需修改</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#opset vertion改为11后 MaxPool 需要拆分为 kernel=3的组合</span></span><br><span class="line">maxpool(k=<span class="number">5</span>, s=<span class="number">1</span>) -&gt; replaced <span class="keyword">with</span> two maxpool(k=<span class="number">3</span>,s=<span class="number">1</span>)</span><br><span class="line">maxpool(k=<span class="number">9</span>, s=<span class="number">1</span>) -&gt; replaced <span class="keyword">with</span> four maxpool(k=<span class="number">3</span>,s=<span class="number">1</span>)</span><br><span class="line">maxpool(k=<span class="number">13</span>, s=<span class="number">1</span>)-&gt; replaced <span class="keyword">with</span> six maxpool(k=<span class="number">3</span>,s=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>参考TI官方对YOLOx的更改 <a href="https://github.com/TexasInstruments/edgeai-yolox/blob/main/README_2d_od.md">edgeai-yolox&#x2F;README_2d_od</a>，将Slice替换为一个卷积层，再对MaxPool拆分，最后激活函数Silu替换为Relu，再重新训练，得到新模型，设为opset_version&#x3D;11重新导出onnx编译后，即可只生成2个bin文件（net+io），完全的支持tidl运行加速；</p>
<h1 id="ONNX模型转换及推理"><a href="#ONNX模型转换及推理" class="headerlink" title="ONNX模型转换及推理"></a>ONNX模型转换及推理</h1><p>使用<code>torch.onnx.export(model, input, &quot;XXX.onnx&quot;, verbose=False, export_params=True, opset_version=11)</code>得到 <code>.onnx</code>；</p>
<blockquote>
<p>注意要确保加载的模型是一个完整的PyTorch模型对象，而不是一个包含模型权重的字典, 否则会报错<code>&#39;dict&#39; object has no attribute &#39;modules&#39;</code>；<br>因此需要在项目保存<code>.pth</code>模型文件时设置同时<em>保存网络结构</em>，或者在项目代码中<em>导入完整模型</em>后使用<code>torch.onnx.export</code><br><strong>opset_version只支持到13</strong>，导出默认是14，会报错<br>opset_version为13时不支持resize, 现改为<strong>11</strong></p>
</blockquote>
<p>使用ONNX Runtime 运行推理，验证模型转换的正确性</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np    </span><br><span class="line"><span class="keyword">import</span> onnxruntime    </span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> onnx</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment">#导入模型和推理图片</span></span><br><span class="line">model_path = <span class="string">&quot;./XXX.onnx&quot;</span></span><br><span class="line">input_file=<span class="string">&quot;1.jpg&quot;</span></span><br><span class="line">session = onnxruntime.InferenceSession(model_path, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the name of the first input of the model</span></span><br><span class="line">input_name = session.get_inputs()[<span class="number">0</span>].name  </span><br><span class="line">input_details  = session.get_inputs()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Model input details:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> input_details:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line">output_details = session.get_outputs()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Model output details:&quot;</span>, )</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> output_details:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line"></span><br><span class="line">input_shape = input_details[<span class="number">0</span>].shape</span><br><span class="line">input_height, input_width = input_shape[<span class="number">2</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pre-Process input</span></span><br><span class="line">img_bgr = cv2.imread(input_file)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;image size:&quot;</span>, img_bgr.shape)</span><br><span class="line">img_bgr2 = cv2.resize(img_bgr, ( input_width,input_height))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;image resize:&quot;</span>, img_bgr2.shape)</span><br><span class="line">img_rgb = img_bgr2[:,:,::-<span class="number">1</span>]</span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"><span class="comment"># 预处理-归一化</span></span><br><span class="line">input_tensor = img_rgb / <span class="number">255</span>    <span class="comment"># 预处理-构造输入 Tensor</span></span><br><span class="line">input_tensor = np.expand_dims(input_tensor, axis=<span class="number">0</span>) <span class="comment"># 加 batch 维度</span></span><br><span class="line">input_tensor = input_tensor.transpose((<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)) <span class="comment"># N, C, H, W</span></span><br><span class="line">input_tensor = np.ascontiguousarray(input_tensor)   <span class="comment"># 将内存不连续存储的数组，转换为内存连续存储的数组，使得内存访问速度更快</span></span><br><span class="line">input_tensor = torch.from_numpy(input_tensor).to(device).<span class="built_in">float</span>() <span class="comment"># 转 Pytorch Tensor</span></span><br><span class="line">input_tensor = input_tensor[:, :<span class="number">1</span>, :, :]    <span class="comment">#[1, &quot;1&quot;, 384, 128]</span></span><br><span class="line"><span class="built_in">print</span>(input_tensor.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Run inference session</span></span><br><span class="line">raw_result = session.run([], &#123;input_name: input_tensor.numpy()&#125;)</span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> raw_result:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;result shape:&quot;</span>, result.shape)</span><br></pre></td></tr></table></figure>
<p><code>print(result)</code> :正常应该输出正确的推理结果，如果数值全都一样(-4.59512)，可能是没有检测到有效的目标或者模型效果太差</p>
<h1 id="TIDL-编译转换"><a href="#TIDL-编译转换" class="headerlink" title="TIDL 编译转换"></a>TIDL 编译转换</h1><p>得到onnx相关文件后，使用ti提供的工具进行编译和推理，这里采用三种不同的模型转换方法： </p>
<ul>
<li><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/06_01_01_12/exports/docs/tidl_j7_01_00_01_00/ti_dl/docs/user_guide_html/md_tidl_model_import.html">TIDL Importer</a> ：部署于EVM板，网络结构需要全部支持TIDL</li>
<li><a href="https://github.com/TexasInstruments/edgeai-tidl-tools/tree/08_06_00_05">Edgeai-tidl-tools</a>：可部署于SK板，需要onnx运行环境，配置灵活，网络结构要求少</li>
<li><a href="https://dev.ti.com/edgeaistudio/">Edge AI Studio</a>：TIDL tools的在线版本</li>
</ul>
<h2 id="TIDL-Importer"><a href="#TIDL-Importer" class="headerlink" title="TIDL Importer"></a>TIDL Importer</h2><p>TIDL Importer 是RTOS SDK中提供的导入工具，需要网络结构完全支持tidl，以使模型都通过tidl加速（即转换只生成net,io 2个bin文件）</p>
<p>下面的流程重构了文件夹架构，原文件跳来跳去改起来很麻烦，就合并到了XXX文件夹，原文件路径可参考上一篇官方例程： <a href="https://wangyujie.site/TDA4VM3/#a-%E4%BD%BF%E7%94%A8TIDL-Importer-by-RTOS-SDK">TDA4③_使用TIDL Importer导入YOLOX</a></p>
<ol>
<li><p>配置文件：新建文件夹：<code>SDK/$&#123;TIDL_INSTALL_PATH&#125;/ti_dl/test/testvecs/XXX</code><br> 拷贝 onnx 文件至 XXX 文件夹 (<em>此处是自定义模型，不使用prototxt, 经测试可以正常编译</em>)</p>
 <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#XXX文件夹结构</span></span><br><span class="line">├── detection_list.txt</span><br><span class="line">├── device_configs</span><br><span class="line">│   ├── am62a_config.cfg</span><br><span class="line">│   ├── j721e_config.cfg</span><br><span class="line">│   ├── j721s2_config.cfg</span><br><span class="line">│   └── j784s4_config.cfg</span><br><span class="line">├── output</span><br><span class="line">├── indata</span><br><span class="line">│   └── 1.jpg</span><br><span class="line">├── XXX.onnx</span><br><span class="line">└── tidl_import_XXX.txt</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写转换配置文件：新建<strong>tidl_import_XXX.txt</strong>，可参考同目录下其他例程，详细参数配置见<a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/06_01_01_12/exports/docs/tidl_j7_01_00_01_00/ti_dl/docs/user_guide_html/md_tidl_model_import.html">文档</a></p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#tidl_import_XXX.txt:</span></span><br><span class="line">modelType          = 2</span><br><span class="line">numParamBits       = 8</span><br><span class="line">numFeatureBits     = 8</span><br><span class="line">quantizationStyle  = 3</span><br><span class="line">inputNetFile       = <span class="string">&quot;../../test/testvecs/XXX/XXX_yolox_221_sig_11.onnx&quot;</span></span><br><span class="line">outputNetFile      = <span class="string">&quot;../../test/testvecs/XXX/output/825_tidl_net_sig_SDK8_6.bin&quot;</span></span><br><span class="line">outputParamsFile   = <span class="string">&quot;../../test/testvecs/XXX/output/825_tidl_io__sig_SDK8_6&quot;</span></span><br><span class="line">inDataNorm  = 1</span><br><span class="line">inMean = 0 0 0</span><br><span class="line">inScale = 0.003921568627 0.003921568627 0.003921568627</span><br><span class="line">inDataFormat = 1</span><br><span class="line">inWidth  = 128</span><br><span class="line">inHeight = 256 </span><br><span class="line">inNumChannels = 3</span><br><span class="line">numFrames = 50</span><br><span class="line">inData  =   <span class="string">&quot;../../test/testvecs/XXX/detection_list.txt&quot;</span></span><br><span class="line">perfSimConfig = ../../test/testvecs/XXX/device_configs/j721s2_config.cfg</span><br><span class="line">debugTraceLevel=1</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Debug:<br><code>inData</code>配置数据输入(回车分隔)，数量与<code>numFrames</code>要匹配；<br><code>perfSimConfig</code>选择对应设备的配置文件；<br><code>inScale</code>配置太大可能导致tensor不匹配<br><code>metaLayersNamesList</code>注释掉, 除非与TI提供的元架构相同；</p>
</blockquote>
</li>
<li><p>执行编译，得到可执行文件 <code>.bin</code></p>
 <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> TIDL_INSTALL_PATH=/home/wyj/sda2/TAD4VL_SKD_8_5/ti-processor-sdk-rtos-j721s2-evm-08_05_00_11/tidl_j721s2_08_05_00_16</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;TIDL_INSTALL_PATH&#125;</span>/ti_dl/utils/tidlModelImport</span><br><span class="line"></span><br><span class="line">./out/tidl_model_import.out <span class="variable">$&#123;TIDL_INSTALL_PATH&#125;</span>/ti_dl/test/testvecs/XXX/tidl_import_XXX.txt</span><br><span class="line"></span><br><span class="line"><span class="comment">#successful Memory allocation</span></span><br><span class="line"><span class="comment">#../../test/testvecs/XXX/output/生成的文件分析：</span></span><br><span class="line">tidl_net_XXX.bin        <span class="comment">#Compiled network file 网络模型数据</span></span><br><span class="line">tidl_io_XXX.bin         <span class="comment">#Compiled I/O file 网络输入配置文件</span></span><br><span class="line">tidl_net_XXX.bin.svg    <span class="comment">#tidlModelGraphviz tool生成的网络图</span></span><br><span class="line">tidl_out.png, tidl_out.txt  <span class="comment">#执行的目标检测测试结果</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>TIDL运行(inference)<br><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/06_01_01_12/exports/docs/tidl_j7_01_00_01_00/ti_dl/docs/user_guide_html/md_tidl_sample_test.html">TI Deep Learning Library User Guide: TIDL Inference</a></p>
 <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#在文件ti_dl/test/testvecs/config/config_list.txt顶部加入:</span></span><br><span class="line">1 testvecs/XXX/tidl_infer_XXX.txt</span><br><span class="line">0</span><br><span class="line"></span><br><span class="line"><span class="comment">#新建tidl_infer_yolox.txt:</span></span><br><span class="line">inFileFormat    = 2</span><br><span class="line">numFrames   = 10</span><br><span class="line">netBinFile      = <span class="string">&quot;testvecs/XXX/output/825_tidl_net_sig_SDK8_6.bin&quot;</span></span><br><span class="line">ioConfigFile   = <span class="string">&quot;testvecs/XXX/output/825_tidl_io_sig_SDK8_61.bin&quot;</span></span><br><span class="line">inData  =   testvecs/XXX/detection_list.txt</span><br><span class="line">outData =   testvecs/XXX/infer_out/inference.bin</span><br><span class="line">inResizeMode = 0</span><br><span class="line"><span class="comment">#0 : Disable, 1- Classification top 1 and 5 accuracy, 2 – Draw bounding box for OD, 3 - Pixel level color blending</span></span><br><span class="line">postProcType = 2</span><br><span class="line">debugTraceLevel = 1</span><br><span class="line">writeTraceLevel = 0</span><br><span class="line">writeOutput = 1</span><br><span class="line"></span><br><span class="line"><span class="comment">#运行，结果在ti_dl/test/testvecs/output/</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;TIDL_INSTALL_PATH&#125;</span>/ti_dl/test &amp;&amp; ./PC_dsp_test_dl_algo.out</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="Edge-AI-Studio"><a href="#Edge-AI-Studio" class="headerlink" title="Edge AI Studio"></a>Edge AI Studio</h2><p>参考yolox的编译过程：<a href="https://wangyujie.site/TDA4VM3/#b-%E4%BD%BF%E7%94%A8TIDL-Tools%EF%BC%88by-Edge-AI-Studio%EF%BC%89">YOLOX的模型转换与SK板端运行</a>，修改数据预处理与compile_options部分，最后重写画框部分（optional）</p>
<blockquote>
<p><strong>Debug:</strong><br><code>[ONNXRuntimeError] : 6 ... </code>: compile_options中设置deny_list，剔除不支持的层，如<code>&#39;Slice&#39;</code>，TIDL支持的算子见：<a href="https://github.com/TexasInstruments/edgeai-tidl-tools/blob/master/docs/supported_ops_rts_versions.md">supported_ops_rts_versions</a>    (resize支持2*操作)<br>compile_options中要注释掉object_detection的配置</p>
</blockquote>
<p>打包下载编译生成的工件：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Pack.ipynb</span></span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">zip_folder</span>(<span class="params">folder_path, zip_path</span>):</span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(zip_path, <span class="string">&#x27;w&#x27;</span>, zipfile.ZIP_DEFLATED) <span class="keyword">as</span> zipf:</span><br><span class="line">        <span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(folder_path):</span><br><span class="line">            <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">                file_path = os.path.join(root, file)</span><br><span class="line">                zipf.write(file_path, os.path.relpath(file_path, folder_path))</span><br><span class="line"></span><br><span class="line">folder_path = <span class="string">&#x27;./output&#x27;</span> <span class="comment"># 指定要下载的文件夹路径</span></span><br><span class="line">zip_path = <span class="string">&#x27;./output.zip&#x27;</span> <span class="comment"># 指定要保存的zip文件路径</span></span><br><span class="line">zip_folder(folder_path, zip_path)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> FileLink</span><br><span class="line">FileLink(zip_path) <span class="comment"># 生成下载链接</span></span><br></pre></td></tr></table></figure>

<h2 id="EdgeAI-TIDL-Tools"><a href="#EdgeAI-TIDL-Tools" class="headerlink" title="EdgeAI-TIDL-Tools"></a><a href="https://github.com/TexasInstruments/edgeai-tidl-tools/blob/08_06_00_05/docs/custom_model_evaluation.md">EdgeAI-TIDL-Tools</a></h2><p>环境搭建见：<a href="https://wangyujie.site/TDA4VM2/#EdgeAI-TIDL-Tools">TDA4②</a></p>
<p>研读 <a href="https://github.com/TexasInstruments/edgeai-tidl-tools/blob/08_06_00_05/examples/osrt_python/ort/onnxrt_ep.py">edgeai-tidl-tools&#x2F;examples&#x2F;osrt_python&#x2F;ort&#x2F;onnxrt_ep.py</a>:<br>进入搭建好的环境：（例）<code>pyenv activate benchmark</code> 或 <code>conda activate tidl</code><br>运行：<code>./scripts/run_python_examples.sh</code><br>下面基于例程进行基本的修改以编译运行自定义模型, 至少需要修改四个文件：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#新建运行脚本./script/run.sh</span></span><br><span class="line">CURDIR=`<span class="built_in">pwd</span>`</span><br><span class="line"><span class="built_in">export</span> SOC=am68pa</span><br><span class="line"><span class="built_in">export</span> TIDL_TOOLS_PATH=$(<span class="built_in">pwd</span>)/tidl_tools</span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$LD_LIBRARY_PATH</span>:<span class="variable">$TIDL_TOOLS_PATH</span></span><br><span class="line"><span class="built_in">export</span> ARM64_GCC_PATH=$(<span class="built_in">pwd</span>)/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu</span><br><span class="line">    <span class="built_in">cd</span> <span class="variable">$CURDIR</span>/examples/osrt_python/ort</span><br><span class="line">    <span class="comment">#python3 onnxrt_ep.py -c</span></span><br><span class="line">    python3 onnxrt_ep.py</span><br><span class="line">    <span class="comment">#python3 onnxrt_ep.py -d</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#修改examples/osrt_python/ort/onnxrt_ep.py</span></span><br><span class="line">def infer_image(sess, image_files, config): <span class="comment">#此处修改模型输入数据格式</span></span><br><span class="line">models = [<span class="string">&#x27;custom_model_name&#x27;</span>]  <span class="comment">#修改对应的模型名称</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#修改examples/osrt_python/model_configs.py 导入并配置模型</span></span><br><span class="line"><span class="comment">#onnx文件移入model/public文件夹</span></span><br><span class="line">    <span class="string">&#x27;custom_model_name&#x27;</span> : &#123;</span><br><span class="line">        <span class="string">&#x27;model_path&#x27;</span> : os.path.join(models_base_path, <span class="string">&#x27;custom_model_name.onnx&#x27;</span>),</span><br><span class="line">        <span class="string">&#x27;source&#x27;</span> : &#123;<span class="string">&#x27;model_url&#x27;</span>: <span class="string">&#x27;https..XXX./.onnx&#x27;</span>, <span class="string">&#x27;opt&#x27;</span>: True,  <span class="string">&#x27;infer_shape&#x27;</span> : True&#125;,</span><br><span class="line">        <span class="string">&#x27;mean&#x27;</span>: [123.675, 116.28, 103.53],</span><br><span class="line">        <span class="string">&#x27;scale&#x27;</span> : [0.017125, 0.017507, 0.017429],</span><br><span class="line">        <span class="string">&#x27;num_images&#x27;</span> : numImages,</span><br><span class="line">        <span class="string">&#x27;num_classes&#x27;</span>: 1000,</span><br><span class="line">        <span class="string">&#x27;session_name&#x27;</span> : <span class="string">&#x27;onnxrt&#x27;</span> ,</span><br><span class="line">        <span class="string">&#x27;model_type&#x27;</span>: <span class="string">&#x27;classification&#x27;</span></span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line"><span class="comment">#examples/osrt_python/common_utils.py 配置编译选项</span></span><br><span class="line"><span class="string">&quot;deny_list&quot;</span>:<span class="string">&quot;Slice&quot;</span>, <span class="comment">#&quot;MaxPool&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#运行编译</span></span><br><span class="line">./scripts/run.sh</span><br></pre></td></tr></table></figure>
<p>配置编译选项文档：<a href="https://github.com/TexasInstruments/edgeai-tidl-tools/blob/master/examples/osrt_python/README.md#optional-options">edgeai-tidl-tools&#x2F;examples&#x2F;osrt_python&#x2F;README.md</a></p>
<blockquote>
<p><strong>Debug:</strong><br>有些模型可能要到model_configs中找到链接手动下载放入models&#x2F;public<br><code>&#39;TIDLCompilationProvider&#39; is not in available:</code>环境问题，没有进入配置好的环境，正常应该是: <code>Available execution providers :  [&#39;TIDLExecutionProvider&#39;, &#39;TIDLCompilationProvider&#39;, &#39;CPUExecutionProvider&#39;]</code></p>
</blockquote>
<h3 id="onnxrt-ep-py详解"><a href="#onnxrt-ep-py详解" class="headerlink" title="onnxrt_ep.py详解"></a>onnxrt_ep.py详解</h3><p><a href="https://github.com/TexasInstruments/edgeai-tidl-tools/blob/08_06_00_05/examples/osrt_python/ort/onnxrt_ep.py">edgeai-tidl-tools&#x2F;examples&#x2F;osrt_python&#x2F;ort&#x2F;onnxrt_ep.py</a> 是主要运行文件，也是修改的最多的部分，因此梳理此处代码有助于理解<em>tidl编译和运行的全流程</em>。</p>
<blockquote>
<p><strong>Debug</strong>:<br>其中容易出问题的是预处理部分，image size不对很容易出问题。<br>替换的某些测试图片读不进去导致报错，<del>原理未知</del> 权限问题，sudo nautilus 右键属性更改读写权限</p>
</blockquote>
<details>
<summary>onnxrt_ep.py code</summary>

<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> onnxruntime <span class="keyword">as</span> rt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> PIL</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageFont, ImageDraw, ImageEnhance</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> platform</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> postprogress <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># directory reach, 获取当前目录和父目录</span></span><br><span class="line">current = os.path.dirname(os.path.realpath(__file__))</span><br><span class="line">parent = os.path.dirname(current)</span><br><span class="line">sys.path.append(parent)<span class="comment"># setting path，将父级目录路径添加到系统路径中，以供后续导入模块使用</span></span><br><span class="line"><span class="keyword">from</span> common_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> model_configs <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> postprogress <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译基本选项</span></span><br><span class="line">required_options = &#123;</span><br><span class="line"><span class="string">&quot;tidl_tools_path&quot;</span>:tidl_tools_path,</span><br><span class="line"><span class="string">&quot;artifacts_folder&quot;</span>:artifacts_folder</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser()  <span class="comment"># 实例化一个参数解析器</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;-c&#x27;</span>,<span class="string">&#x27;--compile&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;Run in Model compilation mode&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;-d&#x27;</span>,<span class="string">&#x27;--disable_offload&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>,  <span class="built_in">help</span>=<span class="string">&#x27;Disable offload to TIDL&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;-z&#x27;</span>,<span class="string">&#x27;--run_model_zoo&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>,  <span class="built_in">help</span>=<span class="string">&#x27;Run model zoo models&#x27;</span>)</span><br><span class="line">args = parser.parse_args()  <span class="comment"># 解析命令行参数</span></span><br><span class="line">os.environ[<span class="string">&quot;TIDL_RT_PERFSTATS&quot;</span>] = <span class="string">&quot;1&quot;</span>   <span class="comment"># 设置环境变量 TIDL_RT_PERFSTATS 的值为 &quot;1&quot;</span></span><br><span class="line"></span><br><span class="line">so = rt.SessionOptions()    <span class="comment"># 创建一个会话选项对象</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Available execution providers : &quot;</span>, rt.get_available_providers()) <span class="comment">#可用的执行单元</span></span><br><span class="line"><span class="comment">#编译用图片</span></span><br><span class="line">calib_images = [<span class="string">&#x27;../../../test_data/line_test_images.jpg&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;../../../test_data/line_test_images2.jpg&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;../../../test_data/line_test_images3.jpg&#x27;</span></span><br><span class="line">]</span><br><span class="line"><span class="comment">#测试用图片</span></span><br><span class="line">test_images =  [<span class="string">&#x27;../../../test_data/line_test_images.jpg&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;../../../test_data/line_test_images2.jpg&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;../../../test_data/line_test_images3.jpg&#x27;</span>] </span><br><span class="line"></span><br><span class="line">sem = multiprocessing.Semaphore(<span class="number">0</span>)  <span class="comment"># 创建</span></span><br><span class="line"><span class="keyword">if</span> platform.machine() == <span class="string">&#x27;aarch64&#x27;</span>: <span class="comment">#检查是否在板端</span></span><br><span class="line">    ncpus = <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    ncpus = os.cpu_count()</span><br><span class="line">idx = <span class="number">0</span></span><br><span class="line">nthreads = <span class="number">0</span></span><br><span class="line">run_count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="string">&quot;SOC&quot;</span> <span class="keyword">in</span> os.environ: <span class="comment">#检查是否设置了SOC环境变量，无则exit</span></span><br><span class="line">    SOC = os.environ[<span class="string">&quot;SOC&quot;</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Please export SOC var to proceed&quot;</span>)</span><br><span class="line">    exit(-<span class="number">1</span>)</span><br><span class="line"><span class="keyword">if</span> (platform.machine() == <span class="string">&#x27;aarch64&#x27;</span>  <span class="keyword">and</span> args.<span class="built_in">compile</span> == <span class="literal">True</span>): <span class="comment">#若在板端且需要编译，exit</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Compilation of models is only supported on x86 machine \n\</span></span><br><span class="line"><span class="string">        Please do the compilation on PC and copy artifacts for running on TIDL devices &quot;</span> )</span><br><span class="line">    exit(-<span class="number">1</span>)</span><br><span class="line"><span class="keyword">if</span>(SOC == <span class="string">&quot;am62&quot;</span>):</span><br><span class="line">    args.disable_offload = <span class="literal">True</span></span><br><span class="line">    args.<span class="built_in">compile</span> = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#计算benchmark</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_benchmark_output</span>(<span class="params">interpreter</span>):</span><br><span class="line">    benchmark_dict = interpreter.get_TI_benchmark_data()    <span class="comment"># 获取模型推理的统计数据字典</span></span><br><span class="line">    proc_time = copy_time = <span class="number">0</span></span><br><span class="line">    cp_in_time = cp_out_time = <span class="number">0</span></span><br><span class="line">    subgraphIds = []</span><br><span class="line">    <span class="keyword">for</span> stat <span class="keyword">in</span> benchmark_dict.keys():</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;proc_start&#x27;</span> <span class="keyword">in</span> stat:</span><br><span class="line">            value = stat.split(<span class="string">&quot;ts:subgraph_&quot;</span>)</span><br><span class="line">            value = value[<span class="number">1</span>].split(<span class="string">&quot;_proc_start&quot;</span>)</span><br><span class="line">            subgraphIds.append(value[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(subgraphIds)):        <span class="comment"># 计算处理时间、拷贝输入时间和拷贝输出时间</span></span><br><span class="line">        proc_time += benchmark_dict[<span class="string">&#x27;ts:subgraph_&#x27;</span>+<span class="built_in">str</span>(subgraphIds[i])+<span class="string">&#x27;_proc_end&#x27;</span>] - benchmark_dict[<span class="string">&#x27;ts:subgraph_&#x27;</span>+<span class="built_in">str</span>(subgraphIds[i])+<span class="string">&#x27;_proc_start&#x27;</span>]</span><br><span class="line">        cp_in_time += benchmark_dict[<span class="string">&#x27;ts:subgraph_&#x27;</span>+<span class="built_in">str</span>(subgraphIds[i])+<span class="string">&#x27;_copy_in_end&#x27;</span>] - benchmark_dict[<span class="string">&#x27;ts:subgraph_&#x27;</span>+<span class="built_in">str</span>(subgraphIds[i])+<span class="string">&#x27;_copy_in_start&#x27;</span>]</span><br><span class="line">        cp_out_time += benchmark_dict[<span class="string">&#x27;ts:subgraph_&#x27;</span>+<span class="built_in">str</span>(subgraphIds[i])+<span class="string">&#x27;_copy_out_end&#x27;</span>] - benchmark_dict[<span class="string">&#x27;ts:subgraph_&#x27;</span>+<span class="built_in">str</span>(subgraphIds[i])+<span class="string">&#x27;_copy_out_start&#x27;</span>]</span><br><span class="line">        copy_time += cp_in_time + cp_out_time</span><br><span class="line">    copy_time = copy_time <span class="keyword">if</span> <span class="built_in">len</span>(subgraphIds) == <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    totaltime = benchmark_dict[<span class="string">&#x27;ts:run_end&#x27;</span>] -  benchmark_dict[<span class="string">&#x27;ts:run_start&#x27;</span>]  <span class="comment">#计算总时间</span></span><br><span class="line">    <span class="keyword">return</span> copy_time, proc_time, totaltime</span><br><span class="line"></span><br><span class="line"><span class="comment">#图像预处理并推理</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">infer_image</span>(<span class="params">sess, image_files, config</span>):</span><br><span class="line">    input_details = sess.get_inputs()</span><br><span class="line">    input_name = input_details[<span class="number">0</span>].name</span><br><span class="line">    floating_model = (input_details[<span class="number">0</span>].<span class="built_in">type</span> == <span class="string">&#x27;tensor(float)&#x27;</span>)   <span class="comment"># 判断是否为浮点模型</span></span><br><span class="line">    height = input_details[<span class="number">0</span>].shape[<span class="number">2</span>]  <span class="comment">#384</span></span><br><span class="line">    width  = input_details[<span class="number">0</span>].shape[<span class="number">3</span>]  <span class="comment">#128</span></span><br><span class="line">    <span class="built_in">print</span>(image_files)</span><br><span class="line">    imgs=image_files</span><br><span class="line">    img_bgr = cv2.imread(image_files)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;image size:&quot;</span>, img_bgr.shape)</span><br><span class="line">    img_bgr2 = cv2.resize(img_bgr, ( width,height))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;image resize:&quot;</span>, img_bgr2.shape)</span><br><span class="line">    img_rgb = img_bgr2[:,:,::-<span class="number">1</span>]    <span class="comment">#(384, 128, 3)</span></span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="comment"># 预处理-归一化</span></span><br><span class="line">    input_tensor = img_rgb / <span class="number">255</span>    <span class="comment"># 预处理-构造输入 Tensor</span></span><br><span class="line">    input_tensor = np.expand_dims(input_tensor, axis=<span class="number">0</span>) <span class="comment"># 加 batch 维度 (1, 384, 128, 3)</span></span><br><span class="line">    input_tensor = input_tensor.transpose((<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)) <span class="comment"># N, C, H, W</span></span><br><span class="line">    input_tensor = np.ascontiguousarray(input_tensor)   <span class="comment"># 将内存不连续存储的数组，转换为内存连续存储的数组，使得内存访问速度更快</span></span><br><span class="line">    input_tensor = torch.from_numpy(input_tensor).to(device).<span class="built_in">float</span>() <span class="comment"># 转 Pytorch Tensor</span></span><br><span class="line">    input_data = input_tensor[:, :<span class="number">1</span>, :, :]    <span class="comment">#转单通道</span></span><br><span class="line">    <span class="built_in">print</span>(input_data.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#推理图片，计时</span></span><br><span class="line">    start_time = time.time()  <span class="comment"># 记录开始时间</span></span><br><span class="line">    output = <span class="built_in">list</span>(sess.run(<span class="literal">None</span>, &#123;input_name: input_data.numpy()&#125;))  <span class="comment"># 进行推理并获取输出结果</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;output.shape:&quot;</span>, output[<span class="number">0</span>].shape)</span><br><span class="line">    stop_time = time.time()</span><br><span class="line">    infer_time = stop_time - start_time  <span class="comment"># 计算推理时间</span></span><br><span class="line">    <span class="comment"># 获取拷贝时间、子图处理时间和总时间</span></span><br><span class="line">    copy_time, sub_graphs_proc_time, totaltime = get_benchmark_output(sess)</span><br><span class="line">    proc_time = totaltime - copy_time</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> imgs, output, proc_time, sub_graphs_proc_time, height, width</span><br><span class="line"></span><br><span class="line"><span class="comment">#main 主程序####################################################################</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_model</span>(<span class="params">model, mIdx</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nRunning_Model : &quot;</span>, model, <span class="string">&quot; \n&quot;</span>)</span><br><span class="line">    config = models_configs[model]</span><br><span class="line">    <span class="comment"># 将编译配置更新到 delegate_options 中</span></span><br><span class="line">    delegate_options = &#123;&#125;</span><br><span class="line">    delegate_options.update(required_options)</span><br><span class="line">    delegate_options.update(optional_options)   </span><br><span class="line">    <span class="comment">#   拼接 &quot;artifacts_folder&quot; 的路径，将 model 名称添加到文件夹路径中</span></span><br><span class="line">    delegate_options[<span class="string">&#x27;artifacts_folder&#x27;</span>] = delegate_options[<span class="string">&#x27;artifacts_folder&#x27;</span>] + <span class="string">&#x27;/&#x27;</span> + model + <span class="string">&#x27;/&#x27;</span> <span class="comment">#+ &#x27;tempDir/&#x27; </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># delete the contents of this folder</span></span><br><span class="line">    <span class="keyword">if</span> args.<span class="built_in">compile</span> <span class="keyword">or</span> args.disable_offload:    <span class="comment"># 如果命令行参数中有 --compile 或 --disable_offload</span></span><br><span class="line">        os.makedirs(delegate_options[<span class="string">&#x27;artifacts_folder&#x27;</span>], exist_ok=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(delegate_options[<span class="string">&#x27;artifacts_folder&#x27;</span>], topdown=<span class="literal">False</span>):</span><br><span class="line">            [os.remove(os.path.join(root, f)) <span class="keyword">for</span> f <span class="keyword">in</span> files]</span><br><span class="line">            [os.rmdir(os.path.join(root, d)) <span class="keyword">for</span> d <span class="keyword">in</span> dirs]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#编译和测试选不同的数据集</span></span><br><span class="line">    <span class="keyword">if</span>(args.<span class="built_in">compile</span> == <span class="literal">True</span>):   <span class="comment"># 如果参数中存在 --compile</span></span><br><span class="line">        input_image = calib_images</span><br><span class="line">        <span class="keyword">import</span> onnx</span><br><span class="line">        log = <span class="string">f&#x27;\nRunning shape inference on model <span class="subst">&#123;config[<span class="string">&quot;model_path&quot;</span>]&#125;</span> \n&#x27;</span></span><br><span class="line">        <span class="built_in">print</span>(log)</span><br><span class="line">        onnx.shape_inference.infer_shapes_path(config[<span class="string">&#x27;model_path&#x27;</span>], config[<span class="string">&#x27;model_path&#x27;</span>])  <span class="comment"># 根据校准图像执行形状推断</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        input_image = test_images</span><br><span class="line">    numFrames = config[<span class="string">&#x27;num_images&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span>(args.<span class="built_in">compile</span>):   <span class="comment"># 如果 numFrames 大于校准帧数，则将其设置为校准帧数</span></span><br><span class="line">        <span class="keyword">if</span> numFrames &gt; delegate_options[<span class="string">&#x27;advanced_options:calibration_frames&#x27;</span>]:</span><br><span class="line">            numFrames = delegate_options[<span class="string">&#x27;advanced_options:calibration_frames&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">############   set interpreter  ################################</span></span><br><span class="line">    <span class="comment">#根据不同的命令行参数选择不同的解释器</span></span><br><span class="line">    <span class="keyword">if</span> args.disable_offload : </span><br><span class="line">        EP_list = [<span class="string">&#x27;CPUExecutionProvider&#x27;</span>]</span><br><span class="line">        sess = rt.InferenceSession(config[<span class="string">&#x27;model_path&#x27;</span>] , providers=EP_list,sess_options=so)</span><br><span class="line">    <span class="keyword">elif</span> args.<span class="built_in">compile</span>:</span><br><span class="line">        EP_list = [<span class="string">&#x27;TIDLCompilationProvider&#x27;</span>,<span class="string">&#x27;CPUExecutionProvider&#x27;</span>]</span><br><span class="line">        sess = rt.InferenceSession(config[<span class="string">&#x27;model_path&#x27;</span>] ,providers=EP_list, provider_options=[delegate_options, &#123;&#125;], sess_options=so)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        EP_list = [<span class="string">&#x27;TIDLExecutionProvider&#x27;</span>,<span class="string">&#x27;CPUExecutionProvider&#x27;</span>]</span><br><span class="line">        sess = rt.InferenceSession(config[<span class="string">&#x27;model_path&#x27;</span>] ,providers=EP_list, provider_options=[delegate_options, &#123;&#125;], sess_options=so)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">############  run  session  ############################</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(input_image)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;-----------image:&quot;</span>, i, <span class="string">&quot;-----------&quot;</span>)</span><br><span class="line">        input_images=input_image[i]</span><br><span class="line">        <span class="comment"># 运行推断函数，获取输出结果，处理时间和子图时间，以及高度和宽度</span></span><br><span class="line">        imgs, output, proc_time, sub_graph_time, height, width  = infer_image(sess, input_images, config)</span><br><span class="line">        <span class="comment"># 计算总处理时间和子图时间</span></span><br><span class="line">        total_proc_time = total_proc_time + proc_time <span class="keyword">if</span> (<span class="string">&#x27;total_proc_time&#x27;</span> <span class="keyword">in</span> <span class="built_in">locals</span>()) <span class="keyword">else</span> proc_time</span><br><span class="line">        sub_graphs_time = sub_graphs_time + sub_graph_time <span class="keyword">if</span> (<span class="string">&#x27;sub_graphs_time&#x27;</span> <span class="keyword">in</span> <span class="built_in">locals</span>()) <span class="keyword">else</span> sub_graph_time</span><br><span class="line">        total_proc_time = total_proc_time /<span class="number">1000000</span></span><br><span class="line">        sub_graphs_time = sub_graphs_time/<span class="number">1000000</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># output post processing</span></span><br><span class="line">        <span class="keyword">if</span>(args.<span class="built_in">compile</span> == <span class="literal">False</span>):  <span class="comment"># post processing enabled only for inference, 如果不是编译模式，则执行后处理</span></span><br><span class="line">            output = deploy_preprocess(output[<span class="number">0</span>])   <span class="comment">#获取推理结果并进行处理</span></span><br><span class="line">            <span class="comment">#print(output)</span></span><br><span class="line">            pred_points = get_predicted_points(output[<span class="number">0</span>])   <span class="comment">#得到预测点位</span></span><br><span class="line">            <span class="built_in">print</span>(pred_points)</span><br><span class="line">            eval_results = &#123;&#125;</span><br><span class="line">            eval_results[<span class="string">&#x27;pred_points&#x27;</span>] = pred_points</span><br><span class="line">            img = cv2.imread(input_images)  <span class="comment">#导入图片用来画线</span></span><br><span class="line">            img_plot = plot_slots(img, eval_results)    <span class="comment">#画线</span></span><br><span class="line">            cv2.imshow(<span class="string">&#x27;XXX&#x27;</span>, img_plot)    <span class="comment">#显示结果</span></span><br><span class="line">            key = cv2.waitKey(<span class="number">1000</span>) &amp; <span class="number">0xFF</span></span><br><span class="line">            cv2.destroyAllWindows()</span><br><span class="line">            save_path = os.path.join(<span class="string">&#x27;../../../output_images&#x27;</span>,<span class="string">&#x27;test_image&#x27;</span>+<span class="built_in">str</span>(i+<span class="number">1</span>)+<span class="string">&#x27;.jpg&#x27;</span>) <span class="comment">#保存路径</span></span><br><span class="line">            cv2.imencode(<span class="string">&#x27;.jpg&#x27;</span>, img_plot)[<span class="number">1</span>].tofile(save_path)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> args.<span class="built_in">compile</span> <span class="keyword">or</span> args.disable_offload :   <span class="comment"># 如果是编译模式或者禁用了offload，则生成参数YAML文件</span></span><br><span class="line">            gen_param_yaml(delegate_options[<span class="string">&#x27;artifacts_folder&#x27;</span>], config, <span class="built_in">int</span>(height), <span class="built_in">int</span>(width))</span><br><span class="line">        log = <span class="string">f&#x27;\n \nCompleted_Model : <span class="subst">&#123;mIdx+<span class="number">1</span>:5d&#125;</span>, Name : <span class="subst">&#123;model:50s&#125;</span>, Total time : <span class="subst">&#123;total_proc_time/(i+<span class="number">1</span>):<span class="number">10.2</span>f&#125;</span>, Offload Time : <span class="subst">&#123;sub_graphs_time/(i+<span class="number">1</span>):<span class="number">10.2</span>f&#125;</span> , DDR RW MBs : 0\n \n &#x27;</span> <span class="comment">#&#123;classes&#125; \n \n&#x27;</span></span><br><span class="line">        <span class="built_in">print</span>(log)  <span class="comment"># 打印日志信息</span></span><br><span class="line">        <span class="keyword">if</span> ncpus &gt; <span class="number">1</span>:   <span class="comment"># 如果使用了多个CPU，则释放信号量</span></span><br><span class="line">            sem.release()</span><br><span class="line"></span><br><span class="line">models = [<span class="string">&#x27;XXX_yolox&#x27;</span>]</span><br><span class="line">log = <span class="string">f&#x27;\nRunning <span class="subst">&#123;<span class="built_in">len</span>(models)&#125;</span> Models - <span class="subst">&#123;models&#125;</span>\n&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(log)</span><br><span class="line"></span><br><span class="line"><span class="comment">#以下为线程控制，由此处进入运行程序&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">join_one</span>(<span class="params">nthreads</span>): <span class="comment"># 定义一个函数来加入一个线程</span></span><br><span class="line">    <span class="keyword">global</span> run_count</span><br><span class="line">    sem.acquire()     <span class="comment"># 获取一个信号量，控制线程同步</span></span><br><span class="line">    run_count = run_count + <span class="number">1</span>   <span class="comment"># 增加运行计数</span></span><br><span class="line">    <span class="keyword">return</span> nthreads - <span class="number">1</span> <span class="comment"># 返回线程数减1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">spawn_one</span>(<span class="params">models, idx, nthreads</span>):   <span class="comment"># 定义一个函数来创建并启动一个线程</span></span><br><span class="line">    <span class="comment"># 创建一个新的进程，目标函数是 run_model，参数是 models 和 idx</span></span><br><span class="line">    p = multiprocessing.Process(target=run_model, args=(models,idx,))</span><br><span class="line">    p.start()   <span class="comment"># 启动进程</span></span><br><span class="line">    <span class="keyword">return</span> idx + <span class="number">1</span>, nthreads + <span class="number">1</span>    <span class="comment"># 返回新的 idx 和 nthreads</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ncpus &gt; <span class="number">1</span>:   <span class="comment"># 如果有多个CPU，则创建并启动多个线程</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">min</span>(<span class="built_in">len</span>(models), ncpus)):</span><br><span class="line">        idx, nthreads = spawn_one(models[idx], idx, nthreads)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> idx &lt; <span class="built_in">len</span>(models):     <span class="comment"># 当还有未处理的 model 时, 等待一个线程完成，并减少线程数</span></span><br><span class="line">        nthreads = join_one(nthreads)</span><br><span class="line">        idx, nthreads = spawn_one(models[idx], idx, nthreads)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(nthreads):</span><br><span class="line">        nthreads = join_one(nthreads)</span><br><span class="line"><span class="keyword">else</span> : <span class="comment">#如果只有一个CPU：使用一个循环顺序地处理每个模型。每个模型会直接调用run_model函数进行处理。</span></span><br><span class="line">    <span class="keyword">for</span> mIdx, model <span class="keyword">in</span> <span class="built_in">enumerate</span>(models):</span><br><span class="line">        run_model(model, mIdx)</span><br></pre></td></tr></table></figure>
</details>


<h2 id="model-artifacts"><a href="#model-artifacts" class="headerlink" title="model-artifacts"></a>model-artifacts</h2><p>分析编译深度学习模型后生成的文件：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">└── model-artifacts         <span class="comment">#文件都是以最后的输出层命名，分为四块网络结构</span></span><br><span class="line">    ├── <span class="number">1</span>102_tidl_io_1.<span class="built_in">bin</span>  <span class="comment">#io配置文件</span></span><br><span class="line">    ├── <span class="number">1</span>102_tidl_net.<span class="built_in">bin</span>   <span class="comment">#网络模型的二进制文件</span></span><br><span class="line">    ├── allowedNode.txt     <span class="comment">#允许的节点列表文件</span></span><br><span class="line">    ├── onnxrtMetaData.txt  <span class="comment">#ONNX运行时的元数据文件</span></span><br><span class="line">    ├── param.yaml          <span class="comment">#参数配置文件</span></span><br><span class="line">    ├── XXX_yolox.onnx      <span class="comment">#深度学习模型的原始ONNX文件</span></span><br><span class="line">    └── tempDir             <span class="comment">#模型编译过程的临时文件和输出文件</span></span><br><span class="line">        ├── <span class="number">1</span>102_calib_raw_data.<span class="built_in">bin</span> <span class="comment">#用于校准的原始数据文件</span></span><br><span class="line">        ├── <span class="number">1</span>102_tidl_io_1.<span class="built_in">bin</span>      <span class="comment">#输入数据的二进制文件</span></span><br><span class="line">        ├── <span class="number">1</span>102_tidl_io__LayerPerChannelMean.<span class="built_in">bin</span>   <span class="comment">#存储每个通道的平均值的二进制文件。对于量化和归一化操作，需要存储每个通道的平均值。</span></span><br><span class="line">        ├── <span class="number">1</span>102_tidl_io_.perf_sim_config.txt   <span class="comment">#性能模拟的配置文件</span></span><br><span class="line">        ├── <span class="number">1</span>102_tidl_io_.qunat_stats_config.txt    <span class="comment">#量化统计的配置文件</span></span><br><span class="line">        ├── <span class="number">1</span>102_tidl_io__stats_tool_out.<span class="built_in">bin</span>    <span class="comment">#输出二进制文件。用于存储进行量化统计时的一些中间结果。</span></span><br><span class="line">        ├── <span class="number">1</span>102_tidl_net       <span class="comment">#编译后的深度学习模型相关文件</span></span><br><span class="line">        │   ├── bufinfolog.csv  <span class="comment">#缓冲区信息的CSV文件，可能包含模型各个层的输入和输出缓冲区的大小和信息。</span></span><br><span class="line">        │   ├── bufinfolog.txt  <span class="comment">#缓冲区信息的文本文件</span></span><br><span class="line">        │   └── perfSimInfo.<span class="built_in">bin</span> <span class="comment">#性能模拟信息的二进制文件。可能包含模型在性能模拟时的一些统计数据。</span></span><br><span class="line">        ├── <span class="number">1</span>102_tidl_net.<span class="built_in">bin</span></span><br><span class="line">        ├── <span class="number">1</span>102_tidl_net.bin_netLog.txt        <span class="comment">#模型编译日志的文本文件</span></span><br><span class="line">        │   ├── <span class="comment">#TIDL Layer Name, Out Data Name, Group, #Ins, #Outs</span></span><br><span class="line">        │   ├── <span class="comment">#Inbuf Ids, Outbuf Id: 输入输出缓冲区的标识符， In NCHW, Out NCHW: 输入输出数据的格式和维度信息</span></span><br><span class="line">        │   └── <span class="comment">#MACS: 模型在推理过程中进行的乘加运算，用于衡量模型的计算量和复杂度。</span></span><br><span class="line">        ├── <span class="number">1</span>102_tidl_net.bin_paramDebug.csv    <span class="comment">#包含模型参数的调试信息的CSV文件。记录了每个层量化前后的参数差异，模型通常以浮点数形式进行训练，量化通常将浮点参数转换为固定位数的整数参数。</span></span><br><span class="line">        │   ├── <span class="comment">#meanDifference: 参数的平均差异，maxDifference: 参数的最大差异，</span></span><br><span class="line">        │   ├── <span class="comment">#meanOrigFloat: 原始浮点参数的平均值，meanRelDifference: 参数的相对平均差异，</span></span><br><span class="line">        │   ├── <span class="comment">#orgmax: 原始浮点参数的最大值，quantizedMax: 量化后参数的最大值</span></span><br><span class="line">        │   ├── <span class="comment">#orgAtmaxDiff: 原始浮点参数在最大值处的差异，quantizedAtMaxDiff: 量化后参数在最大值处的差异，maxRelDifference: 参数的最大相对差异</span></span><br><span class="line">        │   └── <span class="comment">#Scale: 参数的缩放比例，在量化中，使用缩放因子将浮点参数映射到整数参数；</span></span><br><span class="line">        ├── <span class="number">1</span>102_tidl_net.<span class="built_in">bin</span>.layer_info.txt    <span class="comment">#包含模型各个层信息的文本文件</span></span><br><span class="line">        ├── <span class="number">1</span>102_tidl_net.<span class="built_in">bin</span>.svg   <span class="comment">#该部分模型结构的可视化图像文件</span></span><br><span class="line">        ├── graphvizInfo.txt    <span class="comment">#模型结构的图形化文本信息</span></span><br><span class="line">        └── runtimes_visualization.svg  <span class="comment">#整个网络结构可视化文件</span></span><br></pre></td></tr></table></figure>

<p><strong>为什么有些网络结构编译后被拆分成了多组不同的二进制文件？</strong>（<em>4 subgraph output nodes</em>）: 多网络结构文件拼接成一个完整的网络，但由于不支持的层被offload到arm端运行，因此在相应的位置被拆分，前期网络结构设计时需要尽量避免出现该情况。</p>
<h1 id="TIDL-tools-c-推理-ongoing"><a href="#TIDL-tools-c-推理-ongoing" class="headerlink" title="TIDL tools c++推理(ongoing)"></a>TIDL tools c++推理(ongoing)</h1><p>TIDL runtime 提供的CPP api解决方案仅支持模型推理，因此仍需在PC上运行Python示例以生成模型工件。<br><a href="https://github.com/TexasInstruments/edgeai-tidl-tools/tree/master/examples/osrt_cpp">edgeai-tidl-tools&#x2F;examples&#x2F;osrt_cpp</a></p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> SOC=am68pa</span><br><span class="line"><span class="built_in">mkdir</span> build2 &amp;&amp; <span class="built_in">cd</span> build2</span><br><span class="line">cmake -DFLAG1=val -DFLAG2=val ../../../examples</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ongoing....</span><br></pre></td></tr></table></figure>

<h1 id="SK板运行自定义深度学习模型-ongoing"><a href="#SK板运行自定义深度学习模型-ongoing" class="headerlink" title="SK板运行自定义深度学习模型(ongoing)"></a>SK板运行自定义深度学习模型(ongoing)</h1><p>通过SD卡配置编译生成的模型：</p>
<blockquote>
<p>配置模型文件夹 custom_model 放入&#x2F;opt&#x2F;modelzoo文件夹</p>
<blockquote>
<p>artifacts：存放编译生成的工件，model-artifacts<br>model：原onnx模型，.onnx (.prototxt)<br>param.yaml：配置文件, 其中需要修改model_path等参数 (以modelzoo中例程的param为基准，参照model-artifacts中生成的param修改参数)<br>(dataset.yaml：数据集类别对应文件)</p>
</blockquote>
</blockquote>
<p>通过SD卡配置<code>/opt/edgeai-gst-apps/configs/XXX.yaml</code>，在model参数中索引上面建立的模型文件夹 custom_model, 并根据size修改输入输出，分辨率size一定要改好，否则很容易报错</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#通过minicom连接串口</span></span><br><span class="line">sudo minicom -D /dev/ttyUSB2 -c on</span><br><span class="line">root <span class="comment">#登录</span></span><br><span class="line"><span class="comment">#运行自定义实例</span></span><br><span class="line"><span class="built_in">cd</span> /opt/edgeai-gst-apps/apps_cpp</span><br><span class="line">./bin/Release/app_edgeai ../configs/XXX.yaml</span><br><span class="line"><span class="comment">#Ctrl+C 安全退出</span></span><br></pre></td></tr></table></figure>

<p>如果不是常规的OD、SEG等任务，需要高度自定义的话，需要修改SK板 <code>/opt/edgeai-gst-apps</code> DEMO相关的源码，主要阅读源码并参考两大文档：<br><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-linux-edgeai/TDA4VM/08_06_01/exports/docs/common/sample_apps.html">Edge AI sample apps &mdash; Processor SDK Linux for SK-TDA4VM Documentation</a><br><a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-linux-sk-tda4vm/latest/exports/docs/running_simple_demos.html">Running Simple demos &mdash; Processor SDK Linux for Edge AI Documentation</a><br>下面对demo源码进行研读：</p>
<hr>
<p>也许可以将tidl-tools放到板子里然后运行? 然后选择正确的平台<br>&#x2F;opt&#x2F;vision_apps&#x2F;vx_app_arm_remote_log.out 查arm log的脚本</p>
<hr>
<p>YOLO-pose实例：<a href="https://www.hackster.io/whitney-knitter/practicing-yoga-with-ai-human-pose-estimation-on-the-tda4vm-fe2549?auth_token=68e0af8f809985238fdb2b7554c48a46">Practicing Yoga with AI: Human Pose Estimation on the TDA4VM</a><br>官方视频：<a href="https://www.ti.com/video/6286792047001">Efficient object detection using Yolov5 and TDA4x processors | Video | TI.com</a><br>官方文档：<a href="https://software-dl.ti.com/jacinto7/esd/processor-sdk-linux-edgeai/TDA4VM/08_06_01/exports/docs/common/inference_models.html">4. Deep learning models &mdash; Processor SDK Linux for SK-TDA4VM Documentation</a></p>
<blockquote>
<p>TDA4系列文章：<br><a href="https://wangyujie.site/TDA4VM/">TDA4①：SDK, TIDL, OpenVX</a><br><a href="https://wangyujie.site/TDA4VM2/">TDA4②：环境搭建、模型转换、Demo及Tools</a><br><a href="https://wangyujie.site/TDA4VM3/">TDA4③：YOLOX的模型转换与SK板端运行</a><br><a href="https://wangyujie.site/TDA4VM4/">TDA4④：部署自定义深度学习模型</a></p>
</blockquote>
]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>嵌入式</tag>
      </tags>
  </entry>
  <entry>
    <title>Yolo：Code，Config，Ideas</title>
    <url>/Yolo/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>记录了YOLO的环境配置、资料代码、魔改记录、炼丹经验、论文想法。</p>
<span id="more"></span>

<h1 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h1><p>详细使用见 <a href="https://wangyujie.site/Pytorch/">Anaconda，Pycharm，Jupyter，Pytorch</a></p>
<p><strong>Windows环境配置</strong></p>
<ol>
<li><p>安装<a href="https://www.anaconda.com/"><strong>Anaconda</strong></a>，<br>防止环境装在C盘占空间：修改user目录下.condarc文件里的默认地址，或执行<code>conda config --add D:\Anaconda3\envs </code>,然后<code>conda info</code> 检查envs directories<br>（若报错 The channel is not accessible or is invalid 运行<code>conda config --remove-key channels</code>）</p>
</li>
<li><p><strong>配置环境</strong>：打开Anaconda Prompt<br>创建环境<code>conda create -n pytorch python=3.8</code><br>激活环境<code>conda activate pytorch</code></p>
</li>
<li><p>安装显卡驱动对应的<strong>CUDA</strong>：<code>nvidia-smi</code> 查询支持CUDA版本，<br>再到<a href="https://pytorch.org/get-started/locally/">Pytorch官网</a>复制对应code进行安装, 如：<br><code>conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia</code><br>（验证torch能否使用GPU：<code>python -c &quot;import torch;print(torch.cuda.is_available())&quot;</code>   返回True说明GPU可以被使用）</p>
</li>
<li><p>安装<a href="https://www.jetbrains.com/pycharm/"><strong>Pychram</strong></a>, 用pycharm打开YOLO项目文件夹，配置编辑器<code>D:P\Anaconda3\envs\pytorch\python.exe</code>，在pycharm的terminal中打开pytorch环境</p>
</li>
<li><p>安装各种<strong>包</strong>：<code>pip install -r requirements.txt</code>,<br>换源补装失败的包<code>pip install opencv-python -i https://pypi.tuna.tsinghua.edu.cn/simple/</code></p>
</li>
</ol>
<p><strong>Linux 环境配置</strong></p>
<ol>
<li><p>安装miniconda，相较Anaconda更小巧快捷，功能一样</p>
 <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</span><br><span class="line">bash Miniconda3-latest-Linux-x86_64.sh</span><br><span class="line"><span class="comment">#一路Enter + Yes，最后使修改的PATH环境变量生效：</span></span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br><span class="line">conda   <span class="comment">#验证是否成功</span></span><br><span class="line">conda create -n pytorch python=3.6  <span class="comment">#创建一个名为pytorch的环境</span></span><br><span class="line">conda activate pytorch  <span class="comment">#激活环境</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>（若要安装：<a href="https://www.anaconda.com/">Anaconda</a>，执行下载的.sh文件，输入<code>bash XXX.sh</code>，然后一路enter和yes；激活：<code>cd ///root/anaconda3/bin</code>,输入：<code>source ./activate</code>，终端前出现<code>(base)</code>则激活成功）</p>
</blockquote>
</li>
<li><p>下载pycharm，解压，进入bin文件夹，运行<code>./pycharm.sh</code>以打开pycharm（更简单且能生成图标的方法：<code>sudo snap install pycharm-community --classic</code>）<br>在项目中导入环境<code>.conda/envs/pytorch/bin/python3.6</code></p>
</li>
<li><p>安装CUDA</p>
<ul>
<li><strong>pytorch</strong><br> <code>conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia</code></li>
<li><strong>tensorflow</strong><br> cuda:<code>conda install cudatoolkit=10.0</code><br> cuDNN:<code>conda install cudnn=7.6</code><br> tf:<code>pip install tensorflow-gpu==1.15.0</code>(注意版本匹配)</li>
</ul>
</li>
</ol>
<p>如果requirements中有包实在安不上，手动装包：进<a href="https://pypi.org/">网站</a>搜索包，下载.whl，在包所在位置激活环境运行<code>pip install [].whl</code>(包名中cp38代表python3.8版本)</p>
<h1 id="资料与代码"><a href="#资料与代码" class="headerlink" title="资料与代码"></a>资料与代码</h1><table>
<thead>
<tr>
<th>Model</th>
<th>Paper</th>
<th>Code</th>
</tr>
</thead>
<tbody><tr>
<td>YOLOv1</td>
<td><a href="https://arxiv.org/pdf/1506.02640.pdf">You Only Look Once:Unified, Real-Time Object Detection</a></td>
<td><a href="https://pjreddie.com/darknet/yolov1/">Code</a></td>
</tr>
<tr>
<td>YOLOv2</td>
<td><a href="https://arxiv.org/pdf/1612.08242.pdf">YOLO9000:Better, Faster, Stronger</a></td>
<td><a href="https://pjreddie.com/darknet/yolo/">Code</a></td>
</tr>
<tr>
<td>YOLOv3</td>
<td><a href="https://arxiv.org/pdf/1804.02767.pdf">YOLOv3: An Incremental Improvement</a></td>
<td><a href="https://github.com/ultralytics/yolov3">Code</a></td>
</tr>
<tr>
<td>YOLOv4</td>
<td><a href="https://arxiv.org/pdf/2004.10934.pdf">YOLOv4: Optimal Speed and Accuracy of Object Detection</a></td>
<td><a href="https://github.com/Tianxiaomo/pytorch-YOLOv4">Code</a></td>
</tr>
<tr>
<td>YOLOv5</td>
<td>&#x2F;</td>
<td><a href="https://github.com/ultralytics/yolov5">Code</a></td>
</tr>
<tr>
<td>YOLOv6</td>
<td><a href="https://arxiv.org/pdf/2209.02976.pdf">YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications</a></td>
<td><a href="https://github.com/meituan/YOLOv6">Code</a></td>
</tr>
<tr>
<td>YOLOv7</td>
<td><a href="https://arxiv.org/abs/2207.02696">YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</a></td>
<td><a href="https://github.com/WongKinYiu/yolov7">Code</a></td>
</tr>
<tr>
<td>YOLOv8</td>
<td>&#x2F;</td>
<td><a href="https://github.com/ultralytics/ultralytics">Code</a></td>
</tr>
<tr>
<td>CEAM-YOLOv7</td>
<td><a href="https://ieeexplore.ieee.org/document/9980374/metrics">CEAM-YOLOv7:Improved YOLOv7 Based on Channel Expansion Attention Mechanism for Driver behavior detection</a></td>
<td><a href="https://github.com/Arrowes/CEAM-YOLOv7">Code</a></td>
</tr>
<tr>
<td>FEY-YOLOv7</td>
<td><a href="https://search.ieice.org/bin/summary_advpub.php?id=2023EDP7093&category=D&lang=E&abst=">A Driver Fatigue Detection Algorithm Based on Dynamic Tracking of Small Facial Targets Using YOLOv7</a></td>
<td><a href="https://github.com/Arrowes/FEY-YOLOv7">Code</a></td>
</tr>
</tbody></table>
<p>YOLOv1 - v5历程：<a href="https://blog.csdn.net/wjinjie/article/details/107509243">从yolov1至yolov5的进阶之路</a><br>YOLOv3论文精读视频：<a href="https://www.bilibili.com/video/BV1Vg411V7bJ/">同济子豪兄YOLOV3目标检测</a><br>YOLOv5知识精讲：<a href="https://zhuanlan.zhihu.com/p/172121380">Yolov5核心基础知识完整讲解</a>，<a href="https://blog.csdn.net/qq_38253797/article/details/119043919">YOLOV5-5.x 源码讲解</a><br>YOLOv7网络结构：<a href="https://blog.csdn.net/athrunsunny/article/details/125951001">理解yolov7网络结构</a> ,<a href="https://blog.csdn.net/u010899190/article/details/125883770">Yolov7 基础网络结构详解</a><br>全流程指导视频：<a href="https://www.bilibili.com/video/BV1tf4y1t7ru/">目标检测 YOLOv5 开源代码项目调试与讲解实战-土堆</a></p>
<p>算法精品仓库：<a href="https://github.com/bubbliiiing">Bubbliiiing</a><br><a href="https://github.com/iscyy/yoloair">YOLO Air</a>，<a href="https://github.com/iscyy/yoloair2">YOLO Air2</a><br><a href="https://github.com/positive666/yolov5_research">yolov5_research</a></p>
<h1 id="Ideas"><a href="#Ideas" class="headerlink" title="Ideas"></a>Ideas</h1><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><blockquote>
<p><a href="https://www.kaggle.com/datasets">Kaggle数据集</a><br><a href="https://gas.graviti.cn/open-datasets">格物钛数据集</a><br><a href="https://universe.roboflow.com/roboflow-100">Roboflow数据集</a><br><a href="https://ieee-dataport.org/datasets">IEEE DataPort</a></p>
</blockquote>
<p>标注工具：<a href="https://app.roboflow.com/395841716-qq-com">Roboflow</a></p>
<p>开源驾驶员行为数据集：<a href="https://www.kaggle.com/c/state-farm-distracted-driver-detection/data">StateFarm-distracted-driver-detection</a></p>
<p>数据增强：抖动模糊；三种不同的数据增强方法合成三通道；针对红外图像优化<br>扩大数据集：旋转 偏移（首先要保证原始数据量够）；混合数据集——彩色+红外<br>各集种类分配不均，测试集要用不同的人</p>
<h2 id="Anchor"><a href="#Anchor" class="headerlink" title="Anchor"></a>Anchor</h2><p>设计——anchor的计算函数Autoanchor<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Yolo1.png" alt="图 1">  </p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><ol>
<li>在 <code>models/common.py</code> 加入新的结构代码</li>
<li>在<code>models/yolo.py</code> 的parse_model函数中引入上面新写的结构名称</li>
<li><code>.yaml</code> 修改网络结构<br><img data-src="https://raw.gitmirror.com/Arrowes/Blog/main/images/Yolo2.png" alt="图 2"></li>
</ol>
<h2 id="注意力模块"><a href="#注意力模块" class="headerlink" title="注意力模块"></a>注意力模块</h2><p><a href="https://zhuanlan.zhihu.com/p/330535757">CV中即插即用的注意力模块</a><br><a href="https://blog.csdn.net/weixin_43694096/article/details/124443059?spm=1001.2014.3001.5502">手把手带你YOLOv5 (v6.1)添加注意力机制</a></p>
<blockquote>
<p>位置：<br>在上采样+concat之后接一个注意力机制可能会更好？<br>backbone结尾使用一个注意力机制？<br>每个block（如residual block）结尾使用比每个Conv里使用更好？</p>
</blockquote>
<p>transformer自注意力模块 CBAM注意力模块 CA注意力模块 SE注意力模块</p>
<h2 id="激活函数-activations-py"><a href="#激活函数-activations-py" class="headerlink" title="激活函数 activations.py"></a>激活函数 activations.py</h2><p><a href="https://blog.csdn.net/m0_70388905/article/details/128753641">改进激活函数为ReLU、RReLU、Hardtanh、ReLU6、Sigmoid、Tanh、Mish、Hardswish、ELU、CELU等</a></p>
<blockquote>
<p>activations.py：激活函数代码写在了activations.py文件里，可引入新的激活函数<br>common.py：替换激活函数，很多卷积组都涉及到了激活函数（Conv，BottleneckCSP），所以改的时候要全面</p>
</blockquote>
<p>例：插入激活函数：Mish<br>1.在utils&#x2F;activation.py中定义Mish激活函数<br>2.重构Conv模块，改激活函数：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Conv</span>(nn.Module):</span><br><span class="line">    <span class="comment"># Standard convolution</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c1, c2, k=<span class="number">1</span>, s=<span class="number">1</span>, p=<span class="literal">None</span>, g=<span class="number">1</span>, act=<span class="literal">True</span></span>):  <span class="comment"># ch_in, ch_out, kernel, stride, padding, groups</span></span><br><span class="line">        <span class="built_in">super</span>(Conv, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn = nn.BatchNorm2d(c2)</span><br><span class="line">        <span class="comment">#self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())</span></span><br><span class="line">        self.act = nn.Mish() <span class="keyword">if</span> act <span class="keyword">is</span> <span class="literal">True</span> <span class="keyword">else</span> (act <span class="keyword">if</span> <span class="built_in">isinstance</span>(act, nn.Module) <span class="keyword">else</span> nn.Identity())</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ReLU</span>(nn.Module):</span><br><span class="line">    __constants__ = [<span class="string">&#x27;inplace&#x27;</span>]</span><br><span class="line">    inplace: <span class="built_in">bool</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplace: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ReLU, self).__init__()</span><br><span class="line">        self.inplace = inplace</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> F.relu(x, inplace=self.inplace)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">extra_repr</span>(<span class="params">self</span>):</span><br><span class="line">        inplace_str = <span class="string">&#x27;inplace=True&#x27;</span> <span class="keyword">if</span> self.inplace <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> inplace_str</span><br></pre></td></tr></table></figure>

<h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>例：改 EIOU loss</p>
<ol>
<li>修改 general.py，增加EIOU。<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">elif</span> EIoU:</span><br><span class="line">                w=(w1-w2)*(w1-w2)</span><br><span class="line">                h=(h1-h2)*(h1-h2)</span><br><span class="line">                <span class="keyword">return</span> iou-(rho2/c2+w/(cw**<span class="number">2</span>)+h/(ch**<span class="number">2</span>))<span class="comment">#EIOU  2021.12.29</span></span><br></pre></td></tr></table></figure></li>
<li>将loss.py中边框位置回归损失函数改为eiou。<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">iou = bbox_iou(pbox.T, tbox[i], x1y1x2y2=<span class="literal">False</span>, EIoU=<span class="literal">True</span>)  <span class="comment"># iou(prediction, target)</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="参数配置（YOLOv5）"><a href="#参数配置（YOLOv5）" class="headerlink" title="参数配置（YOLOv5）"></a>参数配置（YOLOv5）</h2><p><strong>Detect参数</strong><br>调用电脑摄像头:<br>右上角py配置 &gt; Edit Configurations &gt; Parameters<br><code>--view-img --source 0</code></p>
<p>调用手机摄像头：<br>下载 <a href="https://www.123pan.com/s/goS7Vv-QeKbd.html">IP摄像头</a> App，关闭代理，连同一个网，Parameters配置为：<br><code>--source http://admin:admin@192.168.43.1:8081</code> 具体地址见 APP</p>
<p><strong>Train参数</strong><br><code>action=&#39;store_true&#39;</code> 触发了为true，否则为false 和 default&#x3D;False 效果一样</p>
<p><strong>YOLOv8（没搞懂）</strong><br>该版本参数集中配置ultralytics&#x2F;yolo&#x2F;configs&#x2F;default.yaml<br>model参数可以是pt也可以是yaml。</p>
<blockquote>
<p>pt:相当于使用预训练权重进行训练，比如选择为yolov8n.pt，就是训练一个yolov8n模型，并且训练前导入这个pt的权重。<br>yaml:相当于直接初始化一个模型进行训练，比如选择为yolov8n.yaml，就是训练一个yolov8n模型，权重是随机初始化。</p>
</blockquote>
<p>data.yaml数据只能用绝对地址<br>要修改代码先卸ultralytics包，利用setup.py</p>
<h1 id="炼丹经验"><a href="#炼丹经验" class="headerlink" title="炼丹经验"></a>炼丹经验</h1><ul>
<li><p><strong>数据集</strong>：输入图像的大小要求必须是32的倍数；Resize保持原始图像比例调整大小更安全；标注时标注框的设计影响精度</p>
</li>
<li><p><strong>配置</strong>：mosic有时没用可删；删卷积层可减少计算量；若显存不足需要调小batchsize或数据集分辨率；可以从小模型中学到的权重开始，对更大模型进行训练</p>
<ul>
<li>大的batch_size往往建议可以相应取大点learning_rate, 因为梯度震荡小，大learning_rate可以加速收敛过程，也可以防止陷入到局部最小值，而小batch_size用小learning_rate迭代，防止错过最优点，一直上下震荡没法收敛</li>
<li>参数调优过程一般要反复多次进行<code>微调&lt;—&gt;训练&lt;—&gt;测试</code>，最终得出符合需求&#x2F;较优的HyperPara，应用在项目中	<code>data/hyps/hyp.finetune.yaml</code></li>
</ul>
</li>
</ul>
<p><strong>小目标检测</strong>：小目标检测效果不好主要原因为小目标尺寸问题。<br>以网络的输入608×608为例，yolov5中下采样使用了5次，因此最后的特征图大小是19×19，38×38，76×76。三个特征图中，最大的76×76负责检测小目标，而对应到608×608上，每格特征图的感受野是608&#x2F;76&#x3D;8×8大小。即如果原始图像中目标的宽或高小于8像素，网络很难学习到目标的特征信息。<br>另外很多图像分辨率很大，如果简单的进行下采样，下采样的倍数太大，容易丢失数据信息。但是倍数太小，网络前向传播需要在内存中保存大量的特征图，极大耗尽GPU资源,很容易发生显存爆炸，无法正常的训练及推理。<br>这种情况可以使用<strong>分割</strong>的方式，将大图先分割成小图，再对每个小图检测，不过这样方式有优点也有缺点： </p>
<blockquote>
<p>优点：准确性 分割后的小图，再输入目标检测网络中，对于最小目标像素的下限会大大降低。<br>比如分割成608×608大小，送入输入图像大小608×608的网络中，按照上面的计算方式，原始图片上，长宽大于8个像素的小目标都可以学习到特征。<br>缺点：增加计算量 比如原本1920×1080的图像，如果使用直接大图检测的方式，一次即可检测完。但采用分割的方式，切分成4张912×608大小的图像，再进行N次检测，会大大增加检测时间。<br><a href="https://blog.csdn.net/weixin_56184890/article/details/119840555">YOLOV5 模型和代码修改——针对小目标识别-CSDN博客</a></p>
</blockquote>
<p>此外，也可以增加一个小目标检测层：<a href="https://blog.csdn.net/m0_70388905/article/details/125392908">增加小目标检测层-CSDN博客</a></p>
<p>使用云服务器快速训练（收费）：<a href="https://www.autodl.com/home">AutoDL算力云 | 弹性、好用、省钱</a><br><a href="https://www.autodl.com/docs/gpu/">AutoDL帮助文档-GPU选型</a></p>
<h1 id="其他未实现的想法"><a href="#其他未实现的想法" class="headerlink" title="其他未实现的想法"></a>其他未实现的想法</h1><p>剪枝：<a href="https://blog.csdn.net/m0_70388905/article/details/128222629">模型剪枝、蒸馏、压缩-CSDN博客</a><br><a href="https://github.com/VainF/Torch-Pruning">GitHub - Torch-Pruning: [CVPR 2023] Towards Any Structural Pruning; </a></p>
<p>融合EfficientNet和YoloV5：主要思想是训练一个图像分类模型(EfficientNet)，它可以实现非常高的AUC(约0.99)，并找到一种方法将其与目标检测模型融合。这被称为“2 class filter”</p>
<p>加权框融合(WBF)后处理：对目标检测模型产生的框进行过滤，从而使结果更加准确和正确的技术。它的性能超过了现有的类似方法，如NMS和soft-NMS。</p>
<p>用5折交叉验证<br>双流网络<br>矩形训练<br>PERCLOS值怎么显示？<br>把图像增强工作流加入算法？</p>
<p>The author uses ArcFace loss to measure the error of prediction. This loss was proposed for facial recognition in 2018. Other sophisticated approaches have also been published in recent years, such as <a href="https://openaccess.thecvf.com/content/CVPR2022W/Biometrics/papers/Boutros_ElasticFace_Elastic_Margin_Loss_for_Deep_Face_Recognition_CVPRW_2022_paper.pdf">ElasticFace</a>. author can compare the proposed loss with this approach.</p>
<p><a href="https://blog.csdn.net/weixin_56184890/article/details/119840555">YOLOV5 模型和代码修改——针对小目标识别</a></p>
]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
</search>
