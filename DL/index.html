<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/128.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/16.png">
  <link rel="mask-icon" href="/images/arrow.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css">
  <script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"wangyujie.space","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"default"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"manual"},"fancybox":false,"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="记录深度学习基本概念，不断更新中，项目地址：DLpractice">
<meta property="og:type" content="article">
<meta property="og:title" content="DL：深度学习相关概念">
<meta property="og:url" content="https://wangyujie.space/DL/index.html">
<meta property="og:site_name" content="Arrow的笔记本">
<meta property="og:description" content="记录深度学习基本概念，不断更新中，项目地址：DLpractice">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL1.png">
<meta property="og:image" content="https://zh.d2l.ai/_images/output_mlp_76f463_51_0.svg">
<meta property="og:image" content="https://zh.d2l.ai/_images/output_mlp_76f463_81_0.svg">
<meta property="og:image" content="https://zh.d2l.ai/_images/output_mlp_76f463_21_0.svg">
<meta property="og:image" content="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL_ELU.png">
<meta property="og:image" content="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMyLnpoaW1nLmNvbS84MC92Mi02MDRiZTExNGZhMDQ3OGYzYTEwNTk5MjNmZDEwMjJkMV9oZC5wbmc?x-oss-process=image/format,png">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-93a99cd695aeb1b8edf0c4b4eac8b7a9_1440w.webp?source=1940ef5c">
<meta property="og:image" content="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL2.png">
<meta property="og:image" content="https://pic1.zhimg.com/50/v2-d552433faa8363df84c53b905443a556_720w.webp?source=1940ef5c">
<meta property="og:image" content="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL-Conv.jpg">
<meta property="og:image" content="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL3.gif">
<meta property="og:image" content="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL4.png">
<meta property="og:image" content="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL5.png">
<meta property="og:image" content="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL6.png">
<meta property="og:image" content="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL7.png">
<meta property="og:image" content="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL8.png">
<meta property="og:image" content="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL9.png">
<meta property="og:image" content="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL10.png">
<meta property="og:image" content="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL11ROC.jpg">
<meta property="og:image" content="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL12AUC.png">
<meta property="og:image" content="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL13.png">
<meta property="og:image" content="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL14.png">
<meta property="og:image" content="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL15.png">
<meta property="og:image" content="https://i-blog.csdnimg.cn/direct/dbf3bfb8a37c4c1582d09b9ebd6ad01b.png#pic_center">
<meta property="og:image" content="https://i-blog.csdnimg.cn/blog_migrate/480474efbee3b54d014a3f6691284354.jpeg">
<meta property="og:image" content="https://i-blog.csdnimg.cn/blog_migrate/62be732a9003bfc80c298a2ecd5058a8.jpeg">
<meta property="og:image" content="https://mmbiz.qpic.cn/mmbiz_png/teF4oHzZ4IQzKII5nhSaQrQV4tmXKQvf0ibE3QUVDR8X6FcDBqicuTE3riaO2QDLS5nibEoMzI7ugWPu33yVZUAydQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-c4e6229a1fd42692d090108481be34a6_1440w.webp">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-b03d43ed4b3dc4c02e68712e57023cff_1440w.webp">
<meta property="og:image" content="https://i-blog.csdnimg.cn/blog_migrate/9623846bf1a2b09682eab74e606063bb.png">
<meta property="og:image" content="https://i-blog.csdnimg.cn/blog_migrate/a3e9e76563206c4bab09b91762341533.png">
<meta property="og:image" content="https://i-blog.csdnimg.cn/blog_migrate/945ac8d55f31965189e6fcdc2b86a3d0.png">
<meta property="og:image" content="https://i-blog.csdnimg.cn/blog_migrate/bbc1b91aef9aa68f0fc1dff0720c1a29.png">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-ba9edb24f8cbf10ee77bacb7f10befa7_1440w.webp">
<meta property="article:published_time" content="2022-12-28T12:22:05.000Z">
<meta property="article:modified_time" content="2025-10-16T14:16:35.786Z">
<meta property="article:author" content="Arrow">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL1.png">

<link rel="canonical" href="https://wangyujie.space/DL/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>DL：深度学习相关概念 | Arrow的笔记本</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Arrow的笔记本" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta custom-logo">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Arrow的笔记本</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <a>
        <img class="custom-logo-image" src="/images/arrow.png" alt="Arrow的笔记本">
      </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="https://wangyujie.space/pintree/" rel="section"><i class="fa fa-sitemap fa-fw"></i>书签</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wangyujie.space/DL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Arrow">
      <meta itemprop="description" content="记录一些杂七杂八的东西">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Arrow的笔记本">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          DL：深度学习相关概念
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-12-28 20:22:05" itemprop="dateCreated datePublished" datetime="2022-12-28T20:22:05+08:00">2022-12-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-10-16 22:16:35" itemprop="dateModified" datetime="2025-10-16T22:16:35+08:00">2025-10-16</time>
              </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>15k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>53 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>记录深度学习基本概念，不断更新中，项目地址：<a target="_blank" rel="noopener" href="https://github.com/Arrowes/DLpractice">DLpractice</a></p>
<span id="more"></span>

<p>从机器学习到深度学习：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/subconscious/p/4107357.html">从机器学习谈起</a>，<a target="_blank" rel="noopener" href="https://www.cnblogs.com/subconscious/p/5058741.html">从神经元到深度学习</a><br>什么是卷积讲解视频：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1sb411P7pQ/?share_source=copy_web&vd_source=b148fb6f311bfe6f3870ad8f4dfda92a">大白话讲解卷积神经网络工作原理</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/scutan90/DeepLearning-500-questions">DL500问</a></p>
<h1 id="深度学习框架"><a href="#深度学习框架" class="headerlink" title="深度学习框架"></a>深度学习框架</h1><pre class="mermaid">graph LR
A[程序框架]-->B[A.黑箱]
A-->C[B.模块化] -->1.处理数据
C-->2.构建网络
C-->3.损失函数
C-->4.优化函数
C-->5.模型保存
A-->E[C.定义]</pre>

<img src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL1.png" width = "60%" />

<p>GPU 网络和数据要同时送进GPU</p>
<h2 id="激活函数-Activate-Function"><a href="#激活函数-Activate-Function" class="headerlink" title="激活函数 Activate Function"></a>激活函数 Activate Function</h2><p>激活函数是深度学习神经网络中的一个重要组成部分，它用来引入<em>非线性性质</em>，使神经网络能够学习复杂的函数关系。激活函数接收神经元的输入，并产生输出作为下一层神经元的输入。在神经网络中，激活函数通常被应用于每个神经元的输出，使神经网络能够进行非线性映射和学习。</p>
<p>激活函数的主要作用有以下几点：</p>
<ul>
<li>非线性映射：激活函数引入非线性性质，使神经网络可以逼近和表示复杂的非线性函数。如果没有激活函数，多层神经网络的组合将等效于单一层的线性变换。</li>
<li>特征提取：激活函数有助于神经网络从输入数据中提取关键特征，例如边缘、纹理、形状等，以便更好地完成分类、回归和其他任务。</li>
<li>解决梯度消失问题：某些激活函数（如ReLU）有助于减轻梯度消失问题，使深层网络能够更好地进行反向传播和训练。</li>
</ul>
<p>一些常见的激活函数包括：</p>
<table>
<thead>
<tr>
<th>激活函数</th>
<th>特点</th>
<th>图像</th>
</tr>
</thead>
<tbody><tr>
<td>$$softmax(x_i) &#x3D; \frac{e^{x_i}}{\sum_{j&#x3D;0}^{N} e^{x_j}}$$</td>
<td>将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质;常用在分类网络的最后一层，把网络输出转化为各类别的概率。</td>
<td>首先对每个未规范化的预测求幂，这样可以确保输出非负。为了确保最终输出的概率值总和为1，再让每个求幂后的结果除以它们的总和</td>
</tr>
<tr>
<td>挤压函数（squashing function）$$sigmoid(x) &#x3D; \frac 1{1 + exp(−x)}$$</td>
<td>将输入映射到范围(0, 1)，常用于二元分类问题。sigmoid可以视为softmax的特例</td>
<td><img src="https://zh.d2l.ai/_images/output_mlp_76f463_51_0.svg"  /></td>
</tr>
<tr>
<td>双曲正切 $$tanh(x) &#x3D; \frac {1 − exp(−2x)}{1 + exp(−2x)}$$</td>
<td>将输入映射到范围(-1, 1)，也用于某些分类和回归问题, 当输入在0附近时，tanh函数接近线性变换。形状类似于sigmoid函数，不同的是tanh函数关于坐标系原点中心对称。（LSTM）</td>
<td><img src="https://zh.d2l.ai/_images/output_mlp_76f463_81_0.svg"  /></td>
</tr>
<tr>
<td>修正线性单元（Rectified Linear Unit）$$ReLU(x) &#x3D; max(x, 0)$$  $$LeakyReLU&#x3D;max(αx,x)$$</td>
<td>求导表现得特别好：要么让参数消失，要么让参数通过。最常用的激活函数，通常能够加速收敛和减轻梯度消失问题（Transfromer）； LeakyReLU中通常设α&#x3D;0.01来调整负值的零梯度，缓解dead ReLU问题（YOLO） 若α为可学习参数，则为PReLU</td>
<td><img src="https://zh.d2l.ai/_images/output_mlp_76f463_21_0.svg"  /></td>
</tr>
<tr>
<td>指数线性单元 (Exponential Linear Units) <img src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL_ELU.png"/></td>
<td>对小于零的情况采用类似指数计算的方式进行输出。与 ReLU 相比，ELU 有负值，这会使激活的平均值接近零。均值激活接近于零可以使学习更快，因为它们使梯度更接近自然梯度。但计算量较大</td>
<td><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMyLnpoaW1nLmNvbS84MC92Mi02MDRiZTExNGZhMDQ3OGYzYTEwNTk5MjNmZDEwMjJkMV9oZC5wbmc?x-oss-process=image/format,png"  /></td>
</tr>
</tbody></table>
<h2 id="感受野-Receptive-field"><a href="#感受野-Receptive-field" class="headerlink" title="感受野(Receptive field)"></a>感受野(Receptive field)</h2><p>感受野是指在卷积神经网络中，输出特征图上的一个像素点对应于输入图像上的感受区域大小。感受野的大小可以用来衡量网络在某一层上能够“看到”输入图像的范围，从而影响网络对局部和全局信息的感知能力。<br><img src="https://pic1.zhimg.com/80/v2-93a99cd695aeb1b8edf0c4b4eac8b7a9_1440w.webp?source=1940ef5c"  /></p>
<p>$   n_{output.features}&#x3D;[\frac{n_{input.features}+2p_{adding.size}-k_{ernel.size}}{s_{tride.size}}+1]   $<br>较小的感受野通常用于捕获局部特征，而较大的感受野则有助于捕获全局信息。<br><img src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL2.png" width = "50%" /></p>
<img src="https://pic1.zhimg.com/50/v2-d552433faa8363df84c53b905443a556_720w.webp?source=1940ef5c" width = "20%" />

<h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><img alt="图 37" src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL-Conv.jpg" />  

<p>深度可分离卷积是一种高效的卷积方式，它通过先对每个通道单独做空间上的卷积（depthwise），再用 1×1 卷积融合通道信息（pointwise），与标准卷积相比显著减少了参数量和计算成本，特别适合部署在轻量级或移动设备上，同时仍保有较强的特征提取能力。</p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>待续</p>
<h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><img src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL3.gif" width = "60%" />

<p>$$SGD → SGDM → NAG → AdaGrad → AdaDelta → Adam → Nadam$$<br><img src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL4.png" alt="图 4">  </p>
<h3 id="学习率与优化器调度策略"><a href="#学习率与优化器调度策略" class="headerlink" title="学习率与优化器调度策略"></a>学习率与优化器调度策略</h3><table>
<thead>
<tr>
<th>策略名称</th>
<th>作用</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Warmup</strong></td>
<td>稳定训练初期</td>
<td>学习率从小逐渐升高，避免梯度爆炸</td>
</tr>
<tr>
<td><strong>余弦退火（Cosine Annealing）</strong></td>
<td>平滑收敛</td>
<td>学习率按余弦曲线下降，后期趋近于零</td>
</tr>
<tr>
<td><strong>Step Decay</strong></td>
<td>分段衰减</td>
<td>每隔固定 epoch 将学习率乘以一个因子</td>
</tr>
<tr>
<td><strong>Exponential Decay</strong></td>
<td>指数衰减</td>
<td>学习率按指数函数持续下降</td>
</tr>
<tr>
<td><strong>Cyclical Learning Rate (CLR)</strong></td>
<td>提升探索能力</td>
<td>学习率在两个边界之间周期性波动</td>
</tr>
<tr>
<td><strong>OneCycle Policy</strong></td>
<td>快速收敛</td>
<td>学习率先升后降，动量反向变化</td>
</tr>
<tr>
<td><strong>自适应学习率（如 Adam、RMSprop）</strong></td>
<td>自动调整</td>
<td>根据梯度历史动态调整每个参数的学习率</td>
</tr>
</tbody></table>
<h3 id="动量与梯度控制策略"><a href="#动量与梯度控制策略" class="headerlink" title="动量与梯度控制策略"></a>动量与梯度控制策略</h3><table>
<thead>
<tr>
<th>策略名称</th>
<th>作用</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>动量（Momentum）</strong></td>
<td>加速收敛</td>
<td>保留上一次梯度方向，减少震荡</td>
</tr>
<tr>
<td><strong>周期性动量调整</strong></td>
<td>提升泛化</td>
<td>动量值随训练周期变化，配合 CLR 使用</td>
</tr>
<tr>
<td><strong>梯度裁剪（Gradient Clipping）</strong></td>
<td>防止梯度爆炸</td>
<td>限制梯度最大值，常用于 RNN 或 Transformer</td>
</tr>
<tr>
<td><strong>梯度累积（Gradient Accumulation）</strong></td>
<td>显存优化</td>
<td>多个小 batch 累积后再更新参数，适用于大模型</td>
</tr>
</tbody></table>
<h3 id="模型正则化与泛化策略"><a href="#模型正则化与泛化策略" class="headerlink" title="模型正则化与泛化策略"></a>模型正则化与泛化策略</h3><table>
<thead>
<tr>
<th>策略名称</th>
<th>作用</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Dropout</strong></td>
<td>防止过拟合</td>
<td>随机丢弃神经元，增强模型鲁棒性</td>
</tr>
<tr>
<td><strong>L1&#x2F;L2 正则化</strong></td>
<td>限制权重</td>
<td>控制模型复杂度，避免过拟合</td>
</tr>
<tr>
<td><strong>Early Stopping</strong></td>
<td>提前终止训练</td>
<td>验证集性能不再提升时停止训练</td>
</tr>
<tr>
<td><strong>Label Smoothing</strong></td>
<td>提升分类鲁棒性</td>
<td>将标签分布平滑处理，减少过拟合倾向</td>
</tr>
<tr>
<td><strong>Stochastic Depth</strong></td>
<td>深度网络正则化</td>
<td>随机跳过某些层，常用于 ResNet&#x2F;ViT</td>
</tr>
</tbody></table>
<h3 id="常见组合推荐"><a href="#常见组合推荐" class="headerlink" title="常见组合推荐"></a>常见组合推荐</h3><table>
<thead>
<tr>
<th>目标</th>
<th>推荐策略组合</th>
</tr>
</thead>
<tbody><tr>
<td>稳定训练</td>
<td>Warmup + Cosine Annealing + Gradient Clipping</td>
</tr>
<tr>
<td>控制过拟合</td>
<td>Dropout + L2 正则化 + Early Stopping</td>
</tr>
<tr>
<td>提升精度</td>
<td>Mixup + Label Smoothing + OneCycle Policy</td>
</tr>
<tr>
<td>显存受限</td>
<td>Gradient Accumulation + Mixed Precision Training</td>
</tr>
<tr>
<td>多任务场景</td>
<td>MTL + Shared Backbone + Task-specific Heads</td>
</tr>
</tbody></table>
<h2 id="Batch-size"><a href="#Batch-size" class="headerlink" title="Batch size"></a>Batch size</h2><p>batch size的大小影响的是训练过程中的完成<em>每个epoch所需的时间</em> $^1$（假设算力确定了）和每次迭代(iteration)之间<em>梯度的平滑程度</em> $^2$。</p>
<blockquote>
<ol>
<li>假设训练集大小为N，每个epoch中mini-batch大小为b，那么完成每个epoch所需的迭代次数为 N&#x2F;b , 因此完成每个epoch所需的时间会随着迭代次数的增加而增加</li>
<li>如pytorch\tensorflow等深度学习框架，在进行mini-batch的loss反向传播时，一般都是先将每个mini-batch中每个样本得到的loss求sum后再平均化之后再反求梯度，进行迭代，因此b的大小决定了相邻迭代batch之间的梯度平滑程度。一个batch内所含样本越多，这个batch的梯度应该越能反映真实的梯度，因此这样的大batch间梯度不会跨越太大</li>
</ol>
</blockquote>
<p>因此：大的batch_size往往建议可以相应取大点learning_rate, 因为梯度震荡小，大 learning_rate可以加速收敛过程，也可以防止陷入到局部最小值，而小batch_size用小learning_rate迭代，防止错过最优点，一直上下震荡没法收敛 </p>
<blockquote>
<ol>
<li>若是loss还能降，指标还在升，那说明欠拟合，还没收敛，应该继续train，增大epoch。</li>
<li>若是loss还能再降，指标也在降，说明过拟合了，那就得采用提前终止（减少epoch）或采用weight_decay等防过拟合措施。</li>
<li>若是设置epoch&#x3D;16，到第8个epoch，loss也不降了，指标也不动了，说明8个epoch就够了，剩下的白算了。</li>
</ol>
</blockquote>
<h2 id="损失函数「loss-function」"><a href="#损失函数「loss-function」" class="headerlink" title="损失函数「loss function」"></a>损失函数「loss function」</h2><p>来度量模型的预测值$\hat{y}$与真实值$y$的差异程度的运算函数，它是一个非负实值函数，通常使用$L(y, \hat{y})$来表示，损失函数越小，模型的鲁棒性就越好。<br><strong>基于距离度量的损失函数</strong><br>基于距离度量的损失函数通常将输入数据映射到基于距离度量的特征空间上，如欧氏空间、汉明空间等，将映射后的样本看作空间上的点，采用合适的损失函数度量特征空间上样本真实值和模型预测值之间的距离。特征空间上两个点的距离越小，模型的预测性能越好。</p>
<h3 id="L1范数损失函数（MAE）"><a href="#L1范数损失函数（MAE）" class="headerlink" title="L1范数损失函数（MAE）"></a>L1范数损失函数（MAE）</h3><p>$$L_{MSE}&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^{n}|y_i-\hat{y_i}|$$<br>又称为曼哈顿距离，表示残差的绝对值之和。L1损失函数对离群点有很好的鲁棒性，但它在残差为零处却不可导,且更新的梯度始终相同；</p>
<h3 id="L2损失函数（MSE均方误差损失函数）"><a href="#L2损失函数（MSE均方误差损失函数）" class="headerlink" title="L2损失函数（MSE均方误差损失函数）"></a>L2损失函数（MSE均方误差损失函数）</h3><p>$$L_{MSE}&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^{n}(y_i-\hat{y_i})^2$$<br>在回归问题中，均方误差损失函数用于度量样本点到回归曲线的距离，通过最小化平方损失使样本点可以更好地拟合回归曲线。（L2损失又被称为欧氏距离，是一种常用的距离度量方法，通常用于度量数据点之间的相似度。）<br><strong>基于概率分布度量的损失函数</strong><br>基于概率分布度量的损失函数是将样本间的相似性转化为随机事件出现的可能性，即通过度量样本的真实分布与它估计的分布之间的距离，判断两者的相似度，一般用于涉及概率分布或预测类别出现的概率的应用问题中，在分类问题中尤为常用。</p>
<h3 id="KL散度（-Kullback-Leibler-divergence）"><a href="#KL散度（-Kullback-Leibler-divergence）" class="headerlink" title="KL散度（ Kullback-Leibler divergence）"></a>KL散度（ Kullback-Leibler divergence）</h3><p>$$L_{MSE}&#x3D;\sum_{i&#x3D;1}^{n}\hat{y_i}log(\frac{y_i}{\hat{y_i}})$$<br>也被称为相对熵，是一种非对称度量方法，常用于度量两个概率分布之间的距离。KL散度也可以衡量两个随机分布之间的距离，两个随机分布的相似度越高的，它们的KL散度越小，可以用于比较文本标签或图像的相似性。</p>
<h3 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h3><p><a target="_blank" rel="noopener" href="https://www.bbbdata.com/text/337">https://www.bbbdata.com/text/337</a></p>
<h4 id="香农信息量（Shannon-Information）"><a href="#香农信息量（Shannon-Information）" class="headerlink" title="香农信息量（Shannon Information）"></a>香农信息量（Shannon Information）</h4><p>表示某个事件发生时所携带的信息量：<br>$$I(x) &#x3D; -\log_b P(x) &#x3D; -\ln P(x)$$</p>
<ul>
<li>$ P(x) $：事件 $ x $ 的概率  </li>
<li>$ b $：对数底，通常取 2（单位为 bit）或 $ e $（单位为 nat）  </li>
<li>概率越小，信息量越大（越“惊讶”）</li>
</ul>
<h4 id="CE"><a href="#CE" class="headerlink" title="CE"></a>CE</h4><p>交叉熵（Cross Entropy）是信息论中的一个概念，最初用于估算平均编码长度，引入机器学习后，用于评估当前训练得到的概率分布与真实分布的差异情况。是在不知道真实分布、仅有猜测的概率时，我们知道真相时所获得的信息量期望；交叉熵的意义是，它可用于评估我们认知概率的准确性，在认知概率与真实概率一致，交叉熵是最小的，反过来说，交叉熵越小则说明预测越准确</p>
<p>衡量两个概率分布之间（交叉）的差异，真实分布 $ y $ 与预测分布 $ \hat{y} $：</p>
<p>$$H(y, \hat{y}) &#x3D; - \sum_{i&#x3D;1}^{n} y_i \ln(\hat{y}_i)$$</p>
<ul>
<li>$ y_i $：真实标签（通常是 one-hot）  </li>
<li>$ \hat{y}_i $：模型预测的概率（通常是 softmax 输出）</li>
</ul>
<p>交叉熵是模型对正确类别预测概率的负对数期望</p>
<h4 id="CE-loss"><a href="#CE-loss" class="headerlink" title="CE loss"></a>CE loss</h4><p>交叉熵损失函数(Cross-Entropy Loss)是一种常用于概率预测模型的损失函数。交叉熵损失函数是指，基于模型的预测概率，在知道真实标签时的交叉熵<br>$$CE Loss &#x3D;- \frac{1}{m} \sum_{i&#x3D;1}^{m}\sum_{j&#x3D;1}^{C}y_i,_j · \ln(\hat{y}<em>i,<em>j) &#x3D; - \frac{1}{m} \sum</em>{i&#x3D;1}^{m}\sum</em>{i(y_i&#x3D;k)} · \ln(\hat{y}_i,<em>k) &#x3D; - \frac{1}{m} \sum</em>{i&#x3D;1}^{m}· \ln(\hat{y}_i,_k)$$</p>
<ul>
<li>$m$： 样本的总数（一个 batch 的大小）。求平均就是期望</li>
<li>$ C $：类别数 </li>
<li>i：样本的索引（从 1 到 m）。</li>
<li>j：类别的索引（从 1 到 C）。</li>
<li>如果是One-Hot 编码，y_i,k&#x3D;1,其他为0，可以得到简化的第3个公式</li>
</ul>
<p>C&#x3D;2时，得到二元交叉熵损失<strong>BCE loss</strong> (Binary Cross-Entropy Loss)公式:<br>$$BCE Loss &#x3D;- \frac{1}{m} \sum_{i&#x3D;1}^{m} [y_i \cdot \ln(\hat{y}_i) + (1 - y_i) \cdot \ln(1 - \hat{y}_i)]$$</p>
<ul>
<li>$y_i$：第 $i$ 个样本的真实标签，在二分类中 $y_i$ 的值通常是 1（代表正类）或 0（代表负类）。</li>
<li>$\hat{y}_i$：模型对第 $i$ 个样本预测为正类 (类别 1) 的概率。 (这通常是 Sigmoid 函数的输出，值在 0 和 1 之间)。</li>
<li>$(1 - \hat{y}_i)$：模型对第 $i$ 个样本预测为负类 (类别 0) 的概率。</li>
<li>$\hat{y} &#x3D; \sigma(x) &#x3D; \frac{1}{1 + e^{-x}}$ 时，即$\hat{y}_i$为 Sigmoid 函数的输出，则为<strong>BCEWithLogitsLoss</strong>，意思是这个Loss可以直接输入原始Logits值，而不需要是概率，因为集成了Sigmoid（二分类）或Softmax（多分类）</li>
</ul>
<p>交叉熵越小，期望获得的信息量就越小，说明我们对本身事物越掌握，也即模型的预测概率越准确<br>所以交叉熵损失函数的目的，就是最小化期望信息量，也即最大化模型概率预测的准确程度</p>
<h4 id="Focal-loss"><a href="#Focal-loss" class="headerlink" title="Focal loss"></a>Focal loss</h4><p><code>Focal Loss</code> 是一种专门为解决<strong>类别不平衡问题</strong>而设计的损失函数，尤其在目标检测（如 RetinaNet）和语义分割中非常有效<br>Focal Loss 是对传统的 CE Loss 的一种改进。它的核心思想是： <strong>降低容易分类样本的损失权重，聚焦在难分类样本上。</strong>。它是一个动态缩放的交叉熵损失，通过一个动态缩放因子，可以动态降低训练过程中易区分样本的权重，从而将重心快速聚焦在那些难区分的样本（有可能是正样本，也有可能是负样本，但都是对训练网络有帮助的样本）。<br>这对于数据集中<strong>正负样本极度不平衡</strong>的情况非常有用，比如：</p>
<ul>
<li>检测图像中的小目标</li>
<li>分割稀疏区域（如车道线、肿瘤）</li>
</ul>
<p>以二分类为例，Focal Loss 的公式如下：</p>
<p>$$\text{FL}(p_t) &#x3D;- [\alpha_i y_i (1 - \hat{y}_i)^\gamma \cdot \ln(\hat{y}_i) + (1 - \alpha_i)(1 - y_i) \hat{y}_i^\gamma \cdot \ln(1 - \hat{y}_i)]&#x3D; -\alpha_t (1 - p_t)^\gamma \log(p_t)$$</p>
<p>其中：</p>
<ul>
<li>$ p_t $：模型对真实类别的预测概率</li>
<li>$ \alpha_t $：类别权重（控制正负样本不平衡）,通常设为 0.25 [正类（y&#x3D;1）的重要性是 0.25，而负类（y&#x3D;0）的重要性是 0.75）</li>
<li>$ \gamma $：聚焦因子，控制对易分类样本的抑制程度，通常设为 2<ul>
<li>如果样本<strong>易分类</strong>（模型很有把握，$\hat{y}_i \to 1$），那么 $(1 - \hat{y}_i)^\gamma \to 0$。这个因子的值会变得非常小，从而<strong>极大地降低</strong>了这个“简单”样本对总损失的贡献。</li>
<li>如果样本<strong>难分类</strong>（模型预测错误，$\hat{y}_i \to 0$），那么 $(1 - \hat{y}_i)^\gamma \to 1$。损失几乎不受影响，模型会<strong>重点关注</strong>它。</li>
<li>$\gamma &#x3D; 0$ 时，Focal Loss 就退化为了标准的（加权）BCE Loss。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">focal_loss</span>(<span class="params">inputs, targets, alpha=<span class="number">0.25</span>, gamma=<span class="number">2.0</span></span>):</span><br><span class="line">    BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    pt = torch.exp(-BCE_loss)  <span class="comment"># pt = sigmoid(inputs) if BCE used</span></span><br><span class="line">    focal_term = alpha * (<span class="number">1</span> - pt) ** gamma</span><br><span class="line">    <span class="keyword">return</span> (focal_term * BCE_loss).mean()</span><br></pre></td></tr></table></figure>

<p>one-hot独热编码：将类别变量转换为机器学习算法易于利用的一种形式的过程。</p>
<h3 id="IoU-Loss"><a href="#IoU-Loss" class="headerlink" title="IoU Loss"></a>IoU Loss</h3><p>在目标检测和图像分割任务中，交并比 IoU Loss（Intersection over Union Loss）用于衡量预测框与真实框之间的重叠程度。相比于传统的 L1&#x2F;L2 或 Smooth L1 损失，IoU 类损失更关注几何上的匹配，对边界框回归尤为关键。下面是主流几种 IoU 类损失的介绍：</p>
<table>
<thead>
<tr>
<th>损失类型</th>
<th>特点与优势</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td>IoU Loss</td>
<td>基础形式，仅考虑重叠区域比例</td>
<td>边界框回归基础版本</td>
</tr>
<tr>
<td>GIoU Loss</td>
<td>引入最小闭包区域作为惩罚项，解决无交集的问题</td>
<td>较大偏移框，对空区域更敏感</td>
</tr>
<tr>
<td>DIoU Loss</td>
<td>加入中心点距离作为惩罚，提高定位精度</td>
<td>对目标位置要求高的检测任务</td>
</tr>
<tr>
<td>CIoU Loss</td>
<td>综合考虑中心点距离、重叠率、纵横比</td>
<td>高精度框回归，如人脸检测等</td>
</tr>
<tr>
<td>SIoU Loss</td>
<td>引入角度、方向对齐等几何信息，优化收敛速度</td>
<td>高稳定性和收敛效率的检测模型</td>
</tr>
</tbody></table>
<ul>
<li><p><strong>IoU</strong>：<br>$$IoU &#x3D; \frac{\text{Area of Overlap}}{\text{Area of Union}}$$</p>
</li>
<li><p><strong>GIoU Loss</strong>：<br>GIOU Loss 是传统 IoU Loss 的改进版，它在 IoU 的基础上增加了一个惩罚项，解决了当预测框与真实框<strong>不相交</strong>时 IoU Loss&#x3D;0, 梯度为零、无法优化的问题，使得模型能够学习如何将分离的框相互靠近。</p>
</li>
</ul>
<p>$$L_{GIoU} &#x3D; 1 - GIOU$$</p>
<p>$$GIOU &#x3D; IoU - \frac{|C| - |A \cup B|}{|C|}$$</p>
<ul>
<li><strong>IoU</strong>: 预测框 A 和真实框 B 的交并比。</li>
<li><strong>C</strong>: 能同时包含 A 和 B 的<strong>最小闭包矩形</strong>（Smallest Enclosing Box）。</li>
<li>$\frac{|C| - |A \cup B|}{|C|}$: 这就是<strong>惩罚项</strong>。它计算的是闭包区域中不属于两个框联合区域的面积比例。两个框距离越远，这个惩罚项越大，GIOU 值就越小。</li>
</ul>
<p><strong>取值范围</strong>：GIOU 的值域为 $[-1, 1]$。<strong>1</strong>: 完美重合。<strong>趋近 -1</strong>: 不重叠且相距非常远。<br><strong>既是度量也是损失</strong>：不仅可以像 IoU 一样作为评估指标，其 $1 - GIOU$ 的形式更是一个优秀的损失函数。<br><strong>关注非重叠区域</strong>：通过闭包区域 C，它不仅仅关心重叠部分，还关心两个框的<strong>相对位置关系</strong>。</p>
<p><strong>收敛速度问题</strong>：在训练后期，当预测框与真实框重叠方式比较特殊时（如一个框完全包含另一个框），GIOU 会退化成 IoU，无法进一步区分对齐的好坏，可能导致收敛较慢。<br><strong>对齐方式不敏感</strong>：当两个框的 IoU 和 GIOU 值相同时，它们中心点的距离和长宽比可能差异很大，GIOU 并未考虑这些因素。这也催生了后续的 DIOU 和 CIOU 等更优的损失函数。</p>
<ul>
<li><p><strong>DIoU Loss</strong>：<br>$$DIoU &#x3D; IoU - \frac{\rho^2(b, b^{gt})}{c^2}$$<br>其中 $ \rho $ 是中心距离，$ c $ 是对角线长度。</p>
</li>
<li><p><strong>CIoU Loss</strong>：<br>在 DIoU 基础上增加形状约束项，综合角度与纵横比。</p>
</li>
</ul>
<p>IoU Loss 系列通过引入几何对齐、惩罚项等方式，让模型在训练时更关注框的位置与形状，提升检测精度和稳定性。</p>
<h1 id="注意力机制（Attention-Mechanism）"><a href="#注意力机制（Attention-Mechanism）" class="headerlink" title="注意力机制（Attention Mechanism）"></a>注意力机制（Attention Mechanism）</h1><p>自上而下有意识的聚焦称为<strong>聚焦式注意力</strong>，自下而上无意识、由外界刺激引发的注意力称为<strong>显著式注意力</strong>。<br>神经网络中的注意力机制是在计算能力有限的情况下，将计算资源分配给更重要的任务，同时解决信息超载问题的一种资源分配方案，到2014年，Volodymyr的《Recurrent Models of Visual Attention》一文中将其应用在视觉领域，后来伴随着2017年Ashish Vaswani的《Attention is all you need》中Transformer结构的提出，注意力机制在NLP,CV相关问题的网络设计上被广泛应用。<br>注意力有两种，一种是软注意力(soft attention)，另一种则是强注意力(hard attention)。<br><strong>软注意力</strong>更关注区域或者通道，是确定性的注意力，学习完成后直接可以通过网络生成，最关键的地方是软注意力是可微的，这是一个非常重要的地方。可以微分的注意力就可以通过神经网络算出梯度并且前向传播和后向反馈来学习得到注意力的权重。<br><strong>强注意力</strong>是更加关注点，也就是图像中的每个点都有可能延伸出注意力，同时强注意力是一个随机的预测过程，更强调动态变化。当然，最关键是强注意力是一个不可微的注意力，训练过程往往是通过增强学习(reinforcement learning)来完成的。</p>
<h2 id="软注意力的注意力域"><a href="#软注意力的注意力域" class="headerlink" title="软注意力的注意力域"></a>软注意力的注意力域</h2><h3 id="空间域（Spatial-Domain）"><a href="#空间域（Spatial-Domain）" class="headerlink" title="空间域（Spatial Domain）"></a>空间域（Spatial Domain）</h3><p>空间域将原始图片中的空间信息变换到另一个空间中并保留了关键信息。<br>普通的卷积神经网络中的池化层（pooling layer）直接用一些max pooling 或者average pooling 的方法，将图片信息压缩，减少运算量提升准确率。<br>发明者认为之前pooling的方法太过于暴力，直接将信息合并会导致关键信息无法识别出来，所以提出了一个叫 <strong>空间转换器（spatial transformer）</strong> 的模块，将图片中的的空间域信息做对应的空间变换，从而能将关键的信息提取出来。<br><img src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL5.png" width = "50%" /></p>
<h3 id="通道域（Channel-Domain）"><a href="#通道域（Channel-Domain）" class="headerlink" title="通道域（Channel Domain）"></a>通道域（Channel Domain）</h3><p>通道注意力机制在计算机视觉中，更关注特征图中channel之间的关系，而普通的卷积会对通道做通道融合，这个开山鼻祖是SENet,后面有GSoP-Net，FcaNet 对SENet中的squeeze部分改进，EACNet对SENet中的excitation部分改进，SRM,GCT等对SENet中的scale部分改进。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1709.01507">SENet</a>,<a target="_blank" rel="noopener" href="https://github.com/moskomule/senet.pytorch">pytorch</a><br>SENet《Squeeze-and-Excitation Networks》是CVPR17年的一篇文章，提出SE module。在卷积神经网络中，卷积操作更多的是关注感受野，在通道上默认为是所有通道的融合（深度可分离卷积不对通道进行融合，但是没有学习通道之间的关系，其主要目的是为了减少计算量），SENet提出SE模块，将注意力放到通道之间，希望模型可以学习到不同通道之间的权重：<br><img src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL6.png" alt="图 6">  </p>
<h3 id="时域注意力机制"><a href="#时域注意力机制" class="headerlink" title="时域注意力机制"></a>时域注意力机制</h3><p>时域注意力机制在cv领域主要考虑有时序信息的领域，如视频领域中的动作识别方向，其注意力机制主要是在时序列中，关注某一时序即某一帧的信息。</p>
<h3 id="通道和空间注意力机制"><a href="#通道和空间注意力机制" class="headerlink" title="通道和空间注意力机制"></a>通道和空间注意力机制</h3><p>通道和空间注意力是基于通道注意力和空间注意力机制，将两者有效的结合在一起，让注意力能关注到两者，又称混合注意力机制，如CBAM,BAM,scSE等，同时基于混合注意力机制的一些关注点，如Triplet Attention 关注各种跨维度的相互作用；Coordinate Attention, DANet关注长距离的依赖；RGA 关注关系感知注意力。还有一种混合注意力机制，为3D的attention :Residual attention,SimAM, Strip Pooling, SCNet等。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.06521">CBAM</a>,<a target="_blank" rel="noopener" href="https://github.com/luuuyi/CBAM.PyTorch">github</a><br>CBAM (Convolutional Block Attention Module)是SENet的一种拓展，SENet主要基于通道注意力，CBAM是通道注意力和空间注意力融合的注意力机制。<br><img src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL7.png" alt="图 7"><br>如上图所示，输入一个h<em>w</em>c的特征图，通过channel Attention Module 生成通道注意力权重对输入特征图在通道层添加权重，再通过spatial Attention Module 生成空间注意力权重，对特征图在空间层添加权重，输出特征图。</p>
<h1 id="Metrics-评估"><a href="#Metrics-评估" class="headerlink" title="Metrics 评估"></a>Metrics 评估</h1><h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><img src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL8.png" width = "70%" />

<p>X横坐标为正确的分类（即你用标签所标注的真实分类）<br>Y纵坐标为模型所预测的分类（即图片经过模型推理后模型将其辨别为的分类）</p>
<blockquote>
<p>True positives (TP): 猫🐱的图片被正确识别成了猫🐱。（猫🐱的正确分类预测）<br>True negatives(TN): 背景的图片被正确识别为背景。（非猫🐱被预测为其他动物或背景）<br>False positives(FP): 背景的图片被错误识别为猫🐱。（非猫🐱被预测为猫🐱）<br>False negatives(FN): 猫🐱的图片被错误识别为背景。（猫🐱被预测为其他动物或者背景）</p>
</blockquote>
<h2 id="Evaluation-parameters"><a href="#Evaluation-parameters" class="headerlink" title="Evaluation parameters"></a>Evaluation parameters</h2><p><strong>准确率 Accuracy</strong>：在正负样本数量接近的情况下，准确率越高，模型的性能越好（当测试样本不平衡时，该指标会失去意义。）<br>$$Accuracy&#x3D;\frac{TP+TN}{TP+FP+TN+FN}$$<br><strong>精准率（查准率） precision</strong>：代表在总体预测结果中真阳性的预测数，针对预测结果，当区分能力强时，容易将部分（与负样本相似度高）正样本排除。<br>$$precision(P)&#x3D;\frac{TP}{TP+FP}$$<br><strong>召回率（查全率） recall</strong>：所有ground truths中真阳性的预测数，针对原样本，当敏感度高时，容易将部分（与正样本相似度高）负样本也判断为正样本。<br>$$recall(R)&#x3D;\frac{TP}{TP+FN}$$<br><strong>F1 score</strong>：对Precision和Recall两个指标的调和平均值（类似平均速度），F1分值越高，目标检测的准确性越好。F1-score最常用于数据集的类别不平衡的情况。<br>$$F_1 score&#x3D;2\cdot \frac{P\cdot R}{P+R}$$<br><strong>AP</strong>：同时考察Precision和Recall两个指标来衡量模型对于各个类别的性能。<br>$$AP_i&#x3D;\int_0^1P_i(R_i)dR_i$$<br><strong>mAP</strong>：表示AP的平均值，并用作衡量目标检测算法的总体检测精度的度量。<br>将recall设置为横坐标，precision设置为纵坐标。PR曲线下围成的面积即AP，所有类别AP平均值即mAP.<br>$$mAP&#x3D;\frac1n\sum_{i &#x3D; 1}^{n}AP_i$$<br><strong>置信度 Confidence</strong>：置信度设定越大，Prediction约接近1，Recall越接近0，要寻找最优的F1分数，需要遍历置信度。<br><img src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL9.png" alt="图 9">  </p>
<p><strong>交并比 IoU</strong>（Intersection over Union）：是目标检测中使用的一个概念，IoU计算的是“预测的边框”和“真实的边框”的交叠率，即它们的交集和并集的比值。最理想情况是完全重叠，即比值为1。</p>
<p><em><a href="mailto:&#x6d;&#x41;&#80;&#64;&#x30;&#46;&#x35;">&#x6d;&#x41;&#80;&#64;&#x30;&#46;&#x35;</a></em>：IoU阈值设为 0.5，即预测框与真实框的重叠面积占比 ≥ 50% 就算正确。对每个类别计算 AP，再取平均值得到 mAP。评估标准较宽松，适合快速验证模型是否具备基本检测能力。<br><em><a href="mailto:&#109;&#65;&#x50;&#x40;&#48;&#46;&#53;">&#109;&#65;&#x50;&#x40;&#48;&#46;&#53;</a>:0.95</em>：在 IoU 阈值从 0.5 到 0.95（步长为 0.05） 的 10 个点上分别计算 AP，然后取平均。更严格地评估模型在不同定位精度要求下的表现。全面衡量模型稳定性和定位能力，常用于学术论文和高精度场景（如自动驾驶）。<br>如果一个模型在 <a href="mailto:&#x6d;&#x41;&#80;&#64;&#x30;&#46;&#53;">&#x6d;&#x41;&#80;&#64;&#x30;&#46;&#53;</a> 上表现很好，但在 <a href="mailto:&#x6d;&#65;&#80;&#x40;&#x30;&#46;&#x35;">&#x6d;&#65;&#80;&#x40;&#x30;&#46;&#x35;</a>:0.95 上得分低，说明它能大致定位目标，但在精确定位方面表现不佳。<br><img src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL10.png" width = "60%" /></p>
<p><strong>NMS</strong>（Non-Maximum Suppression，非极大值抑制）<br>一种用于目标检测任务的后处理技术，主要用于消除冗余的检测框，保留最可能准确的预测结果。<br>在目标检测中，模型（如YOLO、Faster R-CNN等）通常会对同一目标生成多个重叠的预测框（Bounding Box），每个框带有置信度分数。NMS通过筛选，保留置信度最高且位置最合理的框，抑制其他冗余的框，从而避免重复检测。</p>
<ol>
<li>按置信度排序：将所有预测框按置信度从高到低排序。选择最高置信度的框：取出当前列表中置信度最高的框，加入最终保留列表。</li>
<li>计算IoU并抑制重叠框：计算该框与剩余所有框的交并比（IoU，Intersection over Union）。若某框与当前框的IoU超过设定的阈值（如0.5），则认为它们是同一目标，直接删除。</li>
<li>重复步骤2~3：对剩余框重复上述过程，直到所有框被处理。<br>IoU阈值：通常设为0.3~0.7，控制框的重叠容忍度。阈值越低，抑制越严格。<br>置信度阈值：预过滤掉低置信度的框（例如只保留置信度≥0.5的框）。</li>
</ol>
<p><strong>ROC曲线</strong>(Receiver Operating Characteristic 受试者工作特征)<br>$$TPR&#x3D;\frac{TP}{TP+FN},FPR&#x3D;\frac{FP}{FP+TN}$$可以理解为分类器对正样本的覆盖敏感性和对负样本的敏感性的权衡。<br>在ROC曲线图中，每个点以对应的FPR值为横坐标，以TPR值为纵坐标<br><img src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL11ROC.jpg" width = "40%" /></p>
<p><strong>AUC值</strong>：PR曲线下方的面积<br><img src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL12AUC.png" width = "70%" /></p>
<blockquote>
<p>1.AUC &#x3D; 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。<br>2.0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。<br>3.AUC &#x3D; 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。<br>4.AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。</p>
</blockquote>
<p>ROC曲线图中，越靠近(0,1)的点对应的模型分类性能越好。而且可以明确的一点是，ROC曲线图中的点对应的模型，它们的不同之处仅仅是在分类时选用的阈值(Threshold)不同，每个点所选用的阈值都对应某个样本被预测为正类的概率值。</p>
<h2 id="模型计算量-FLOPs-和参数量-Params"><a href="#模型计算量-FLOPs-和参数量-Params" class="headerlink" title="模型计算量(FLOPs)和参数量(Params)"></a>模型计算量(FLOPs)和参数量(Params)</h2><p><strong>计算量 FLOPs</strong>：FLOPs指浮点运算次数，Floating-point Operations Per Second，s是指秒，即每秒浮点运算次数的意思，考量一个网络模型的计算量的标准。硬件要求是在于芯片的floaps（指的是gpu的运算能力）<br>FLOPS：（全部大写），每秒所执行的浮点运算次数，理解为计算速度, 是一个衡量硬件性能&#x2F;模型速度的指标，即一个芯片的算力。<br>MACCs：multiply-accumulate operations，乘-加操作次数，MACCs 大约是 FLOPs 的一半。将 w[0]∗x[0]+… 视为一个乘法累加或 1 个 MACC。<br>MAC: Memory Access Cost 内存访问代价。指的是输入单个样本（一张图像），模型&#x2F;卷积层完成一次前向传播所发生的内存交换总量，即模型的空间复杂度，单位是 Byte。<br><strong>参数量 Params</strong>：是指网络模型中需要训练的参数总数。硬件要求在于显存大小<br>1.<strong>卷积层</strong><br>计算时间复杂度(计算量)<br>$$Time\sim O(\sum_{l&#x3D;1}^D M_l^2\cdot K_l^2\cdot C_{l-1}\cdot C_l)$$</p>
<p>计算空间复杂度(参数量)<br>$$Space\sim O(\sum_{l&#x3D;1}^D K_l^2\cdot C_{l-1}\cdot C_l+\sum_{l&#x3D;1}^D M^2\cdot C_l)$$</p>
<figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">参数量</span><br><span class="line">(kernel*kernel) *channel_input*channel_output</span><br><span class="line">kernel*kernel 就是 weight * weight</span><br><span class="line">其中kernel*kernel ＝ <span class="number">1</span>个<span class="built_in">feature</span>的参数量</span><br><span class="line"></span><br><span class="line">计算量</span><br><span class="line">(kernel*kernel*<span class="built_in">map</span>*<span class="built_in">map</span>) *channel_input*channel_output</span><br><span class="line">kernel*kernel 就是weight*weight</span><br><span class="line"><span class="built_in">map</span>*<span class="built_in">map</span>是下个featuremap的大小，也就是上个weight*weight到底做了多少次运算</span><br><span class="line">其中kernel*kernel*<span class="built_in">map</span>*<span class="built_in">map</span>＝　<span class="number">1</span>个<span class="built_in">feature</span>的计算量</span><br></pre></td></tr></table></figure>
<p>2.池化层<br>无参数<br>3.<strong>全连接层</strong><br><code>参数量＝计算量＝weight_in*weight_out  #模型里面最费参数的就是全连接层</code></p>
<p><strong>换算计算量</strong>,一般一个参数是指一个float，也就是４个字节,1kb&#x3D;1024字节</p>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p><img src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL13.png" alt="图 13"><br><img src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL14.png" alt="图 14"><br><img src="https://raw.gitmirror.com/Arrowes/Blog/main/images/DL15.png" alt="图 15">  </p>
<h1 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h1><h2 id="联邦学习"><a href="#联邦学习" class="headerlink" title="联邦学习"></a>联邦学习</h2><p>联邦学习（Federated Learning）是一种先进的分布式机器学习方法，它在数据隐私保护和数据利用效率方面具有显著的优势。在联邦学习中，多个参与方（也称为客户端或节点）可以在保持数据本地化的同时，共享模型训练的成果。<br>让各个企业自己进行模型的训练，各个企业在完成模型的训练之后，将各自模型的参数上传至一个中心服务器（也可以是点对点），中心服务器结合各个企业的参数（可以上传梯度，也可以是自己更新后的参数），重新拟定新的参数（例如通过加权平均，这一步叫做联邦聚合），将新的参数下发至各个企业，企业将新参数部署到模型上，从而继续新的训练，这个过程可以进行反复的迭代，直到模型收敛，或者满足其他的条件。<br><img src="https://i-blog.csdnimg.cn/direct/dbf3bfb8a37c4c1582d09b9ebd6ad01b.png#pic_center" width = "50%" /></p>
<h2 id="分布式训练"><a href="#分布式训练" class="headerlink" title="分布式训练"></a>分布式训练</h2><p>单机单卡情况下，信息都在一台机器上，无所谓分发。而分布式训练中，信息是要被“分发”的，分发的不同方式，常被称为“并行方式”。通常，习惯上将分发方分为“数据并行”和“模型并行”两种：</p>
<ul>
<li>模型并行(Model Parallelism)：将模型进行切分，完整的数据 被送至各个训练节点，与 切分后的模型 进行运算，最后将多个节点的运算结果合并；适用于模型规模大的情况</li>
<li>数据并行(Data Parallelism)：将样本数据进行切分，切分后的数据 被送至各个训练节点，与 完整的模型 进行运算，最后将多个节点的信息进行合并；适用于数据量大的情况<ol>
<li>数据划分：不同GPU设备上划分出不同的mini-batch，作为训练的数据集</li>
<li>前向+反向:不同GPU设备上用相同的模型，用各自接收到的mini-batch数据进行训练（前向和反向传播)</li>
<li>梯度同步更新:每个GPU设备得到了mini-batch训练后的权重值，这些值需要汇总然后更新至每一个GPU设备，保证每一次迭代后，每个GPU设备上的模型完全一致。  <img src="https://i-blog.csdnimg.cn/blog_migrate/480474efbee3b54d014a3f6691284354.jpeg" width = "50%" /></li>
</ol>
</li>
</ul>
<blockquote>
<p>分布式训练中的学习率自动缩放：在数据并行中，多个GPU同时处理不同的数据子集（每个GPU的批量大小为 B），总批量大小为 B × GPU数量。例如，单GPU批量大小为256，使用4个GPU时，总批量大小变为1024。<br>更大的批量意味着梯度估计的方差更小，更新方向更准确。为了保持参数更新的有效步长与单GPU训练一致，需要按比例调整学习率。</p>
</blockquote>
<p>分布式系统中因为面临大量的信息同步、更新需求，因此传统的点对点(P2P, Point-to-point)的通信方式不能很好的满足需求。需要使用集合通信库(Collective communication Library)，用于分布式训练时，多个计算设备之间的集合通信，常见的有 Open MPI、NCCL:</p>
<ul>
<li>Open MPI:Open MPI项目是一个开源MPI（消息传递接口 ）实现，由学术，研究和行业合作伙伴联盟开发和维护。因此，Open MPI可以整合高性能计算社区中所有专家，技术和资源，以构建可用的最佳MPI库。</li>
<li>Gloo:facebook开源的一套集体通信库，他提供了对机器学习中有用的一些集合通信算法如：barrier, broadcast, allreduce</li>
<li>NCCL:NVIDIA Collective Communications Library, 英伟达基于NCIDIA-GPU的一套开源的集体通信库，如其官网描述：NVIDIA集体通信库（NCCL）实现了针对NVIDIA GPU性能优化的多GPU和多节点集体通信原语。NCCL提供了诸如all-gather, all-reduce, broadcast, reduce, reduce-scatter等实现，这些实现优化后可以通过PCIe和NVLink等高速互联，从而实现高带宽和低延迟。 因为NCCL则是NVIDIA基于自身硬件定制的，能做到更有针对性且更方便优化，故在英伟达硬件上，NCCL的效果往往比其它的通信库更好。<ul>
<li>P2P（Peer-to-Peer）是指单个节点内的 GPU 之间直接通信，而不需要通过 CPU 或系统内存中转，可以显著提高通信效率。但若某些 GPU 之间没有直接的 P2P 连接（NVLink 或 PCIe P2P），NCCL可能会初始化后挂死，通过设置 NCCL_P2P_DISABLE&#x3D;1，可以强制 NCCL 使用系统内存中转的方式代替 P2P 通信，从而避免这些问题。</li>
</ul>
</li>
</ul>
<p>NCCL遇到显卡P2P通信问题:<a target="_blank" rel="noopener" href="https://huo.zai.meng.li/p/vllm%E5%90%AF%E5%8A%A8%E6%97%B6nccl%E9%81%87%E5%88%B0%E6%98%BE%E5%8D%A1p2p%E9%80%9A%E4%BF%A1%E9%97%AE%E9%A2%98/">1</a> <a target="_blank" rel="noopener" href="https://huo.zai.meng.li/p/vllm%E5%90%AF%E5%8A%A8%E6%97%B6nccl%E9%81%87%E5%88%B0%E6%98%BE%E5%8D%A1p2p%E9%80%9A%E4%BF%A1%E9%97%AE%E9%A2%98/">2</a></p>
<h2 id="上采样相关概念"><a href="#上采样相关概念" class="headerlink" title="上采样相关概念"></a>上采样相关概念</h2><p>放大特征图（feature map）尺寸的技术：转置卷积 (Deconvolution)、上采样 (Upsampling) 和 上池化 (Unpooling)</p>
<ol>
<li><strong>转置卷积</strong>：又称反卷积，与普通卷积相反，它将小尺寸的输入映射到更大的输出。常用于语义分割、图像生成等任务中。<br>通过在输入特征图的像素之间填充0（这个过程称为 dilation），并在周围添加 padding，然后进行一次标准的卷积操作，从而实现输出尺寸的放大。卷积核的权重是通过反向传播学习得到的。由于其学习特性，反卷积能够生成比插值方法更精细、信息更丰富的特征图。但计算量更大。<img src="https://i-blog.csdnimg.cn/blog_migrate/62be732a9003bfc80c298a2ecd5058a8.jpeg" width = "20%" /></li>
<li><strong>上采样</strong>：泛指所有将图像或特征图分辨率扩大的技术。在深度学习中，它通常特指那些不带可学习参数的、基于插值（Interpolation）的方法。</li>
</ol>
<ul>
<li>最近邻插值 (Nearest Neighbor Interpolation): 将输出图像中每个像素的值设为输入图像中最近邻像素的值。这种方法简单快速，但容易产生块状效应。</li>
<li>双线性插值 (Bilinear Interpolation): 考虑了输入图像中四个最近邻像素的加权平均值，生成的图像更平滑。</li>
<li>双三次插值 (Bicubic Interpolation): 考虑了更广泛的邻域（16个像素），效果更好，但计算更复杂。</li>
</ul>
<ol start="3">
<li><strong>上池化</strong>：池化（Pooling）操作的逆操作，特别是最大池化（Max Pooling）的逆操作。它旨在将特征图恢复到池化前的大小。<br>利用池化过程中记录的位置信息来恢复特征图的结构。在进行最大池化时，不仅会保留池化区域内的最大值，还会记录这个最大值在原始特征图中的位置索引。在上池化阶段，会创建一个与池化前尺寸相同的全零特征图，然后根据之前记录的位置索引，将池化后的特征值放回相应位置。其余位置则保持为0。</li>
</ol>
<p>在U-Net等经典的图像分割网络中，解码器部分可能会先使用上采样或上池化来放大尺寸，然后再通过卷积层（或反卷积层）来学习和丰富特征。<br><img src="https://mmbiz.qpic.cn/mmbiz_png/teF4oHzZ4IQzKII5nhSaQrQV4tmXKQvf0ibE3QUVDR8X6FcDBqicuTE3riaO2QDLS5nibEoMzI7ugWPu33yVZUAydQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" width = "100%" /><br>图（a）是输入层；<br>图（b）是14*14反卷积的结果；<br>图（c）（d）是28*28的UnPooling和反卷积的结果；<br>图（e）（f）是56*56的Unpooling和反卷积的结果；<br>图（g）（h）是112*112 UnPooling和反卷积的结果；<br>图（i）（j）是224*224的UnPooling和反卷积的结果</p>
<h1 id="MMDetection"><a href="#MMDetection" class="headerlink" title="MMDetection"></a>MMDetection</h1><h2 id="基本概念和环境搭建"><a href="#基本概念和环境搭建" class="headerlink" title="基本概念和环境搭建"></a>基本概念和环境搭建</h2><p><a target="_blank" rel="noopener" href="https://mmdetection.readthedocs.io/zh-cn/latest/">MMDetection</a> 由 7 个主要部分组成，apis、structures、datasets、models、engine、evaluation 和 visualization。</p>
<ul>
<li>apis 为模型推理提供高级 API。</li>
<li>structures 提供 bbox、mask 和 DetDataSample 等数据结构。</li>
<li>datasets 支持用于目标检测、实例分割和全景分割的各种数据集。<ul>
<li>transforms 包含各种数据增强变换。</li>
<li>samplers 定义了不同的数据加载器采样策略。</li>
</ul>
</li>
<li>models 是检测器最重要的部分，包含检测器的不同组件。<ul>
<li>detectors 定义所有检测模型类。</li>
<li>data_preprocessors 用于预处理模型的输入数据。</li>
<li>backbones 包含各种骨干网络。</li>
<li>necks 包含各种模型颈部组件。</li>
<li>dense_heads 包含执行密集预测的各种检测头。</li>
<li>roi_heads 包含从 RoI 预测的各种检测头。</li>
<li>seg_heads 包含各种分割头。</li>
<li>losses 包含各种损失函数。</li>
<li>task_modules 为检测任务提供模块，例如 assigners、samplers、box coders 和 prior generators。</li>
<li>layers 提供了一些基本的神经网络层。</li>
</ul>
</li>
<li>engine 是运行时组件的一部分。<ul>
<li>runner 为 MMEngine 的执行器提供扩展。</li>
<li>schedulers 提供用于调整优化超参数的调度程序。</li>
<li>optimizers 提供优化器和优化器封装。</li>
<li>hooks 提供执行器的各种钩子。</li>
</ul>
</li>
<li>evaluation 为评估模型性能提供不同的指标。</li>
<li>visualization 用于可视化检测结果。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://mmdetection.readthedocs.io/zh-cn/latest/get_started.html">文档 - 环境安装与验证</a></p>
<ol>
<li>使用 MIM 安装 MMEngine 和 MMCV。</li>
<li>从源码安装MMDetection</li>
<li>验证<blockquote>
<p>Debug:<br>1.验证推理时报AssertionError: MMCV &#x3D;&#x3D; 2.2.0 is used but incompatible. Please install mmcv&gt;&#x3D;2.0.0rc4, &lt;2.2.0. 解决: mmdet&#x2F;<strong>init</strong>.py”, line 17 强行改版本适配 &lt;&#x3D;<br>2.ModuleNotFoundError: No module named ‘mmdet’ 解决：编译mmdetection：python setup.py develop<br>3.cuda版本问题：conda install pytorch &#x3D;&#x3D; 1.13.1 torchvision &#x3D;&#x3D; 0.14.1 torchaudio&#x3D;&#x3D;0.13.1 cudatoolkit&#x3D;11.7 pytorch-cuda&#x3D;11.7 -c pytorch -c nvidia</p>
</blockquote>
</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://mmdetection.readthedocs.io/zh-cn/latest/user_guides/index.html#id1">训练 &amp; 测试</a>：使用开源模型和数据集来执行常见的训练和测试任务</p>
<p><a target="_blank" rel="noopener" href="https://mmdetection.readthedocs.io/zh-cn/latest/user_guides/index.html#id2">实用工具</a>：</p>
<ul>
<li><p>查看模型配置:<code>python tools/misc/print_config.py ./configs/_base_/models/mask-rcnn_r50_fpn.py</code></p>
</li>
<li><p>列出所有模型：<code>models = DetInferencer.list_models(&#39;mmdet&#39;)</code></p>
</li>
<li><p>推理：推理的高层编程接口——推理器Inferencer</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mmdet.apis <span class="keyword">import</span> DetInferencer</span><br><span class="line">inferencer = DetInferencer(<span class="string">&#x27;rtmdet_tiny_8xb32-300e_coco&#x27;</span>)   <span class="comment"># 初始化模型</span></span><br><span class="line">inferencer(<span class="string">&#x27;demo/demo.jpg&#x27;</span>, show=<span class="literal">False</span>,out_dir=<span class="string">&#x27;./outputs&#x27;</span>,print_result=<span class="literal">True</span>)   <span class="comment"># 推理示例图片</span></span><br><span class="line"><span class="comment"># 快速验证：</span></span><br><span class="line">python demo/image_demo.py demo/demo.jpg rtmdet_tiny_8xb32-300e_coco.py --weights rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth --device cpu</span><br></pre></td></tr></table></figure>
</li>
<li><p>下载COCO数据集：<code>python tools/misc/download_dataset.py --dataset-name coco2017</code></p>
</li>
<li><p>测试：<code>python tools/test.py $&#123;CONFIG_FILE&#125; $&#123;CHECKPOINT_FILE&#125; [--out $&#123;RESULT_FILE&#125;] [--show]</code></p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/test.py configs/rtmdet/rtmdet_l_8xb32-300e_coco.py checkpoints/rtmdet_l_8xb32-300e_coco_20220719_112030-5a0be7c4.pth --show-dir rtmdet_l_8xb32-300e_coco_results</span><br></pre></td></tr></table></figure></li>
<li><p>训练：</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python tools/train.py &#123;CONFIG_FILE&#125; [optional arguments]</span><br><span class="line">python tools/train.py configs/retinanet/retinanet_r50_fpn_1x_coco.py</span><br></pre></td></tr></table></figure></li>
<li><p><code>tensorboard --logdir=work_dirs</code>:<br>  tensorboard可视化–在<code>../_base_/default_runtime.py</code>–visualizer中：</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vis_backends = [</span><br><span class="line">    <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">&#x27;LocalVisBackend&#x27;</span>),</span><br><span class="line">    <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">&#x27;TensorboardVisBackend&#x27;</span>)</span><br><span class="line">]</span><br></pre></td></tr></table></figure></li>
</ul>
<p><a target="_blank" rel="noopener" href="https://mmdetection.readthedocs.io/zh-cn/latest/advanced_guides/index.html">进阶教程</a><br>自定义模型重点看：<a target="_blank" rel="noopener" href="https://mmdetection.readthedocs.io/zh-cn/latest/advanced_guides/index.html#id2">组件定制</a><br>深入理解看：<a target="_blank" rel="noopener" href="https://mmdetection.readthedocs.io/zh-cn/latest/article.html">中文教程</a><br><a target="_blank" rel="noopener" href="https://mmengine.readthedocs.io/zh-cn/latest/get_started/introduction.html">MMEngine</a>，较深入时要看<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/337375549">算法组件</a></p>
<p><img src="https://pic3.zhimg.com/80/v2-c4e6229a1fd42692d090108481be34a6_1440w.webp" alt="MM"></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/341954021">整体构建细节</a></p>
<p>Pipeline: 由一系列按照插入顺序运行的数据处理模块组成，每个模块完成某个特定功能，例如 Resize，因为其流式顺序运行特性，故叫做 Pipeline。</p>
<p>MMDataParallel:处理Dataloader中pytorch 无法解析的DataContainer 对象,且额外实现了 <code>train_step()</code> 和 <code>val_step() </code>两个函数，可以被 Runner 调用</p>
<img src="https://pic4.zhimg.com/80/v2-b03d43ed4b3dc4c02e68712e57023cff_1440w.webp" width = "80%" />

<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_16137569/article/details/121316235">MMDetection框架入门教程（完全版）</a><br>Pytorch</p>
<ol>
<li>构建数据集：新建一个类，并继承Dataset类，重写__getitem__()方法实现数据和标签的加载和遍历功能，并以pipeline的方式定义数据预处理流程</li>
<li>构建数据加载器：传入相应的参数实例化DataLoader</li>
<li>构建模型：新建一个类，并继承Module类，重写forward()函数定义模型的前向过程</li>
<li>定义损失函数和优化器：根据算法选择合适和损失函数和优化器</li>
<li>训练和验证：循环从DataLoader中获取数据和标签，送入网络模型，计算loss，根据反传的梯度使用优化器进行迭代优化</li>
<li>其他操作：在主调函数里可以任意穿插训练Tricks、日志打印、检查点保存等操作</li>
</ol>
<p>MMDetection</p>
<ol>
<li>注册数据集：CustomDataset是MMDetection在原始的Dataset基础上的再次封装，其__getitem__()方法会根据训练和测试模式分别重定向到prepare_train_img()和prepare_test_img()函数。用户以继承CustomDataset类的方式构建自己的数据集时，需要重写load_annotations()和get_ann_info()函数，定义数据和标签的加载及遍历方式。完成数据集类的定义后，还需要使用DATASETS.register_module()进行模块注册。</li>
<li>注册模型：模型构建的方式和Pytorch类似，都是新建一个Module的子类然后重写forward()函数。唯一的区别在于MMDetection中需要继承BaseModule而不是Module，BaseModule是Module的子类，MMLab中的任何模型都必须继承此类。另外，MMDetection将一个完整的模型拆分为backbone、neck和head三部分进行管理，所以用户需要按照这种方式，将算法模型拆解成3个类，分别使用BACKBONES.register_module()、NECKS.register_module()和HEADS.register_module()完成模块注册。</li>
<li>构建配置文件：配置文件用于配置算法各个组件的运行参数，大体上可以包含四个部分：datasets、models、schedules和runtime。完成相应模块的定义和注册后，在配置文件中配置好相应的运行参数，然后MMDetection就会通过Registry类读取并解析配置文件，完成模块的实例化。另外，配置文件可以通过_base_字段实现继承功能，以提高代码复用率。</li>
<li>训练和验证：在完成各模块的代码实现、模块的注册、配置文件的编写后，就可以使用.&#x2F;tools&#x2F;train.py和.&#x2F;tools&#x2F;test.py对模型进行训练和验证，不需要用户编写额外的代码。<br><img src="https://i-blog.csdnimg.cn/blog_migrate/9623846bf1a2b09682eab74e606063bb.png" alt="alt text"><br>蓝色部分表示Pytorch流程，橙色部分表示MMDetection流程，绿色部分表示和算法框架无关的通用流程。</li>
</ol>
<h2 id="Registry注册机制"><a href="#Registry注册机制" class="headerlink" title="Registry注册机制"></a>Registry注册机制</h2><p>从本质上讲，MMDetection 的注册机制是一个全局的键值映射系统，其“键”是字符串（例如，’ResNet’），“值”则是对应的类或函数（例如，ResNet 类）。这个机制由 MMDetection 的底层库 MMCV (OpenMMLab Computer Vision Foundation) 提供。<br>在 MMDetection 中，几乎所有的模型组件，包括骨干网络 (Backbones)、颈部 (Necks)、检测头 (Heads)、损失函数 (Losses)、数据增强流程 (Pipelines) 等，都是通过注册机制来管理的。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例化一个注册器用来管理模型</span></span><br><span class="line">MODELS = Registry(<span class="string">&#x27;myModels&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在类的创建过程中, 使用函数装饰器进行注册</span></span><br><span class="line"><span class="meta">@MODELS.register_module() </span><span class="comment">#该装饰器是实现注册的核心。它将 ResNet 类和字符串 &#x27;ResNet&#x27; 关联起来。</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, depth</span>):</span><br><span class="line">        self.depth = depth</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Initialize ResNet&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(depth))</span><br><span class="line"><span class="built_in">print</span>(MODELS)</span><br><span class="line"><span class="string">&quot;&quot;&quot; 打印结果为:</span></span><br><span class="line"><span class="string">Registry(name=myModels, items=&#123;&#x27;ResNet&#x27;: &lt;class &#x27;__main__.ResNet&#x27;&gt;&#125;)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置参数, 一般cfg从配置文件中获取</span></span><br><span class="line">backbone_cfg = <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">&#x27;ResNet&#x27;</span>, depth=<span class="number">101</span>)</span><br><span class="line"><span class="comment"># 实例化模型(将配置参数传给模型的构造函数), 得到实例化对象</span></span><br><span class="line">my_backbone = MODELS.build(backbone_cfg)</span><br><span class="line"><span class="built_in">print</span>(my_backbone)</span><br><span class="line"><span class="string">&quot;&quot;&quot; 打印结果为:</span></span><br><span class="line"><span class="string">Initialize ResNet101</span></span><br><span class="line"><span class="string">&lt;__main__.ResNet object at 0x000001E68E99E198&gt;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>@TRANSFORMS.register_module()</code><br>这是 MMDetection 库提供的一个 Python 装饰器。它用于将一个新的模块（通常是一个定义数据增强或预处理操作的类）注册到 MMDetection 库的流水线系统中。位于一个类定义的上方。<br>TRANSFORMS: 这是 MMDetection 中的一个注册表，用于存储目标检测流水线中使用的不同数据增强和预处理步骤。<br>register_module(): 这是 TRANSFORMS 注册表中的一个函数，用于注册一个新的模块。</li>
</ul>
<h2 id="Hook机制"><a href="#Hook机制" class="headerlink" title="Hook机制"></a>Hook机制</h2><p><a target="_blank" rel="noopener" href="https://mmengine.readthedocs.io/zh-cn/latest/tutorials/hook.html">MMEngine - Hook</a><br>Hook可以理解为一种触发器，可以在程序预定义的位置执行预定义的函数。MMCV根据算法的生命周期预定义了6个可以插入自定义函数的位点，用户可以在每个位点自由地插入任意数量的函数操作，如下图所示：<br><img src="https://i-blog.csdnimg.cn/blog_migrate/a3e9e76563206c4bab09b91762341533.png" width = "50%" /><br>MMCV已经实现了部分常用Hook，其中默认Hook不需要用户自行注册，通过配置文件配置对应的参数即可；定制Hook则需要用户在配置文件中手动配置custom_hooks字段进行注册。<br><img src="https://i-blog.csdnimg.cn/blog_migrate/945ac8d55f31965189e6fcdc2b86a3d0.png" alt="alt text"><br>和其他模块不同，当我们定义好一个Hook(并注册到HOOKS注册器中)之后，还需要注册到Runner中才能使用，前后一共进行两次注册。第一次注册到HOOKS是为了程序能够根据Hook名称找到对应的模块，第二次注册到Runner中是为了程序执行到预定义位置时能够调用对应的函数。</p>
<p>Runner是MMCV用来管理训练过程的一个类，封装了 OpenMMLab 体系下各个框架的训练和验证详细流程，其负责管理训练和验证过程中的整个生命周期；它内部会维护一个list类型变量self._hooks，我们需要把训练过程会调用的Hook实例对象按照优先级顺序全部添加到self._hooks中，这个过程通过Runner.register_hook()函数实现。MMCV预定义了几种优先级, 数字越小表示优先级越高, 如果觉得默认的分级方式颗粒度过大, 也可以直接传入0~100的整数进行精细划分。</p>
<p>实现一个Hook包含5个步骤：</p>
<ol>
<li>定义一个类，继承Hook基类</li>
<li>根据自定义Hook的功能有选择地重写Hook基类中对应的函数</li>
<li>注册自定义Hook模块到HOOKS查询表中（register_module）</li>
<li>实例化Hook模块并注册到Runner中（register_hook）</li>
<li>使用回调函数调用重写的Hook函数（call_hook）</li>
</ol>
<h2 id="算法实现流程"><a href="#算法实现流程" class="headerlink" title="算法实现流程"></a>算法实现流程</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>在Pytorch中Dataset的遍历是通过重写<code>__getitem__()</code>函数实现的，而MMDetection已经重写了<code>__getitem__()</code>函数，可以根据当前运行模式调用<code>prepare_train_img()</code>或<code>prepare_test_img()</code>，两者的区别在于是否加载训练标签。所以我们只需要重写<code>load_annotations()</code>和<code>get_ann_info()</code>函数</p>
<p>完成自定义的Dataset类后要加上<code>@DATASETS.register_module()</code>将当前模块注册到DATASETS表中。</p>
<h3 id="注册模型"><a href="#注册模型" class="headerlink" title="注册模型"></a>注册模型</h3><p>相比Pytorch的区别：<br>继承的父类从Module变成了BaseModule<br>需要按照backbone、neck和head的结构将模型拆解成3个部分，分别定义并注册到BACKBONES、NECKS以及HEADS当中：</p>
<ol>
<li>Backbone: 任何一个 batch 的图片先输入到 backbone 中进行特征提取，典型的骨干网络是 ResNet, Darknet <code>mmdet/models/backbones</code></li>
<li>Neck: 输出的单尺度或者多尺度特征图输入到 neck 模块中进行特征融合或者增强，neck 可以认为是 backbone 和 head 的连接层，主要负责对 backbone 的特征进行高效融合和增强，能够对输入的单尺度或者多尺度特征进行融合、增强输出等。典型的 neck 是 FPN <code>mmdet/models/necks</code></li>
<li>Head: 上述多尺度特征最终输入到 head 部分，一般都会包括分类和回归分支输出;目标检测算法输出一般包括分类和框坐标回归两个分支，不同算法 head 模块复杂程度不一样，灵活度比较高。在网络构建方面，理解目标检测算法主要是要理解 head 模块。<code>mmdet/models/dense_heads + roi_heads</code><br>虽然 head 部分的网络构建比较简单，但是由于正负样本属性定义、正负样本采样和 bbox 编解码模块都在 head 模块中进行组合调用，故 MMDetection 中最复杂的模块就是 head。<ol>
<li>Enhance: 在整个网络构建阶段都可以引入一些即插即用增强算子来增加提取提取能力，典型的例如 SPP、DCN、注意力机制 等等</li>
<li>BBox Assigner，BBox Sampler：目标检测 head 输出一般是特征图，对于分类任务存在严重的正负样本不平衡，可以通过正负样本属性分配和采样策略控制 <code>mmdet/core/bbox/assigners + samplers</code></li>
<li>BBox Encoder：为了方便收敛和平衡多分支，一般都会对 gt bbox 进行编码，如归一化 <code>mmdet/core/bbox/coder</code></li>
<li>Loss: 最后一步是计算分类和回归 loss，进行训练 <code>mmdet/models/losses</code></li>
</ol>
</li>
<li>Training tricks: 在训练过程中也包括非常多的 trick，例如优化器选择等，参数调节也非常关键</li>
</ol>
<h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p>在 MMDetection 中，一个模型被定义为一个配置文件 和对应被存储在 checkpoint 文件内的模型参数的集合。<br>在MMDetection框架下，不需要另外实现迭代训练&#x2F;测试流程的代码，只需要执行现成的train.py或test.py即可，由配置文件实现<br><a target="_blank" rel="noopener" href="https://mmdetection.readthedocs.io/zh-cn/latest/user_guides/config.html#id1">配置文件</a><br><a target="_blank" rel="noopener" href="https://mmengine.readthedocs.io/zh-cn/latest/advanced_tutorials/config.html">MMEngine - 配置（CONFIG）详细文档</a></p>
<p><code>./mmdetection/configs/_base_/..</code></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">_base_ = [</span><br><span class="line">    <span class="string">&#x27;mmdetection/configs/_base_/models/fast_rcnn_r50_fpn.py&#x27;</span>,		<span class="comment"># models</span></span><br><span class="line">    <span class="string">&#x27;mmdetection/configs/_base_/datasets/coco_detection.py&#x27;</span>,		<span class="comment"># datasets</span></span><br><span class="line">    <span class="string">&#x27;mmdetection/configs/_base_/schedules/schedule_1x.py&#x27;</span>,			<span class="comment"># schedules</span></span><br><span class="line">    <span class="string">&#x27;mmdetection/configs/_base_/default_runtime.py&#x27;</span>,				<span class="comment"># defualt_runtime</span></span><br><span class="line">]</span><br><span class="line"><span class="comment"># 这些配置信息样例如下：</span></span><br><span class="line"><span class="comment"># 1. 模型配置(models) =========================================</span></span><br><span class="line">model = <span class="built_in">dict</span>(</span><br><span class="line">	<span class="built_in">type</span>=<span class="string">&#x27;FastRCNN&#x27;</span>,			<span class="comment"># 模型名称是FastRCNN</span></span><br><span class="line">	backbone=<span class="built_in">dict</span>(				<span class="comment"># BackBone是ResNet</span></span><br><span class="line">        <span class="built_in">type</span>=<span class="string">&#x27;ResNet&#x27;</span>,</span><br><span class="line">        ...,</span><br><span class="line">    ),</span><br><span class="line">    neck=<span class="built_in">dict</span>(					<span class="comment"># Neck是FPN</span></span><br><span class="line">        <span class="built_in">type</span>=<span class="string">&#x27;FPN&#x27;</span>,</span><br><span class="line">        ...,</span><br><span class="line">    ),</span><br><span class="line">    roi_head=<span class="built_in">dict</span>(				<span class="comment"># Head是StandardRoIHead</span></span><br><span class="line">        <span class="built_in">type</span>=<span class="string">&#x27;StandardRoIHead&#x27;</span>,</span><br><span class="line">        ...,</span><br><span class="line">        loss_cls=<span class="built_in">dict</span>(...),		<span class="comment"># 分类损失函数</span></span><br><span class="line">        loss_bbox=<span class="built_in">dict</span>(...),	<span class="comment"># 回归损失函数</span></span><br><span class="line">    ),</span><br><span class="line">    train_cfg=<span class="built_in">dict</span>(				<span class="comment"># 训练参数配置</span></span><br><span class="line">    	assigner=<span class="built_in">dict</span>(...),		<span class="comment"># BBox Assigner</span></span><br><span class="line">    	sampler=<span class="built_in">dict</span>(...),		<span class="comment"># BBox Sampler</span></span><br><span class="line">    	...</span><br><span class="line">	),</span><br><span class="line">    test_cfg =<span class="built_in">dict</span>(				<span class="comment"># 测试参数配置</span></span><br><span class="line">    	nms=<span class="built_in">dict</span>(...),			<span class="comment"># NMS后处理</span></span><br><span class="line">    	...,</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 数据集配置(datasets) =========================================</span></span><br><span class="line">dataset_type = <span class="string">&#x27;...&#x27;</span>			<span class="comment"># 数据集名称</span></span><br><span class="line">data_root = <span class="string">&#x27;...&#x27;</span>				<span class="comment"># 数据集根目录</span></span><br><span class="line">img_norm_cfg = <span class="built_in">dict</span>(...)		<span class="comment"># 图像归一化参数</span></span><br><span class="line">train_pipeline = [				<span class="comment"># 训练数据处理Pipeline</span></span><br><span class="line">	...,</span><br><span class="line">]</span><br><span class="line">test_pipeline = [...]			<span class="comment"># 测试数据处理Pipeline</span></span><br><span class="line">data = <span class="built_in">dict</span>(</span><br><span class="line">	samples_per_gpu=<span class="number">2</span>,			<span class="comment"># batch_size</span></span><br><span class="line">    workers_per_gpu=<span class="number">2</span>,			<span class="comment"># GPU数量</span></span><br><span class="line">	train=<span class="built_in">dict</span>(					<span class="comment"># 训练集配置</span></span><br><span class="line">		<span class="built_in">type</span>=dataset_type,</span><br><span class="line">        ann_file=data_root + <span class="string">&#x27;annotations/instances_train2017.json&#x27;</span>,	<span class="comment"># 标注问加你</span></span><br><span class="line">        img_prefix=data_root + <span class="string">&#x27;train2017/&#x27;</span>,	<span class="comment"># 图像前缀</span></span><br><span class="line">		pipline=trian_pipline,					<span class="comment"># 数据预处理pipeline</span></span><br><span class="line">	),</span><br><span class="line">	val=<span class="built_in">dict</span>(					<span class="comment"># 验证集配置</span></span><br><span class="line">		...,</span><br><span class="line">		pipline=test_pipline,</span><br><span class="line">	),</span><br><span class="line">	test=<span class="built_in">dict</span>(					<span class="comment"># 测试集配置</span></span><br><span class="line">		...,</span><br><span class="line">		pipline=test_pipline,</span><br><span class="line">	)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 训练策略配置(schedules) =========================================</span></span><br><span class="line">evaluation = <span class="built_in">dict</span>(interval=<span class="number">1</span>, metric=<span class="string">&#x27;bbox&#x27;</span>)</span><br><span class="line">optimizer = <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">&#x27;SGD&#x27;</span>, lr=<span class="number">0.02</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">0.0001</span>)</span><br><span class="line">optimizer_config = <span class="built_in">dict</span>(grad_clip=<span class="literal">None</span>)</span><br><span class="line">lr_config = <span class="built_in">dict</span>(</span><br><span class="line">    policy=<span class="string">&#x27;step&#x27;</span>,</span><br><span class="line">    warmup=<span class="string">&#x27;linear&#x27;</span>,</span><br><span class="line">    warmup_iters=<span class="number">500</span>,</span><br><span class="line">    warmup_ratio=<span class="number">0.001</span>,</span><br><span class="line">    step=[<span class="number">8</span>, <span class="number">11</span>])</span><br><span class="line">runner = <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">&#x27;EpochBasedRunner&#x27;</span>, max_epochs=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 运行配置(runtime) =========================================</span></span><br><span class="line">checkpoint_config = <span class="built_in">dict</span>(interval=<span class="number">1</span>)</span><br><span class="line">log_config = <span class="built_in">dict</span>(interval=<span class="number">50</span>, hooks=[<span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">&#x27;TextLoggerHook&#x27;</span>)])</span><br><span class="line">custom_hooks = [<span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">&#x27;NumClassCheckHook&#x27;</span>)]</span><br><span class="line">dist_params = <span class="built_in">dict</span>(backend=<span class="string">&#x27;nccl&#x27;</span>)</span><br><span class="line">log_level = <span class="string">&#x27;INFO&#x27;</span></span><br><span class="line">load_from = <span class="literal">None</span></span><br><span class="line">resume_from = <span class="literal">None</span></span><br><span class="line">workflow = [(<span class="string">&#x27;train&#x27;</span>, <span class="number">1</span>)]</span><br></pre></td></tr></table></figure>
<p>需要继承的配置文件，新建一个配置文件的时候，一般都是继承这4个基础配置文件，然后在此基础上进行针对性调整：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从_base_中继承的原始优化器</span></span><br><span class="line">optimizer = <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">&#x27;SGD&#x27;</span>, lr=<span class="number">0.02</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">0.0001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改学习率</span></span><br><span class="line">optimizer = <span class="built_in">dict</span>(lr=<span class="number">0.001</span>)		</span><br><span class="line"><span class="comment"># 修改后optimizer变成</span></span><br><span class="line">optimizer = <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">&#x27;SGD&#x27;</span>, lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">0.0001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将原来的SGD替换成AdamW</span></span><br><span class="line">optimizer = <span class="built_in">dict</span>(_delete_=<span class="literal">True</span>, <span class="built_in">type</span>=<span class="string">&#x27;AdamW&#x27;</span>, lr=<span class="number">0.0001</span>, weight_decay=<span class="number">0.0001</span>)  </span><br><span class="line"><span class="comment"># 替换后optimizer变成</span></span><br><span class="line">optimizer = <span class="built_in">dict</span>(<span class="built_in">type</span>=<span class="string">&#x27;AdamW&#x27;</span>, lr=<span class="number">0.0001</span>, weight_decay=<span class="number">0.0001</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="训练和测试"><a href="#训练和测试" class="headerlink" title="训练和测试"></a>训练和测试</h3><p><strong>train.py</strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">	<span class="comment"># Step1: 解析配置文件, args.config是配置文件路径</span></span><br><span class="line">	cfg = Config.fromfile(args.config)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># Step2: 初始化模型, 函数内部调用的是DETECTORS.build(cfg)</span></span><br><span class="line">	model = build_detector(cfg.model)</span><br><span class="line">  model.init_weights()</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># Step3: 初始化训练集和验证集, 函数内部调用build_from_cfg(cfg, DATASETS), 等价于DATASETS.build(cfg)</span></span><br><span class="line">	datasets = [build_dataset(cfg.data.train)]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(cfg.workflow) == <span class="number">2</span>:</span><br><span class="line">        val_dataset = copy.deepcopy(cfg.data.val)</span><br><span class="line">        val_dataset.pipeline = cfg.data.train.pipeline <span class="comment"># 验证集在训练过程中使用train pipeline而不是test pipeline</span></span><br><span class="line">        datasets.append(build_dataset(val_dataset))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step4: 传入模型和数据集, 准备开始训练模型</span></span><br><span class="line">    train_detector(model, datasets, cfg)</span><br></pre></td></tr></table></figure>
<p><strong>train_detector</strong></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_detector</span>(<span class="params">model, dataset, cfg</span>):</span><br><span class="line">	<span class="comment"># 获取Runner类型, EpochBasedRunner或IterBasedRuner</span></span><br><span class="line">	runner_type = <span class="string">&#x27;EpochBasedRunner&#x27;</span> <span class="keyword">if</span> <span class="string">&#x27;runner&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> cfg <span class="keyword">else</span> cfg.runner[<span class="string">&#x27;type&#x27;</span>]</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># Step1: 获取dataloader, 因为dataset列表里包含了训练集和验证集, 所以使用for循环的方式构建dataloader</span></span><br><span class="line">	<span class="comment"># build_dataloader()会用DataLoader类进行dataloader的初始化</span></span><br><span class="line">    data_loaders = [</span><br><span class="line">        build_dataloader(</span><br><span class="line">            ds,</span><br><span class="line">            cfg.data.samples_per_gpu,		<span class="comment"># batch_size</span></span><br><span class="line">            runner_type=runner_type) <span class="keyword">for</span> ds <span class="keyword">in</span> dataset</span><br><span class="line">    ]</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># Step2: 封装模型, 为了进行分布式训练</span></span><br><span class="line">	model = MMDataParallel(model.cuda(cfg.gpu_ids[<span class="number">0</span>]), device_ids=cfg.gpu_ids)</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># Step3: 初始化优化器</span></span><br><span class="line">	optimizer = build_optimizer(model, cfg.optimizer)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># Step4: 初始化Runner</span></span><br><span class="line">	runner = build_runner(</span><br><span class="line">        cfg.runner,</span><br><span class="line">        default_args=<span class="built_in">dict</span>(model=model, optimizer=optimizer)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># Step5: 注册默认Hook(注册到runner._hooks列表中)</span></span><br><span class="line">	runner.register_training_hooks(cfg.lr_config, optimizer_config,</span><br><span class="line">                                   cfg.checkpoint_config, cfg.log_config,</span><br><span class="line">                                   cfg.get(<span class="string">&#x27;momentum_config&#x27;</span>, <span class="literal">None</span>))</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># Step6: 注册自定义Hook(注册到runner._hooks列表中)</span></span><br><span class="line">	 <span class="keyword">if</span> cfg.get(<span class="string">&#x27;custom_hooks&#x27;</span>, <span class="literal">None</span>):</span><br><span class="line">        custom_hooks = cfg.custom_hooks</span><br><span class="line">        <span class="keyword">for</span> hook_cfg <span class="keyword">in</span> cfg.custom_hooks:</span><br><span class="line">            hook_cfg = hook_cfg.copy()</span><br><span class="line">            priority = hook_cfg.pop(<span class="string">&#x27;priority&#x27;</span>, <span class="string">&#x27;NORMAL&#x27;</span>)</span><br><span class="line">            hook = build_from_cfg(hook_cfg, HOOKS)</span><br><span class="line">            runner.register_hook(hook, priority=priority)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># Step7: 开始训练流程</span></span><br><span class="line">    <span class="keyword">if</span> cfg.resume_from:</span><br><span class="line">    	<span class="comment"># 恢复检查点</span></span><br><span class="line">        runner.resume(cfg.resume_from)</span><br><span class="line">    <span class="keyword">elif</span> cfg.load_from:</span><br><span class="line">    	<span class="comment"># 加载预训练模型</span></span><br><span class="line">        runner.load_checkpoint(cfg.load_from)</span><br><span class="line">    <span class="comment"># 调用run()方法, 开始迭代过程</span></span><br><span class="line">    runner.run(data_loaders, cfg.workflow)</span><br></pre></td></tr></table></figure>
<p><strong>runner</strong><br>runner 对象内部的 run 方式是一个通用方法，可以运行任何 workflow，目前常用的主要是 train 和 val。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self, data_loaders, workflow, max_epochs=<span class="literal">None</span>, **kwargs</span>):</span><br><span class="line">    <span class="keyword">while</span> self.epoch &lt; self._max_epochs:</span><br><span class="line">        <span class="keyword">for</span> i, flow <span class="keyword">in</span> <span class="built_in">enumerate</span>(workflow):</span><br><span class="line">            mode, epochs = flow</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 如果mode=&#x27;train&#x27;, 则调用self.train()函数</span></span><br><span class="line">            <span class="comment"># 如果mode=&#x27;val&#x27;, 则调用self.val()函数</span></span><br><span class="line">            epoch_runner = <span class="built_in">getattr</span>(self, mode)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">                <span class="keyword">if</span> mode == <span class="string">&#x27;train&#x27;</span> <span class="keyword">and</span> self.epoch &gt;= self._max_epochs:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="comment"># 运行train()或val()</span></span><br><span class="line">                epoch_runner(data_loaders[i], **kwargs)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>train() val()</strong><br>train()和val()的核心函数是run_iter()，根据train_mode参数调用model.train_step()或model.val_step()，这两个函数最终都会指向我们自己模型的forward()函数，返回模型的前向推理结果（一般是Loss值）。Runner到我们自己的模型中间还会经过MMDataParallel、BaseDetector、SingleStageDetector(或TwoStageDetector)四个类，最终调用我们自己模型的forward()函数，执行推理过程。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, data_loader, **kwargs</span>):</span><br><span class="line">    self.model.train()</span><br><span class="line">    self.mode = <span class="string">&#x27;train&#x27;</span></span><br><span class="line">    self.data_loader = data_loader</span><br><span class="line">    self.call_hook(<span class="string">&#x27;before_train_epoch&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i, data_batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.data_loader):</span><br><span class="line">        self.call_hook(<span class="string">&#x27;before_train_iter&#x27;</span>)</span><br><span class="line">        self.run_iter(data_batch, train_mode=<span class="literal">True</span>)</span><br><span class="line">        self.call_hook(<span class="string">&#x27;after_train_iter&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    self.call_hook(<span class="string">&#x27;after_train_epoch&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.no_grad()  </span><span class="comment">#由于测试过程不需要梯度回传，所以val函数加了一个装饰器</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">val</span>(<span class="params">self, data_loader, **kwargs</span>):</span><br><span class="line">	<span class="comment"># 将模块设置为验证模式</span></span><br><span class="line">    self.model.<span class="built_in">eval</span>()</span><br><span class="line">    self.mode = <span class="string">&#x27;val&#x27;</span></span><br><span class="line">    self.data_loader = data_loader</span><br><span class="line">    <span class="keyword">for</span> i, data_batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.data_loader):</span><br><span class="line">        self.run_iter(data_batch, train_mode=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#核心函数实际上是 self.run_iter()，如下：</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_iter</span>(<span class="params">self, data_batch, train_mode, **kwargs</span>):</span><br><span class="line">    <span class="keyword">if</span> train_mode:</span><br><span class="line">        <span class="comment"># 对于每次迭代，最终是调用如下函数</span></span><br><span class="line">        outputs = self.model.train_step(data_batch,...)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 对于每次迭代，最终是调用如下函数</span></span><br><span class="line">        outputs = self.model.val_step(data_batch,...)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;log_vars&#x27;</span> <span class="keyword">in</span> outputs:</span><br><span class="line">        self.log_buffer.update(outputs[<span class="string">&#x27;log_vars&#x27;</span>],...)</span><br><span class="line">    self.outputs = outputs</span><br><span class="line"></span><br><span class="line"><span class="comment">#上述 self.call_hook() 表示在不同生命周期调用所有已经注册进去的 hook，而字符串参数表示对应的生命周期。以 OptimizerHook 为例，其执行反向传播、梯度裁剪和参数更新等核心训练功能：</span></span><br><span class="line"><span class="meta">@HOOKS.register_module()</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">OptimizerHook</span>(<span class="title class_ inherited__">Hook</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, grad_clip=<span class="literal">None</span></span>):</span><br><span class="line">        self.grad_clip = grad_clip</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">after_train_iter</span>(<span class="params">self, runner</span>):</span><br><span class="line">        runner.optimizer.zero_grad()</span><br><span class="line">        runner.outputs[<span class="string">&#x27;loss&#x27;</span>].backward()</span><br><span class="line">        <span class="keyword">if</span> self.grad_clip <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            grad_norm = self.clip_grads(runner.model.parameters())</span><br><span class="line">        runner.optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment">#可以发现 OptimizerHook 注册到的生命周期是 after_train_iter，故在每次 train() 里面运行到self.call_hook(&#x27;after_train_iter&#x27;) 时候就会被调用，其他 hook 也是同样运行逻辑。</span></span><br></pre></td></tr></table></figure>
<p>训练和验证的时候实际上调用了 model 内部的 train_step 和 val_step 函数，理解了两个函数调用流程就理解了 MMDetection 训练和测试流程。</p>
<p>MMDetection的梯度反传优化是通过一个实现了after_train_iter()的Hook实现的<br><img src="https://i-blog.csdnimg.cn/blog_migrate/bbc1b91aef9aa68f0fc1dff0720c1a29.png" alt="alt text"></p>
<h2 id="notes"><a href="#notes" class="headerlink" title="notes"></a>notes</h2><h3 id="Head流程"><a href="#Head流程" class="headerlink" title="Head流程"></a>Head流程</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/343433169">Head流程</a><br><img src="https://pic4.zhimg.com/80/v2-ba9edb24f8cbf10ee77bacb7f10befa7_1440w.webp" alt="Head"></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#============= mmdet/models/detectors/single_stage.py/SingleStageDetector ============</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward_train</span>(<span class="params">...</span>):</span><br><span class="line">    <span class="built_in">super</span>(SingleStageDetector, self).forward_train(img, img_metas)</span><br><span class="line">    <span class="comment"># 先进行 backbone+neck 的特征提取</span></span><br><span class="line">    x = self.extract_feat(img)</span><br><span class="line">    <span class="comment"># 主要是调用 bbox_head 内部的 forward_train 方法</span></span><br><span class="line">    losses = self.bbox_head.forward_train(x, ...)</span><br><span class="line">    <span class="keyword">return</span> losses</span><br></pre></td></tr></table></figure>
<p>读起来有点吃力，后续结合源码读</p>
<h3 id="修改"><a href="#修改" class="headerlink" title="修改"></a>修改</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">│</span><br><span class="line">├───configs</span><br><span class="line">│   └───FedPSD</span><br><span class="line">│           yolo.py</span><br><span class="line">│</span><br><span class="line">├───mmdet</span><br><span class="line">│   ├───datasets</span><br><span class="line">│   │   │   markingpoint.py</span><br><span class="line">│   │   │   __init__.py</span><br><span class="line">│   │   │</span><br><span class="line">│   │   └───pipelines   <span class="comment">#pipeline似乎可以复用，在配置文件里改？</span></span><br><span class="line">│   │       │   formating.py    </span><br><span class="line">│   │       │   loading.py  <span class="comment">#数据加载</span></span><br><span class="line">│   │       │   transforms.py   <span class="comment">#数据处理</span></span><br><span class="line">│   │       └───__init__.py</span><br><span class="line">│   │</span><br><span class="line">│   └───models</span><br><span class="line">│       ├───dense_heads</span><br><span class="line">│       │   │   psd_head.py</span><br><span class="line">│       │   └───__init__.py</span><br><span class="line">│       │</span><br><span class="line">│       └───detectors</span><br><span class="line">│           │   parking_slot_det.py</span><br><span class="line">│           └───__init__.py</span><br></pre></td></tr></table></figure>
<p>在修改之后，需要重新编译mmdet,在根目录使用<br><code>python setup.py install</code> <code>pip install -v -e .</code>, 否则会报错</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Embedded/" rel="prev" title="Embedded：嵌入式应用知识">
      <i class="fa fa-chevron-left"></i> Embedded：嵌入式应用知识
    </a></div>
      <div class="post-nav-item">
    <a href="/Project/" rel="next" title="Project：多个项目汇总">
      Project：多个项目汇总 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6"><span class="nav-number">1.</span> <span class="nav-text">深度学习框架</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-Activate-Function"><span class="nav-number">1.1.</span> <span class="nav-text">激活函数 Activate Function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%84%9F%E5%8F%97%E9%87%8E-Receptive-field"><span class="nav-number">1.2.</span> <span class="nav-text">感受野(Receptive field)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF"><span class="nav-number">1.3.</span> <span class="nav-text">卷积</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">1.4.</span> <span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Optimizer"><span class="nav-number">1.5.</span> <span class="nav-text">Optimizer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%8E%E4%BC%98%E5%8C%96%E5%99%A8%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5"><span class="nav-number">1.5.1.</span> <span class="nav-text">学习率与优化器调度策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E9%87%8F%E4%B8%8E%E6%A2%AF%E5%BA%A6%E6%8E%A7%E5%88%B6%E7%AD%96%E7%95%A5"><span class="nav-number">1.5.2.</span> <span class="nav-text">动量与梯度控制策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%8E%E6%B3%9B%E5%8C%96%E7%AD%96%E7%95%A5"><span class="nav-number">1.5.3.</span> <span class="nav-text">模型正则化与泛化策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E7%BB%84%E5%90%88%E6%8E%A8%E8%8D%90"><span class="nav-number">1.5.4.</span> <span class="nav-text">常见组合推荐</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-size"><span class="nav-number">1.6.</span> <span class="nav-text">Batch size</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E3%80%8Closs-function%E3%80%8D"><span class="nav-number">1.7.</span> <span class="nav-text">损失函数「loss function」</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#L1%E8%8C%83%E6%95%B0%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%88MAE%EF%BC%89"><span class="nav-number">1.7.1.</span> <span class="nav-text">L1范数损失函数（MAE）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#L2%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%88MSE%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%89"><span class="nav-number">1.7.2.</span> <span class="nav-text">L2损失函数（MSE均方误差损失函数）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KL%E6%95%A3%E5%BA%A6%EF%BC%88-Kullback-Leibler-divergence%EF%BC%89"><span class="nav-number">1.7.3.</span> <span class="nav-text">KL散度（ Kullback-Leibler divergence）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.7.4.</span> <span class="nav-text">交叉熵损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A6%99%E5%86%9C%E4%BF%A1%E6%81%AF%E9%87%8F%EF%BC%88Shannon-Information%EF%BC%89"><span class="nav-number">1.7.4.1.</span> <span class="nav-text">香农信息量（Shannon Information）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CE"><span class="nav-number">1.7.4.2.</span> <span class="nav-text">CE</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CE-loss"><span class="nav-number">1.7.4.3.</span> <span class="nav-text">CE loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Focal-loss"><span class="nav-number">1.7.4.4.</span> <span class="nav-text">Focal loss</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#IoU-Loss"><span class="nav-number">1.7.5.</span> <span class="nav-text">IoU Loss</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%88Attention-Mechanism%EF%BC%89"><span class="nav-number">2.</span> <span class="nav-text">注意力机制（Attention Mechanism）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BD%AF%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%9F%9F"><span class="nav-number">2.1.</span> <span class="nav-text">软注意力的注意力域</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A9%BA%E9%97%B4%E5%9F%9F%EF%BC%88Spatial-Domain%EF%BC%89"><span class="nav-number">2.1.1.</span> <span class="nav-text">空间域（Spatial Domain）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9A%E9%81%93%E5%9F%9F%EF%BC%88Channel-Domain%EF%BC%89"><span class="nav-number">2.1.2.</span> <span class="nav-text">通道域（Channel Domain）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%B6%E5%9F%9F%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">2.1.3.</span> <span class="nav-text">时域注意力机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9A%E9%81%93%E5%92%8C%E7%A9%BA%E9%97%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">2.1.4.</span> <span class="nav-text">通道和空间注意力机制</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Metrics-%E8%AF%84%E4%BC%B0"><span class="nav-number">3.</span> <span class="nav-text">Metrics 评估</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5"><span class="nav-number">3.1.</span> <span class="nav-text">混淆矩阵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Evaluation-parameters"><span class="nav-number">3.2.</span> <span class="nav-text">Evaluation parameters</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%A1%E7%AE%97%E9%87%8F-FLOPs-%E5%92%8C%E5%8F%82%E6%95%B0%E9%87%8F-Params"><span class="nav-number">3.3.</span> <span class="nav-text">模型计算量(FLOPs)和参数量(Params)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Transformer"><span class="nav-number">4.</span> <span class="nav-text">Transformer</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5"><span class="nav-number">5.</span> <span class="nav-text">相关概念</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0"><span class="nav-number">5.1.</span> <span class="nav-text">联邦学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83"><span class="nav-number">5.2.</span> <span class="nav-text">分布式训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8A%E9%87%87%E6%A0%B7%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5"><span class="nav-number">5.3.</span> <span class="nav-text">上采样相关概念</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MMDetection"><span class="nav-number">6.</span> <span class="nav-text">MMDetection</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA"><span class="nav-number">6.1.</span> <span class="nav-text">基本概念和环境搭建</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Registry%E6%B3%A8%E5%86%8C%E6%9C%BA%E5%88%B6"><span class="nav-number">6.2.</span> <span class="nav-text">Registry注册机制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hook%E6%9C%BA%E5%88%B6"><span class="nav-number">6.3.</span> <span class="nav-text">Hook机制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E6%B5%81%E7%A8%8B"><span class="nav-number">6.4.</span> <span class="nav-text">算法实现流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">6.4.1.</span> <span class="nav-text">数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%A8%E5%86%8C%E6%A8%A1%E5%9E%8B"><span class="nav-number">6.4.2.</span> <span class="nav-text">注册模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-number">6.4.3.</span> <span class="nav-text">配置文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E5%92%8C%E6%B5%8B%E8%AF%95"><span class="nav-number">6.4.4.</span> <span class="nav-text">训练和测试</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#notes"><span class="nav-number">6.5.</span> <span class="nav-text">notes</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Head%E6%B5%81%E7%A8%8B"><span class="nav-number">6.5.1.</span> <span class="nav-text">Head流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9"><span class="nav-number">6.5.2.</span> <span class="nav-text">修改</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Arrow</p>
  <div class="site-description" itemprop="description">记录一些杂七杂八的东西</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">18</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Arrowes" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Arrowes" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:395841716@qq.com" title="E-Mail → mailto:395841716@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/wangyujie.site" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;wangyujie.site" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i>Zhihu</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/Arrowes?spm=1000.2115.3001.5343" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;Arrowes?spm&#x3D;1000.2115.3001.5343" rel="noopener" target="_blank"><i class="fa fa-crosshairs fa-fw"></i>CSDN</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/23930762?spm_id_from=333.1007.0.0" title="bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;23930762?spm_id_from&#x3D;333.1007.0.0" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>bilibili</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://oshwhub.com/arrows" title="立创EDA → https:&#x2F;&#x2F;oshwhub.com&#x2F;arrows" rel="noopener" target="_blank"><i class="fa fa-microchip fa-fw"></i>立创EDA</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class=""></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Arrow</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">104k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">6:18</span>
</div>

<div>
<span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("11/22/2022 00:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>
</div>
        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  
  <script data-pjax>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="/js/local-search.js"></script>











<script data-pjax>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'dark',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

    </div>
<script src="https://awp-assets.meituan.net/thh/thh_feb_web_portal/js/jquery-3.6.0.min.js"></script>
<script src="/js/code-unfold.js"></script>
<script src="/js/highlight-keywords.js"></script>
</body>
</html>
